{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0c9ae32",
   "metadata": {},
   "source": [
    "# Clustering RAJA Performance Suite Dataset: Thicket Tutorial\n",
    "\n",
    "Thicket is a python-based toolkit for Exploratory Data Analysis (EDA) of parallel performance data that enables performance optimization and understanding of applicationsâ€™ performance on supercomputers. It bridges the performance tool gap between being able to consider only a single instance of a simulation run (e.g., single platform, single measurement tool, or single scale) and finding actionable insights in multi-dimensional, multi-scale, multi-architecture, and multi-tool performance datasets.\n",
    "\n",
    "**NOTE: An interactive version of this notebook is available in the Binder environment.**\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/llnl/thicket-tutorial/develop)\n",
    "\n",
    "***\n",
    "\n",
    "## 1. Import Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76011bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "from itertools import product\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import HTML\n",
    "from IPython.display import Markdown\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import hatchet as ht\n",
    "from hatchet import QueryMatcher\n",
    "\n",
    "import thicket as th\n",
    "from thicket import Thicket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c198209",
   "metadata": {},
   "source": [
    "Disable the absurd number of warnings that NumPy will produce:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6586fbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9038d9",
   "metadata": {},
   "source": [
    "## 2. Utility Functions for Ingesting and Manipulating Performance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c86f0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perf_leaves(thicket):\n",
    "    \"\"\"Get data associated with only the leaf nodes of the thicket.\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    thicket -- thicket object\n",
    "    \"\"\"\n",
    "    query = QueryMatcher().match(\n",
    "        \".\",\n",
    "        lambda row: len(row.index.get_level_values(\"node\")[0].children) == 0\n",
    "    )\n",
    "    new_thicket = thicket.copy()\n",
    "    matches = query.apply(thicket)\n",
    "    idx_names = new_thicket.dataframe.index.names\n",
    "    new_thicket.dataframe.reset_index(inplace=True)\n",
    "    new_thicket.dataframe = new_thicket.dataframe.loc[new_thicket.dataframe[\"node\"].isin(matches)]\n",
    "    new_thicket.dataframe.set_index(idx_names, inplace=True)\n",
    "    squashed_gf = new_thicket.squash()\n",
    "    new_thicket.graph = squashed_gf.graph\n",
    "    new_thicket.statsframe.graph = squashed_gf.graph\n",
    "    \n",
    "    # NEW: need to update the performance data and the statsframe with the remaining (re-indexed) nodes.\n",
    "    # The dataframe is internally updated in squash(), so we can easily just save it to our thicket perfdata.\n",
    "    # For the statsframe, we'll have to come up with a better way eventually, but for now, we'll just create\n",
    "    #    a new statsframe the same way we do when we create a new thicket. \n",
    "    new_thicket.dataframe = squashed_gf.dataframe    \n",
    "    subset_df = new_thicket.dataframe[\"name\"].reset_index().drop_duplicates(subset=[\"node\"])\n",
    "    new_thicket.statsframe = ht.GraphFrame(\n",
    "        graph=squashed_gf.graph,\n",
    "        dataframe=pd.DataFrame(\n",
    "            index=subset_df[\"node\"],\n",
    "            data={\"name\": subset_df[\"name\"].values},\n",
    "        ),\n",
    "    )\n",
    "    return new_thicket\n",
    "\n",
    "def apply_query_to_thicket(thicket, query):\n",
    "    \"\"\"Applies query to thicket object.\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    thicket -- thicket object\n",
    "    query -- thicket query\n",
    "    \"\"\"\n",
    "    new_thicket = thicket.copy()\n",
    "    matches = query.apply(thicket)\n",
    "    idx_names = new_thicket.dataframe.index.names\n",
    "    new_thicket.dataframe.reset_index(inplace=True)\n",
    "    new_thicket.dataframe = new_thicket.dataframe.loc[new_thicket.dataframe[\"node\"].isin(matches)]\n",
    "    new_thicket.dataframe.set_index(idx_names, inplace=True)\n",
    "    squashed_gf = new_thicket.squash()\n",
    "    new_thicket.graph = squashed_gf.graph\n",
    "    new_thicket.statsframe.graph = squashed_gf.graph\n",
    "    \n",
    "    # NEW: need to update the performance data and the statsframe with the remaining (re-indexed) nodes.\n",
    "    # The dataframe is internally updated in squash(), so we can easily just save it to our thicket perfdata.\n",
    "    # For the statsframe, we'll have to come up with a better way eventually, but for now, we'll just create\n",
    "    #    a new statsframe the same way we do when we create a new thicket. \n",
    "    new_thicket.dataframe = squashed_gf.dataframe    \n",
    "    subset_df = new_thicket.dataframe[\"name\"].reset_index().drop_duplicates(subset=[\"node\"])\n",
    "    new_thicket.statsframe = ht.GraphFrame(\n",
    "        graph=squashed_gf.graph,\n",
    "        dataframe=pd.DataFrame(\n",
    "            index=subset_df[\"node\"],\n",
    "            data={\"name\": subset_df[\"name\"].values},\n",
    "        ),\n",
    "    )\n",
    "    return new_thicket\n",
    "\n",
    "def get_node(thicket, node_name):\n",
    "    \"\"\"Get data associated with only the node corresponding to node_name.\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    thicket -- thicket object\n",
    "    node_name -- (str) name of a node\n",
    "    \"\"\"\n",
    "    query = QueryMatcher().match(\n",
    "        \".\",\n",
    "        lambda row: row.index.get_level_values(\"node\")[0].frame[\"name\"] == node_name\n",
    "    )        \n",
    "\n",
    "    return apply_query_to_thicket(thicket, query)\n",
    "\n",
    "def get_nodes_in_group(thicket, group_prefix):\n",
    "    \"\"\"Get data associated with kernels with the specified name prefix\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    thicket -- thicket object\n",
    "    group_prefix -- prefix to satisfy startswith condition\n",
    "    \"\"\"\n",
    "    nodes_thicket = get_perf_leaves(thicket)\n",
    "    query = QueryMatcher().match(\n",
    "        \".\",\n",
    "        lambda row: row.index.get_level_values(\"node\")[0].frame[\"name\"].startswith(group_prefix)\n",
    "    )\n",
    "    return apply_query_to_thicket(nodes_thicket, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7affa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_optimization_level(val):\n",
    "    match = re.search(r\"(?P<opt_level>-O[0123]).*\", val)\n",
    "    if match is None:\n",
    "        raise ValueError(\"Could not find opt level in {}\".format(val))\n",
    "    return match.group(\"opt_level\")\n",
    "\n",
    "def check_for_optimization_level_int(val):\n",
    "    match = re.search(r\"-O(?P<opt_level>[0-3]).*\", val)\n",
    "    if match is None:\n",
    "        raise ValueError(\"Could not find opt level in {}\".format(val))\n",
    "    return match.group(\"opt_level\")\n",
    "\n",
    "def add_metadata_to_thicket(thicket):\n",
    "    \"\"\"Insert additional metadata columns to the performance data table.\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    thicket -- thicket object\n",
    "    \"\"\"\n",
    "    thicket.metadata[\"opt_level\"] = thicket.metadata[\"rajaperf_compiler_options\"].apply(\n",
    "        check_for_optimization_level\n",
    "    )\n",
    "    thicket.metadata[\"opt_level_int\"] = thicket.metadata[\"rajaperf_compiler_options\"].apply(\n",
    "        check_for_optimization_level_int\n",
    "    )\n",
    "    thicket.add_column_from_metadata_to_ensemble(\"opt_level\")\n",
    "    thicket.add_column_from_metadata_to_ensemble(\"opt_level_int\")\n",
    "    thicket.add_column_from_metadata_to_ensemble(\"compiler\")\n",
    "    return [\"opt_level\", \"opt_level_int\", \"compiler\"], thicket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d602c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_ensemble_data(directories, metrics, exc_time_metric=\"time (exc)\", profile_level_name=\"profile\"):\n",
    "    \"\"\"Create a thicket from multiple profiles with specified metric columns.\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    directories -- list of paths to profiles\n",
    "    metrics -- perf metrics\n",
    "    \"\"\"\n",
    "    real_metrics = metrics.copy()\n",
    "    if \"name\" not in real_metrics:\n",
    "        real_metrics.append(\"name\")\n",
    "    if \"nid\" not in real_metrics:\n",
    "        real_metrics.append(\"nid\")\n",
    "    if exc_time_metric not in real_metrics:\n",
    "        real_metrics.append(exc_time_metric)\n",
    "    subthickets = []\n",
    "    for directory in directories:\n",
    "        thicket = Thicket.from_caliperreader(directory)\n",
    "        thicket.dataframe = thicket.dataframe[[*real_metrics]]\n",
    "        thicket.exc_metrics = [m for m in thicket.dataframe.columns if m in thicket.exc_metrics]\n",
    "        thicket.inc_metrics = [m for m in thicket.dataframe.columns if m in thicket.inc_metrics]\n",
    "        if thicket.default_metric not in thicket.dataframe.columns:\n",
    "            if exc_time_metric in thicket.dataframe.columns:\n",
    "                thicket.default_metric = exc_time_metric\n",
    "            else:\n",
    "                raise ValueError(\"Could not set default metric\")\n",
    "        subthickets.append(thicket)\n",
    "    return Thicket.concat_thickets(axis=\"index\", thickets=subthickets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1880c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_speedup_by_opt_level(df, opt_level_metric=\"opt_level\", name_metric=\"name\",\n",
    "                              exc_time_metric=\"time (exc)\", min_opt_level=\"-O0\"):\n",
    "    \"\"\"Calculate speedup based on optimization level (e.g., O0/O1, O0/O2, etc.).\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    df -- performace data table\n",
    "    \"\"\"\n",
    "    index_names = df.index.names\n",
    "    df.reset_index(inplace=True)\n",
    "    min_opt_groups = df.groupby(\n",
    "        by=name_metric\n",
    "    )\n",
    "    min_opt_times = {}\n",
    "    for group_name, group in min_opt_groups:\n",
    "        min_time = group.loc[group[opt_level_metric] == min_opt_level, exc_time_metric].iloc[0]\n",
    "        min_opt_times[group_name] = min_time\n",
    "        \n",
    "    def calc_speedup(row):\n",
    "        base_time = min_opt_times[row[name_metric]]\n",
    "        ret_val = base_time / row[exc_time_metric]\n",
    "        return ret_val\n",
    "    \n",
    "    speedup_col = df.apply(calc_speedup, axis=\"columns\")\n",
    "    df[\"speedup\"] = speedup_col\n",
    "    \n",
    "    df.set_index(index_names, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a811cb0a",
   "metadata": {},
   "source": [
    "## 3. Utility Functions for KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0534d891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_clustering_df(gf, metrics):\n",
    "    tmp_df = gf.dataframe.reset_index()\n",
    "    real_metrics = metrics\n",
    "    if \"name\" in tmp_df.columns and \"name\" not in metrics:\n",
    "        real_metrics.append(\"name\")\n",
    "    return tmp_df[[*metrics]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddd1f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(df, metrics, **kwargs):\n",
    "    \"\"\"Creates normalized versions of the specified metrics.\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    df -- performace data table\n",
    "    metrics -- perf metrics\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    data = df[[*metrics]].to_numpy()\n",
    "    new_data = scaler.fit_transform(data)\n",
    "    for col in range(new_data.shape[1]):\n",
    "        df[\"{}_norm\".format(metrics[col])] = new_data[:, col]\n",
    "    return df\n",
    "\n",
    "\n",
    "def cluster_data(df, x_metric, y_metric, n_clusters=8, use_y_for_label=True):\n",
    "    \"\"\"Conducts a KMeans clustering of data from the DataFrame.\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    df -- performace data table\n",
    "    n_clusters -- no. of clusters\n",
    "    \"\"\"\n",
    "    print(x_metric, y_metric)\n",
    "    km = KMeans(n_clusters=n_clusters)\n",
    "    pts = df[[x_metric, y_metric]].to_numpy()\n",
    "    cluster_labels = km.fit_predict(pts)\n",
    "    label_metric = y_metric if use_y_for_label else x_metric\n",
    "    if label_metric.endswith(\"_norm\"):\n",
    "        label_metric = label_metric[:-len(\"_norm\")]\n",
    "    cluster_label = \"clusters\" if label_metric.startswith(\"pca\") else \"{}_cluster\".format(label_metric)\n",
    "    df[\"{}_id\".format(cluster_label)] = cluster_labels\n",
    "    df[cluster_label] = [\"Cluster {}\".format(l) for l in cluster_labels]\n",
    "    return cluster_label, km.cluster_centers_, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bcf566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified version of the code from\n",
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
    "def silhouette_test(df, x_metric, y_metric):\n",
    "    \"\"\"Silhouette analysis on KMeans clustering.\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    df -- performace data table\n",
    "    \"\"\"\n",
    "    range_n_clusters = [2, 3, 4, 5, 6]\n",
    "    \n",
    "    pts = df[[x_metric, y_metric]].to_numpy()\n",
    "\n",
    "    for n_clusters in range_n_clusters:\n",
    "        # Create a subplot with 1 row and 2 columns\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "        fig.set_size_inches(18, 7)\n",
    "\n",
    "        # The 1st subplot is the silhouette plot\n",
    "        # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "        # lie within [-0.1, 1]\n",
    "        ax1.set_xlim([-0.1, 1])\n",
    "        # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "        # plots of individual clusters, to demarcate them clearly.\n",
    "        ax1.set_ylim([0, len(pts) + (n_clusters + 1) * 10])\n",
    "\n",
    "        # Initialize the clusterer with n_clusters value and a random generator\n",
    "        # seed of 10 for reproducibility.\n",
    "        clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "        cluster_labels = clusterer.fit_predict(pts)\n",
    "\n",
    "        # The silhouette_score gives the average value for all the samples.\n",
    "        # This gives a perspective into the density and separation of the formed\n",
    "        # clusters\n",
    "        silhouette_avg = silhouette_score(pts, cluster_labels)\n",
    "        print(\n",
    "            \"For n_clusters =\",\n",
    "            n_clusters,\n",
    "            \"The average silhouette_score is :\",\n",
    "            silhouette_avg,\n",
    "        )\n",
    "\n",
    "        # Compute the silhouette scores for each sample\n",
    "        sample_silhouette_values = silhouette_samples(pts, cluster_labels)\n",
    "        \n",
    "        y_lower = 10\n",
    "        for i in range(n_clusters):\n",
    "            # Aggregate the silhouette scores for samples belonging to\n",
    "            # cluster i, and sort them\n",
    "            ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "            ith_cluster_silhouette_values.sort()\n",
    "\n",
    "            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "            y_upper = y_lower + size_cluster_i\n",
    "\n",
    "            color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "            ax1.fill_betweenx(\n",
    "                np.arange(y_lower, y_upper),\n",
    "                0,\n",
    "                ith_cluster_silhouette_values,\n",
    "                facecolor=color,\n",
    "                edgecolor=color,\n",
    "                alpha=0.7,\n",
    "            )\n",
    "\n",
    "            # Label the silhouette plots with their cluster numbers at the middle\n",
    "            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "            # Compute the new y_lower for next plot\n",
    "            y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "        ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "        ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "        ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "        # The vertical line for average silhouette score of all the values\n",
    "        ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "        ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "        ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "        # 2nd Plot showing the actual clusters formed\n",
    "        colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "        ax2.scatter(\n",
    "            pts[:, 0], pts[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n",
    "        )\n",
    "\n",
    "        # Labeling the clusters\n",
    "        centers = clusterer.cluster_centers_\n",
    "        # Draw white circles at cluster centers\n",
    "        ax2.scatter(\n",
    "            centers[:, 0],\n",
    "            centers[:, 1],\n",
    "            marker=\"o\",\n",
    "            c=\"white\",\n",
    "            alpha=1,\n",
    "            s=200,\n",
    "            edgecolor=\"k\",\n",
    "        )\n",
    "\n",
    "        for i, c in enumerate(centers):\n",
    "            ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\n",
    "\n",
    "        ax2.set_title(\"The visualization of the clustered data.\")\n",
    "        ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "        ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "        plt.suptitle(\n",
    "            \"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\"\n",
    "            % n_clusters,\n",
    "            fontsize=14,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b990cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(df, x_metric, y_metric, hue_metric, marker_style_metric=None, \n",
    "                  swap_marker_and_hue=False, cluster_centers=None, use_y_for_label=True,\n",
    "                  **kwargs):\n",
    "    \"\"\"Plot cluster with specific configuration.\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    df -- performace data table\n",
    "    \"\"\"\n",
    "    hue = hue_metric\n",
    "    df = df.sort_values(\n",
    "        [\n",
    "            \"opt_level_int\",\n",
    "            hue\n",
    "        ],\n",
    "    )\n",
    "    ax = None\n",
    "    if marker_style_metric is not None:\n",
    "        if swap_marker_and_hue:\n",
    "            hue_order = sorted(df[marker_style_metric].unique().tolist())\n",
    "            style_order = sorted(df[hue].unique().tolist())\n",
    "            ax = sns.scatterplot(\n",
    "                data=df,\n",
    "                x=x_metric,\n",
    "                y=y_metric,\n",
    "                hue=marker_style_metric,\n",
    "                style=hue,\n",
    "                legend=\"auto\",\n",
    "                hue_order=hue_order,\n",
    "                style_order=style_order,\n",
    "                **kwargs\n",
    "            )\n",
    "        else:\n",
    "            hue_order = sorted(df[hue].unique().tolist())\n",
    "            style_order = sorted(df[marker_style_metric].unique().tolist())\n",
    "            ax = sns.scatterplot(\n",
    "                data=df,\n",
    "                x=x_metric,\n",
    "                y=y_metric,\n",
    "                hue=hue,\n",
    "                style=marker_style_metric,\n",
    "                legend=\"auto\",\n",
    "                hue_order=hue_order,\n",
    "                style_order=style_order,\n",
    "                **kwargs\n",
    "            )\n",
    "    else:\n",
    "        hue_order = sorted(df[hue].unique().tolist())\n",
    "        ax = sns.scatterplot(\n",
    "            data=df,\n",
    "            x=x_metric,\n",
    "            y=y_metric,\n",
    "            hue=hue,\n",
    "            legend=\"auto\",\n",
    "            hue_order=hue_order,\n",
    "            **kwargs\n",
    "        )\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4c9966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lines_by_profile(df, x_metric, y_metric, grouping_metric, axes, legend=\"auto\"):\n",
    "    \"\"\"Plot line with specific configuration.\"\"\"\n",
    "    sns.lineplot(data=df, x=x_metric, y=y_metric, style=grouping_metric, ax=axes, legend=legend, color=\"#c0c0c0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c922e597",
   "metadata": {},
   "source": [
    "## 4. Set Variables to be Used for All Clustering\n",
    "\n",
    "Set `directories` to be a list of the directories containing the `.cali` files to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25f953f",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"../data/quartz\"\n",
    "dirs_ordered = [\n",
    "    \"GCC_10.3.1_BaseSeq_08388608/O0/GCC_1031_BaseSeq_O0_8388608_01.cali\",\n",
    "    \"GCC_10.3.1_BaseSeq_08388608/O1/GCC_1031_BaseSeq_O1_8388608_01.cali\",\n",
    "    \"GCC_10.3.1_BaseSeq_08388608/O2/GCC_1031_BaseSeq_O2_8388608_01.cali\",\n",
    "    \"GCC_10.3.1_BaseSeq_08388608/O3/GCC_1031_BaseSeq_O3_8388608_01.cali\"\n",
    "]\n",
    "directories = [\"{}/{}\".format(root, d) for d in dirs_ordered]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f345aff7",
   "metadata": {},
   "source": [
    "Set `topdown_cols` to be a list containing the Topdown metrics to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c32973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "topdown_cols=[\n",
    "    \"Retiring\",\n",
    "    \"Frontend bound\",\n",
    "    \"Backend bound\",\n",
    "    \"Bad speculation\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e479a3a6",
   "metadata": {},
   "source": [
    "## 5. Thicket Clustering\n",
    "\n",
    "Collect the performance data from Quartz. Then, normalize and cluster the 4 high-level topdown metrics (i.e., retiring, frontend bound, backend bound, and bad speculation) with respect to time. Finally, plot the clusters and produce a dataframe containing the node name, metrics, cluster ID, and optimization level.\n",
    "\n",
    "Set variables used for clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cfa497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If desired, set max_time_variance_node to the node to focus on for the clustering\n",
    "max_time_variance_node = \"Polybench_FLOYD_WARSHALL.default\"\n",
    "# Specify the number of clusters to create for each topdown metric\n",
    "n_clusters_retiring = 3\n",
    "n_clusters_frontend_bound = 4\n",
    "n_clusters_backend_bound = 3\n",
    "n_clusters_bad_speculation = 4\n",
    "# If desired, scale the topdown metrics by this factor before clustering\n",
    "topdown_scaling_factor = 1\n",
    "# If true, use sklearn's StandardScaler to create normalized metrics\n",
    "use_normalization = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9315af2a",
   "metadata": {},
   "source": [
    "Read data into a single Thicket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e439c1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "thicket = collect_ensemble_data(directories, topdown_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ea7b97",
   "metadata": {},
   "source": [
    "**NOTE:** Only run one of the next three cells\n",
    "\n",
    "Cell 1: Query the Thicket object to get data associated with only the node corresponding to `max_time_variance_node`\n",
    "\n",
    "Cell 2: Query the Thicket object to get data associated with only the leaf nodes\n",
    "\n",
    "Cell 3: Query the Thicket object to get data associated with kernels with the specified name prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea168dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_node_thicket = get_node(thicket, max_time_variance_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11be2ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_node_thicket = get_perf_leaves(thicket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9b54ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_node_thicket = get_nodes_in_group(thicket, \"Stream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d3e2cb",
   "metadata": {},
   "source": [
    "Add important metadata columns into the performance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdffa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_cols, single_node_thicket = add_metadata_to_thicket(single_node_thicket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba80ae38",
   "metadata": {},
   "source": [
    "Calculate speedup based on optimization level (e.g., O0/O1, O0/O2, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1703fcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_node_thicket.dataframe = calc_speedup_by_opt_level(single_node_thicket.dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337a8cfa",
   "metadata": {},
   "source": [
    "Extract the important data into a DataFrame better formatted for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc244ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = build_clustering_df(single_node_thicket, metadata_cols + topdown_cols + [\"speedup\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91e0e6d",
   "metadata": {},
   "source": [
    "**This cell is optional!**\n",
    "\n",
    "If desired, run this cell to create normalized versions of the specified metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea23bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = normalize_data(df, topdown_cols + [\"speedup\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0832fc",
   "metadata": {},
   "source": [
    "Cluster the topdown metrics with respect to exclusive time.\n",
    "\n",
    "If `topdown_scaling_factor` is set to any value besides 1, the topdown metrics will be multiplied by that value before clustering and divided by that value after clustering.\n",
    "\n",
    "If `use_normalization` is true, use the normalized metrics calculated by `normalize_data` for clustering instead of the vanilla versions.\n",
    "\n",
    "If `silhouette_mode` (in the next cell) is true, don't cluster. Instead run a Silhouette analysis on the data to find the correct K values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8f0f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_mode = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1f94a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cluster_centers = []\n",
    "cluster_labels = []\n",
    "pairs = list(product([\"speedup\"], topdown_cols))\n",
    "for x, y in pairs:\n",
    "    nc = 8\n",
    "    if y == \"Retiring\":\n",
    "        nc = n_clusters_retiring\n",
    "    elif y == \"Frontend bound\":\n",
    "        nc = n_clusters_frontend_bound\n",
    "    elif y == \"Backend bound\":\n",
    "        nc = n_clusters_backend_bound\n",
    "    elif y == \"Bad speculation\":\n",
    "        nc = n_clusters_bad_speculation\n",
    "    df[y] = df[y] * topdown_scaling_factor\n",
    "    if silhouette_mode:\n",
    "        x_met = x\n",
    "        y_met = y\n",
    "        if use_normalization:\n",
    "            x_met = \"{}_norm\".format(x_met)\n",
    "            y_met = \"{}_norm\".format(y_met)\n",
    "        print(\"Printing Silhouette Test for: {} vs {}\".format(x_met, y_met), end=\"\\n\\n\")\n",
    "        silhouette_test(df, x_met, y_met)\n",
    "    else:\n",
    "        if use_normalization:\n",
    "            clab, cc, df = cluster_data(df, \"{}_norm\".format(x), \"{}_norm\".format(y), n_clusters=nc)\n",
    "        else:\n",
    "            clab, cc, df = cluster_data(df, x, y, n_clusters=nc)\n",
    "        df[y] = df[y] / topdown_scaling_factor\n",
    "        cc = [(sc[0], sc[1]/topdown_scaling_factor) for sc in cc]\n",
    "        cluster_centers.append(cc)\n",
    "        cluster_labels.append(clab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d78248",
   "metadata": {},
   "source": [
    "These two cells simply print out the post-clustering DataFrame and the centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a062f169",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=[\"name\", \"speedup\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb3f41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0965157",
   "metadata": {},
   "source": [
    "## 6. Plot the Clusters as Scatterplots\n",
    "\n",
    "The legends will not be printed, but they will be dumped to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ce46ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns_handles = None\n",
    "sns_labels = None\n",
    "\n",
    "is_first = True\n",
    "\n",
    "for clab, mets, ccs in zip(cluster_labels, pairs, cluster_centers):\n",
    "    scatter_palette = sns.color_palette(\"colorblind\", n_colors=4).as_hex()\n",
    "    tmp = scatter_palette[1]\n",
    "    scatter_palette[1] = scatter_palette[2]\n",
    "    scatter_palette[2] = tmp\n",
    "    ax = plot_clusters(df, mets[0], mets[1], hue_metric=\"opt_level\",\n",
    "                       marker_style_metric=\"{}_cluster\".format(mets[1]), swap_marker_and_hue=False,\n",
    "                       palette=scatter_palette, s=3*mpl.rcParams[\"lines.markersize\"]**2,\n",
    "    )\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=1.5*ax.get_xticklabels()[0].get_fontsize())\n",
    "    xlab = ax.xaxis.get_label()\n",
    "    ax.set_xlabel(\n",
    "        \"Speedup\",#xlab.get_text(),\n",
    "        fontfamily=xlab.get_fontfamily(),\n",
    "        fontsize=1.5*xlab.get_fontsize(),\n",
    "        fontstyle=xlab.get_fontstyle(),\n",
    "        fontvariant=xlab.get_fontvariant(),\n",
    "        fontweight=xlab.get_fontweight(),\n",
    "    )\n",
    "    ylab = ax.yaxis.get_label()\n",
    "    ax.set_ylabel(\n",
    "        ylab.get_text(),\n",
    "        fontfamily=ylab.get_fontfamily(),\n",
    "        fontsize=1.5*ylab.get_fontsize(),\n",
    "        fontstyle=ylab.get_fontstyle(),\n",
    "        fontvariant=ylab.get_fontvariant(),\n",
    "        fontweight=ylab.get_fontweight(),\n",
    "    )\n",
    "    ax.set_xlim(0.99, 1.04)\n",
    "    ax.set_ylim(-0.05, 1.0)\n",
    "    plot_lines_by_profile(df, mets[0], mets[1], \"name\", ax)\n",
    "    legend = ax.legend(loc=\"center left\", bbox_to_anchor=(1.25, 0.5), ncol=2)\n",
    "    legend.get_texts()[0].set_text(\"Optimization Level\")\n",
    "    legend.get_texts()[5].set_text(\"Clusters\")\n",
    "    legend_fig = legend.figure\n",
    "    legend_fig.canvas.draw()\n",
    "    bbox = legend.get_window_extent().transformed(legend_fig.dpi_scale_trans.inverted())\n",
    "    legend.remove()\n",
    "    plt.tight_layout()\n",
    "    if is_first:\n",
    "        sns_handles, sns_labels = ax.get_legend_handles_labels()\n",
    "        is_first = False\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
