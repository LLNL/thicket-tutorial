{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b298bc4a",
   "metadata": {
    "papermill": {
     "duration": 0.009169,
     "end_time": "2024-03-26T22:19:26.881103",
     "exception": false,
     "start_time": "2024-03-26T22:19:26.871934",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Thicket Nsight Compute Reader: Thicket Tutorial\n",
    "\n",
    "Nsight Compute (NCU) is a performance profiler for NVIDIA GPUs. NCU report files do not have a calltree, but with the NVTX Caliper service we can forward Caliper annotations to NCU. By profiling the same executable with a calltree profiler like Caliper, we can map the NCU data to the calltree profile and create a Thicket object. \n",
    "\n",
    "**NOTE: An interactive version of this notebook is available in the Binder environment.**\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/llnl/thicket-tutorial/develop)\n",
    "\n",
    "***\n",
    "\n",
    "## 1. Import Necessary Packages\n",
    "\n",
    "The Thicket NCU reader requires an existing install of Nsight Compute, and the `extras/python` directory in the Nsight Compute installation directory must be added to the `PYTHONPATH`. We use `sys.path.append` to add the path to the `PYTHONPATH` in this notebook. If you are not on a Livermore Computing system, you must change this path to match your install of Nsight Compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e9d66ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T22:19:26.898836Z",
     "iopub.status.busy": "2024-03-26T22:19:26.898682Z",
     "iopub.status.idle": "2024-03-26T22:19:27.445595Z",
     "shell.execute_reply": "2024-03-26T22:19:27.445228Z"
    },
    "papermill": {
     "duration": 0.556743,
     "end_time": "2024-03-26T22:19:27.446317",
     "exception": false,
     "start_time": "2024-03-26T22:19:26.889574",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Roundtrip module could not be loaded. Requires jupyter notebook version <= 7.x.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/usr/tce/packages/nsight-compute/nsight-compute-2023.2.2/extras/python\")\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import HTML\n",
    "\n",
    "import thicket as tt\n",
    "\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47163338",
   "metadata": {},
   "source": [
    "## 2. The Dataset\n",
    "\n",
    "The dataset we are using comes from a profile of the RAJA Performance Suite on Lassen. We profile the `block_128` tuning of the `Base_CUDA`, `Lambda_CUDA`, and `RAJA_CUDA` variants, while varying the problem size for 1 million and 2 million. The calltree profiles come from the CUDA Activity Profile Caliper configuration. By changing the `variant` argument in the following cell, we can look at NCU data for different variants.\n",
    "\n",
    "The following are reproduceable steps to generate this dataset:\n",
    "\n",
    "```\n",
    "# Example of building\n",
    "$ . RAJAPerf/scripts/lc-builds/blueos_nvhpc_nvcc_clang_caliper.sh \n",
    "$ make -j\n",
    "\n",
    "# Load CUDA version equal to the CUDA version used to build RAJAPerf\n",
    "$ module load nvhpc/24.1-cuda-11.2.0\n",
    "\n",
    "# Turn off NVIDIA Data Center GPU Manager (DCGM) on Lassen so we can run NCU (get an error if it's on)\n",
    "$ dcgmi profile --pause\n",
    "```\n",
    "\n",
    "```\n",
    "# Example run to Generate the CUDA Activity Profile\n",
    "$ CALI_CONFIG=cuda-activity-profile,output.format=cali lrun -n 1 --smpiargs=\"-disable_gpu_hooks\" bin/raja-perf.exe --variants [Base_CUDA OR Lambda_CUDA OR RAJA_CUDA] --tunings block_128 --size [1048576 OR 2097152] --repfact 0.01\n",
    "\n",
    "# Example run to Generate the NCU Report\n",
    "$ CALI_SERVICES_ENABLE=nvtx lrun -n 1 --smpiargs=\"-disable_gpu_hooks\" ncu \\\n",
    "--nvtx --set default \\\n",
    "--export report \\\n",
    "--metrics sm__throughput.avg.pct_of_peak_sustained_elapsed \\\n",
    "--replay-mode application \\\n",
    "bin/raja-perf.exe --variants [Base_CUDA OR Lambda_CUDA OR RAJA_CUDA] --tunings block_128 --size [1048576 OR 2097152] --repfact 0.01\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ed93c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map all files\n",
    "ncu_dir = \"../data/ncu/\"\n",
    "ncu_report_mapping = {}\n",
    "variant = \"base_cuda\" # OR \"lambda_cuda\" OR \"raja_cuda\"\n",
    "problem_sizes = [\"1M\", \"2M\"]\n",
    "for problem_size in problem_sizes:\n",
    "    full_path = f\"{ncu_dir}{variant}/{problem_size}/\"\n",
    "    ncu_report_mapping[full_path+\"report.ncu-rep\"] = full_path+\"cuda_profile.cali\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972c694b",
   "metadata": {},
   "source": [
    "## 3. Read Calltree Profiles into Thicket\n",
    "\n",
    "The only performance metrics contained in the CUDA Activity Profile will be the CPU time `time` and the GPU time `time (gpu)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "989da4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(1/2) Reading Files: 100%|██████████| 2/2 [00:00<00:00, 14.48it/s]\n",
      "(2/2) Creating Thicket: 100%|██████████| 1/1 [00:00<00:00,  5.76it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>nid</th>\n",
       "      <th>time</th>\n",
       "      <th>time (gpu)</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>node</th>\n",
       "      <th>profile</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">{'name': 'RAJAPerf', 'type': 'function'}</th>\n",
       "      <th>3785253476</th>\n",
       "      <td>23.0</td>\n",
       "      <td>0.000606</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RAJAPerf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4063456299</th>\n",
       "      <td>23.0</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RAJAPerf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">{'name': 'Algorithm', 'type': 'function'}</th>\n",
       "      <th>3785253476</th>\n",
       "      <td>164.0</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Algorithm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4063456299</th>\n",
       "      <td>164.0</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Algorithm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">{'name': 'Algorithm_MEMCPY', 'type': 'function'}</th>\n",
       "      <th>3785253476</th>\n",
       "      <td>168.0</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Algorithm_MEMCPY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4063456299</th>\n",
       "      <td>168.0</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Algorithm_MEMCPY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">{'name': 'cudaDeviceSynchronize', 'type': 'function'}</th>\n",
       "      <th>3785253476</th>\n",
       "      <td>170.0</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cudaDeviceSynchronize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4063456299</th>\n",
       "      <td>170.0</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cudaDeviceSynchronize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">{'name': 'cudaLaunchKernel', 'type': 'function'}</th>\n",
       "      <th>3785253476</th>\n",
       "      <td>169.0</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cudaLaunchKernel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4063456299</th>\n",
       "      <td>169.0</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cudaLaunchKernel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">{'name': 'void RAJA::policy::cuda::impl::forall_cuda_kernel&lt;RAJA::policy::cuda::cuda_exec_explicit&lt;RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal&lt;(RAJA::named_dim)0, 128, 0&gt;, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true&gt;, 1ul, RAJA::Iterators::numeric_iterator&lt;long, long, long*&gt;, void rajaperf::algorithm::MEMCPY::runCudaVariantBlock&lt;128ul&gt;(rajaperf::VariantID)::{lambda(long)#2}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal&lt;(RAJA::named_dim)0, 128, 0&gt;, 128ul&gt;(void rajaperf::algorithm::MEMCPY::runCudaVariantBlock&lt;128ul&gt;(rajaperf::VariantID)::{lambda(long)#2}, RAJA::Iterators::numeric_iterator&lt;long, long, long*&gt;, long)', 'type': 'kernel'}</th>\n",
       "      <th>3785253476</th>\n",
       "      <td>225.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>void RAJA::policy::cuda::impl::forall_cuda_ker...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4063456299</th>\n",
       "      <td>225.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>void RAJA::policy::cuda::impl::forall_cuda_ker...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">{'name': 'Algorithm_MEMSET', 'type': 'function'}</th>\n",
       "      <th>3785253476</th>\n",
       "      <td>165.0</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Algorithm_MEMSET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4063456299</th>\n",
       "      <td>165.0</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Algorithm_MEMSET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">{'name': 'cudaDeviceSynchronize', 'type': 'function'}</th>\n",
       "      <th>3785253476</th>\n",
       "      <td>167.0</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cudaDeviceSynchronize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4063456299</th>\n",
       "      <td>167.0</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cudaDeviceSynchronize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">{'name': 'cudaLaunchKernel', 'type': 'function'}</th>\n",
       "      <th>3785253476</th>\n",
       "      <td>166.0</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cudaLaunchKernel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4063456299</th>\n",
       "      <td>166.0</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cudaLaunchKernel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">{'name': 'void RAJA::policy::cuda::impl::forall_cuda_kernel&lt;RAJA::policy::cuda::cuda_exec_explicit&lt;RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal&lt;(RAJA::named_dim)0, 128, 0&gt;, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true&gt;, 1ul, RAJA::Iterators::numeric_iterator&lt;long, long, long*&gt;, void rajaperf::algorithm::MEMSET::runCudaVariantBlock&lt;128ul&gt;(rajaperf::VariantID)::{lambda(long)#2}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal&lt;(RAJA::named_dim)0, 128, 0&gt;, 128ul&gt;(void rajaperf::algorithm::MEMSET::runCudaVariantBlock&lt;128ul&gt;(rajaperf::VariantID)::{lambda(long)#2}, RAJA::Iterators::numeric_iterator&lt;long, long, long*&gt;, long)', 'type': 'kernel'}</th>\n",
       "      <th>3785253476</th>\n",
       "      <td>224.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>void RAJA::policy::cuda::impl::forall_cuda_ker...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4063456299</th>\n",
       "      <td>224.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>void RAJA::policy::cuda::impl::forall_cuda_ker...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 nid  \\\n",
       "node                                               profile             \n",
       "{'name': 'RAJAPerf', 'type': 'function'}           3785253476   23.0   \n",
       "                                                   4063456299   23.0   \n",
       "{'name': 'Algorithm', 'type': 'function'}          3785253476  164.0   \n",
       "                                                   4063456299  164.0   \n",
       "{'name': 'Algorithm_MEMCPY', 'type': 'function'}   3785253476  168.0   \n",
       "                                                   4063456299  168.0   \n",
       "{'name': 'cudaDeviceSynchronize', 'type': 'func... 3785253476  170.0   \n",
       "                                                   4063456299  170.0   \n",
       "{'name': 'cudaLaunchKernel', 'type': 'function'}   3785253476  169.0   \n",
       "                                                   4063456299  169.0   \n",
       "{'name': 'void RAJA::policy::cuda::impl::forall... 3785253476  225.0   \n",
       "                                                   4063456299  225.0   \n",
       "{'name': 'Algorithm_MEMSET', 'type': 'function'}   3785253476  165.0   \n",
       "                                                   4063456299  165.0   \n",
       "{'name': 'cudaDeviceSynchronize', 'type': 'func... 3785253476  167.0   \n",
       "                                                   4063456299  167.0   \n",
       "{'name': 'cudaLaunchKernel', 'type': 'function'}   3785253476  166.0   \n",
       "                                                   4063456299  166.0   \n",
       "{'name': 'void RAJA::policy::cuda::impl::forall... 3785253476  224.0   \n",
       "                                                   4063456299  224.0   \n",
       "\n",
       "                                                                   time  \\\n",
       "node                                               profile                \n",
       "{'name': 'RAJAPerf', 'type': 'function'}           3785253476  0.000606   \n",
       "                                                   4063456299  0.000590   \n",
       "{'name': 'Algorithm', 'type': 'function'}          3785253476  0.000023   \n",
       "                                                   4063456299  0.000023   \n",
       "{'name': 'Algorithm_MEMCPY', 'type': 'function'}   3785253476  0.000017   \n",
       "                                                   4063456299  0.000017   \n",
       "{'name': 'cudaDeviceSynchronize', 'type': 'func... 3785253476  0.000059   \n",
       "                                                   4063456299  0.000038   \n",
       "{'name': 'cudaLaunchKernel', 'type': 'function'}   3785253476  0.000032   \n",
       "                                                   4063456299  0.000032   \n",
       "{'name': 'void RAJA::policy::cuda::impl::forall... 3785253476       NaN   \n",
       "                                                   4063456299       NaN   \n",
       "{'name': 'Algorithm_MEMSET', 'type': 'function'}   3785253476  0.000015   \n",
       "                                                   4063456299  0.000015   \n",
       "{'name': 'cudaDeviceSynchronize', 'type': 'func... 3785253476  0.000042   \n",
       "                                                   4063456299  0.000029   \n",
       "{'name': 'cudaLaunchKernel', 'type': 'function'}   3785253476  0.000040   \n",
       "                                                   4063456299  0.000033   \n",
       "{'name': 'void RAJA::policy::cuda::impl::forall... 3785253476       NaN   \n",
       "                                                   4063456299       NaN   \n",
       "\n",
       "                                                               time (gpu)  \\\n",
       "node                                               profile                  \n",
       "{'name': 'RAJAPerf', 'type': 'function'}           3785253476         NaN   \n",
       "                                                   4063456299         NaN   \n",
       "{'name': 'Algorithm', 'type': 'function'}          3785253476         NaN   \n",
       "                                                   4063456299         NaN   \n",
       "{'name': 'Algorithm_MEMCPY', 'type': 'function'}   3785253476         NaN   \n",
       "                                                   4063456299         NaN   \n",
       "{'name': 'cudaDeviceSynchronize', 'type': 'func... 3785253476         NaN   \n",
       "                                                   4063456299         NaN   \n",
       "{'name': 'cudaLaunchKernel', 'type': 'function'}   3785253476         NaN   \n",
       "                                                   4063456299         NaN   \n",
       "{'name': 'void RAJA::policy::cuda::impl::forall... 3785253476    0.000051   \n",
       "                                                   4063456299    0.000030   \n",
       "{'name': 'Algorithm_MEMSET', 'type': 'function'}   3785253476         NaN   \n",
       "                                                   4063456299         NaN   \n",
       "{'name': 'cudaDeviceSynchronize', 'type': 'func... 3785253476         NaN   \n",
       "                                                   4063456299         NaN   \n",
       "{'name': 'cudaLaunchKernel', 'type': 'function'}   3785253476         NaN   \n",
       "                                                   4063456299         NaN   \n",
       "{'name': 'void RAJA::policy::cuda::impl::forall... 3785253476    0.000034   \n",
       "                                                   4063456299    0.000020   \n",
       "\n",
       "                                                                                                            name  \n",
       "node                                               profile                                                        \n",
       "{'name': 'RAJAPerf', 'type': 'function'}           3785253476                                           RAJAPerf  \n",
       "                                                   4063456299                                           RAJAPerf  \n",
       "{'name': 'Algorithm', 'type': 'function'}          3785253476                                          Algorithm  \n",
       "                                                   4063456299                                          Algorithm  \n",
       "{'name': 'Algorithm_MEMCPY', 'type': 'function'}   3785253476                                   Algorithm_MEMCPY  \n",
       "                                                   4063456299                                   Algorithm_MEMCPY  \n",
       "{'name': 'cudaDeviceSynchronize', 'type': 'func... 3785253476                              cudaDeviceSynchronize  \n",
       "                                                   4063456299                              cudaDeviceSynchronize  \n",
       "{'name': 'cudaLaunchKernel', 'type': 'function'}   3785253476                                   cudaLaunchKernel  \n",
       "                                                   4063456299                                   cudaLaunchKernel  \n",
       "{'name': 'void RAJA::policy::cuda::impl::forall... 3785253476  void RAJA::policy::cuda::impl::forall_cuda_ker...  \n",
       "                                                   4063456299  void RAJA::policy::cuda::impl::forall_cuda_ker...  \n",
       "{'name': 'Algorithm_MEMSET', 'type': 'function'}   3785253476                                   Algorithm_MEMSET  \n",
       "                                                   4063456299                                   Algorithm_MEMSET  \n",
       "{'name': 'cudaDeviceSynchronize', 'type': 'func... 3785253476                              cudaDeviceSynchronize  \n",
       "                                                   4063456299                              cudaDeviceSynchronize  \n",
       "{'name': 'cudaLaunchKernel', 'type': 'function'}   3785253476                                   cudaLaunchKernel  \n",
       "                                                   4063456299                                   cudaLaunchKernel  \n",
       "{'name': 'void RAJA::policy::cuda::impl::forall... 3785253476  void RAJA::policy::cuda::impl::forall_cuda_ker...  \n",
       "                                                   4063456299  void RAJA::policy::cuda::impl::forall_cuda_ker...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk_cap = tt.Thicket.from_caliperreader(list(ncu_report_mapping.values()))\n",
    "tk_cap.dataframe.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d9eff4",
   "metadata": {},
   "source": [
    "## 4. Add NCU Data\n",
    "\n",
    "The Thicket `add_ncu` function takes one required argument and one optional arguement. The required argument `ncu_report_mapping` is the mapping from the NCU report file to the corresponding calltree profile run. The optional argument `chosen_metrics` allows for a subselection of the NCU performance metrics to add, since there can be hundreds of NCU performance metrics. By default we add all metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60ff359c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing action 600/601: 100%|██████████| 601/601 [00:24<00:00, 24.57it/s] \n",
      "Processing action 600/601: 100%|██████████| 601/601 [00:01<00:00, 389.47it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>nid</th>\n",
       "      <th>time</th>\n",
       "      <th>time (gpu)</th>\n",
       "      <th>name</th>\n",
       "      <th>gpu__time_duration.sum</th>\n",
       "      <th>sm__throughput.avg.pct_of_peak_sustained_elapsed</th>\n",
       "      <th>smsp__maximum_warps_avg_per_active_cycle</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>node</th>\n",
       "      <th>profile</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">{'name': 'RAJAPerf', 'type': 'function'}</th>\n",
       "      <th>3785253476</th>\n",
       "      <td>23.0</td>\n",
       "      <td>0.000606</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RAJAPerf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4063456299</th>\n",
       "      <td>23.0</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RAJAPerf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">{'name': 'Algorithm', 'type': 'function'}</th>\n",
       "      <th>3785253476</th>\n",
       "      <td>164.0</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Algorithm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4063456299</th>\n",
       "      <td>164.0</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Algorithm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">{'name': 'Algorithm_MEMCPY', 'type': 'function'}</th>\n",
       "      <th>3785253476</th>\n",
       "      <td>168.0</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Algorithm_MEMCPY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4063456299</th>\n",
       "      <td>168.0</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Algorithm_MEMCPY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">{'name': 'cudaDeviceSynchronize', 'type': 'function'}</th>\n",
       "      <th>3785253476</th>\n",
       "      <td>170.0</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cudaDeviceSynchronize</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4063456299</th>\n",
       "      <td>170.0</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cudaDeviceSynchronize</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">{'name': 'cudaLaunchKernel', 'type': 'function'}</th>\n",
       "      <th>3785253476</th>\n",
       "      <td>169.0</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cudaLaunchKernel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4063456299</th>\n",
       "      <td>169.0</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cudaLaunchKernel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">{'name': 'void RAJA::policy::cuda::impl::forall_cuda_kernel&lt;RAJA::policy::cuda::cuda_exec_explicit&lt;RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal&lt;(RAJA::named_dim)0, 128, 0&gt;, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true&gt;, 1ul, RAJA::Iterators::numeric_iterator&lt;long, long, long*&gt;, void rajaperf::algorithm::MEMCPY::runCudaVariantBlock&lt;128ul&gt;(rajaperf::VariantID)::{lambda(long)#2}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal&lt;(RAJA::named_dim)0, 128, 0&gt;, 128ul&gt;(void rajaperf::algorithm::MEMCPY::runCudaVariantBlock&lt;128ul&gt;(rajaperf::VariantID)::{lambda(long)#2}, RAJA::Iterators::numeric_iterator&lt;long, long, long*&gt;, long)', 'type': 'kernel'}</th>\n",
       "      <th>3785253476</th>\n",
       "      <td>225.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>void RAJA::policy::cuda::impl::forall_cuda_ker...</td>\n",
       "      <td>43424.0</td>\n",
       "      <td>7.243855</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4063456299</th>\n",
       "      <td>225.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>void RAJA::policy::cuda::impl::forall_cuda_ker...</td>\n",
       "      <td>22752.0</td>\n",
       "      <td>6.849288</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">{'name': 'Algorithm_MEMSET', 'type': 'function'}</th>\n",
       "      <th>3785253476</th>\n",
       "      <td>165.0</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Algorithm_MEMSET</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4063456299</th>\n",
       "      <td>165.0</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Algorithm_MEMSET</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">{'name': 'cudaDeviceSynchronize', 'type': 'function'}</th>\n",
       "      <th>3785253476</th>\n",
       "      <td>167.0</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cudaDeviceSynchronize</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4063456299</th>\n",
       "      <td>167.0</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cudaDeviceSynchronize</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">{'name': 'cudaLaunchKernel', 'type': 'function'}</th>\n",
       "      <th>3785253476</th>\n",
       "      <td>166.0</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cudaLaunchKernel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4063456299</th>\n",
       "      <td>166.0</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cudaLaunchKernel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">{'name': 'void RAJA::policy::cuda::impl::forall_cuda_kernel&lt;RAJA::policy::cuda::cuda_exec_explicit&lt;RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal&lt;(RAJA::named_dim)0, 128, 0&gt;, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true&gt;, 1ul, RAJA::Iterators::numeric_iterator&lt;long, long, long*&gt;, void rajaperf::algorithm::MEMSET::runCudaVariantBlock&lt;128ul&gt;(rajaperf::VariantID)::{lambda(long)#2}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal&lt;(RAJA::named_dim)0, 128, 0&gt;, 128ul&gt;(void rajaperf::algorithm::MEMSET::runCudaVariantBlock&lt;128ul&gt;(rajaperf::VariantID)::{lambda(long)#2}, RAJA::Iterators::numeric_iterator&lt;long, long, long*&gt;, long)', 'type': 'kernel'}</th>\n",
       "      <th>3785253476</th>\n",
       "      <td>224.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>void RAJA::policy::cuda::impl::forall_cuda_ker...</td>\n",
       "      <td>31520.0</td>\n",
       "      <td>8.552663</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4063456299</th>\n",
       "      <td>224.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>void RAJA::policy::cuda::impl::forall_cuda_ker...</td>\n",
       "      <td>18080.0</td>\n",
       "      <td>7.554598</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 nid  \\\n",
       "node                                               profile             \n",
       "{'name': 'RAJAPerf', 'type': 'function'}           3785253476   23.0   \n",
       "                                                   4063456299   23.0   \n",
       "{'name': 'Algorithm', 'type': 'function'}          3785253476  164.0   \n",
       "                                                   4063456299  164.0   \n",
       "{'name': 'Algorithm_MEMCPY', 'type': 'function'}   3785253476  168.0   \n",
       "                                                   4063456299  168.0   \n",
       "{'name': 'cudaDeviceSynchronize', 'type': 'func... 3785253476  170.0   \n",
       "                                                   4063456299  170.0   \n",
       "{'name': 'cudaLaunchKernel', 'type': 'function'}   3785253476  169.0   \n",
       "                                                   4063456299  169.0   \n",
       "{'name': 'void RAJA::policy::cuda::impl::forall... 3785253476  225.0   \n",
       "                                                   4063456299  225.0   \n",
       "{'name': 'Algorithm_MEMSET', 'type': 'function'}   3785253476  165.0   \n",
       "                                                   4063456299  165.0   \n",
       "{'name': 'cudaDeviceSynchronize', 'type': 'func... 3785253476  167.0   \n",
       "                                                   4063456299  167.0   \n",
       "{'name': 'cudaLaunchKernel', 'type': 'function'}   3785253476  166.0   \n",
       "                                                   4063456299  166.0   \n",
       "{'name': 'void RAJA::policy::cuda::impl::forall... 3785253476  224.0   \n",
       "                                                   4063456299  224.0   \n",
       "\n",
       "                                                                   time  \\\n",
       "node                                               profile                \n",
       "{'name': 'RAJAPerf', 'type': 'function'}           3785253476  0.000606   \n",
       "                                                   4063456299  0.000590   \n",
       "{'name': 'Algorithm', 'type': 'function'}          3785253476  0.000023   \n",
       "                                                   4063456299  0.000023   \n",
       "{'name': 'Algorithm_MEMCPY', 'type': 'function'}   3785253476  0.000017   \n",
       "                                                   4063456299  0.000017   \n",
       "{'name': 'cudaDeviceSynchronize', 'type': 'func... 3785253476  0.000059   \n",
       "                                                   4063456299  0.000038   \n",
       "{'name': 'cudaLaunchKernel', 'type': 'function'}   3785253476  0.000032   \n",
       "                                                   4063456299  0.000032   \n",
       "{'name': 'void RAJA::policy::cuda::impl::forall... 3785253476       NaN   \n",
       "                                                   4063456299       NaN   \n",
       "{'name': 'Algorithm_MEMSET', 'type': 'function'}   3785253476  0.000015   \n",
       "                                                   4063456299  0.000015   \n",
       "{'name': 'cudaDeviceSynchronize', 'type': 'func... 3785253476  0.000042   \n",
       "                                                   4063456299  0.000029   \n",
       "{'name': 'cudaLaunchKernel', 'type': 'function'}   3785253476  0.000040   \n",
       "                                                   4063456299  0.000033   \n",
       "{'name': 'void RAJA::policy::cuda::impl::forall... 3785253476       NaN   \n",
       "                                                   4063456299       NaN   \n",
       "\n",
       "                                                               time (gpu)  \\\n",
       "node                                               profile                  \n",
       "{'name': 'RAJAPerf', 'type': 'function'}           3785253476         NaN   \n",
       "                                                   4063456299         NaN   \n",
       "{'name': 'Algorithm', 'type': 'function'}          3785253476         NaN   \n",
       "                                                   4063456299         NaN   \n",
       "{'name': 'Algorithm_MEMCPY', 'type': 'function'}   3785253476         NaN   \n",
       "                                                   4063456299         NaN   \n",
       "{'name': 'cudaDeviceSynchronize', 'type': 'func... 3785253476         NaN   \n",
       "                                                   4063456299         NaN   \n",
       "{'name': 'cudaLaunchKernel', 'type': 'function'}   3785253476         NaN   \n",
       "                                                   4063456299         NaN   \n",
       "{'name': 'void RAJA::policy::cuda::impl::forall... 3785253476    0.000051   \n",
       "                                                   4063456299    0.000030   \n",
       "{'name': 'Algorithm_MEMSET', 'type': 'function'}   3785253476         NaN   \n",
       "                                                   4063456299         NaN   \n",
       "{'name': 'cudaDeviceSynchronize', 'type': 'func... 3785253476         NaN   \n",
       "                                                   4063456299         NaN   \n",
       "{'name': 'cudaLaunchKernel', 'type': 'function'}   3785253476         NaN   \n",
       "                                                   4063456299         NaN   \n",
       "{'name': 'void RAJA::policy::cuda::impl::forall... 3785253476    0.000034   \n",
       "                                                   4063456299    0.000020   \n",
       "\n",
       "                                                                                                            name  \\\n",
       "node                                               profile                                                         \n",
       "{'name': 'RAJAPerf', 'type': 'function'}           3785253476                                           RAJAPerf   \n",
       "                                                   4063456299                                           RAJAPerf   \n",
       "{'name': 'Algorithm', 'type': 'function'}          3785253476                                          Algorithm   \n",
       "                                                   4063456299                                          Algorithm   \n",
       "{'name': 'Algorithm_MEMCPY', 'type': 'function'}   3785253476                                   Algorithm_MEMCPY   \n",
       "                                                   4063456299                                   Algorithm_MEMCPY   \n",
       "{'name': 'cudaDeviceSynchronize', 'type': 'func... 3785253476                              cudaDeviceSynchronize   \n",
       "                                                   4063456299                              cudaDeviceSynchronize   \n",
       "{'name': 'cudaLaunchKernel', 'type': 'function'}   3785253476                                   cudaLaunchKernel   \n",
       "                                                   4063456299                                   cudaLaunchKernel   \n",
       "{'name': 'void RAJA::policy::cuda::impl::forall... 3785253476  void RAJA::policy::cuda::impl::forall_cuda_ker...   \n",
       "                                                   4063456299  void RAJA::policy::cuda::impl::forall_cuda_ker...   \n",
       "{'name': 'Algorithm_MEMSET', 'type': 'function'}   3785253476                                   Algorithm_MEMSET   \n",
       "                                                   4063456299                                   Algorithm_MEMSET   \n",
       "{'name': 'cudaDeviceSynchronize', 'type': 'func... 3785253476                              cudaDeviceSynchronize   \n",
       "                                                   4063456299                              cudaDeviceSynchronize   \n",
       "{'name': 'cudaLaunchKernel', 'type': 'function'}   3785253476                                   cudaLaunchKernel   \n",
       "                                                   4063456299                                   cudaLaunchKernel   \n",
       "{'name': 'void RAJA::policy::cuda::impl::forall... 3785253476  void RAJA::policy::cuda::impl::forall_cuda_ker...   \n",
       "                                                   4063456299  void RAJA::policy::cuda::impl::forall_cuda_ker...   \n",
       "\n",
       "                                                               gpu__time_duration.sum  \\\n",
       "node                                               profile                              \n",
       "{'name': 'RAJAPerf', 'type': 'function'}           3785253476                     NaN   \n",
       "                                                   4063456299                     NaN   \n",
       "{'name': 'Algorithm', 'type': 'function'}          3785253476                     NaN   \n",
       "                                                   4063456299                     NaN   \n",
       "{'name': 'Algorithm_MEMCPY', 'type': 'function'}   3785253476                     NaN   \n",
       "                                                   4063456299                     NaN   \n",
       "{'name': 'cudaDeviceSynchronize', 'type': 'func... 3785253476                     NaN   \n",
       "                                                   4063456299                     NaN   \n",
       "{'name': 'cudaLaunchKernel', 'type': 'function'}   3785253476                     NaN   \n",
       "                                                   4063456299                     NaN   \n",
       "{'name': 'void RAJA::policy::cuda::impl::forall... 3785253476                 43424.0   \n",
       "                                                   4063456299                 22752.0   \n",
       "{'name': 'Algorithm_MEMSET', 'type': 'function'}   3785253476                     NaN   \n",
       "                                                   4063456299                     NaN   \n",
       "{'name': 'cudaDeviceSynchronize', 'type': 'func... 3785253476                     NaN   \n",
       "                                                   4063456299                     NaN   \n",
       "{'name': 'cudaLaunchKernel', 'type': 'function'}   3785253476                     NaN   \n",
       "                                                   4063456299                     NaN   \n",
       "{'name': 'void RAJA::policy::cuda::impl::forall... 3785253476                 31520.0   \n",
       "                                                   4063456299                 18080.0   \n",
       "\n",
       "                                                               sm__throughput.avg.pct_of_peak_sustained_elapsed  \\\n",
       "node                                               profile                                                        \n",
       "{'name': 'RAJAPerf', 'type': 'function'}           3785253476                                               NaN   \n",
       "                                                   4063456299                                               NaN   \n",
       "{'name': 'Algorithm', 'type': 'function'}          3785253476                                               NaN   \n",
       "                                                   4063456299                                               NaN   \n",
       "{'name': 'Algorithm_MEMCPY', 'type': 'function'}   3785253476                                               NaN   \n",
       "                                                   4063456299                                               NaN   \n",
       "{'name': 'cudaDeviceSynchronize', 'type': 'func... 3785253476                                               NaN   \n",
       "                                                   4063456299                                               NaN   \n",
       "{'name': 'cudaLaunchKernel', 'type': 'function'}   3785253476                                               NaN   \n",
       "                                                   4063456299                                               NaN   \n",
       "{'name': 'void RAJA::policy::cuda::impl::forall... 3785253476                                          7.243855   \n",
       "                                                   4063456299                                          6.849288   \n",
       "{'name': 'Algorithm_MEMSET', 'type': 'function'}   3785253476                                               NaN   \n",
       "                                                   4063456299                                               NaN   \n",
       "{'name': 'cudaDeviceSynchronize', 'type': 'func... 3785253476                                               NaN   \n",
       "                                                   4063456299                                               NaN   \n",
       "{'name': 'cudaLaunchKernel', 'type': 'function'}   3785253476                                               NaN   \n",
       "                                                   4063456299                                               NaN   \n",
       "{'name': 'void RAJA::policy::cuda::impl::forall... 3785253476                                          8.552663   \n",
       "                                                   4063456299                                          7.554598   \n",
       "\n",
       "                                                               smsp__maximum_warps_avg_per_active_cycle  \n",
       "node                                               profile                                               \n",
       "{'name': 'RAJAPerf', 'type': 'function'}           3785253476                                       NaN  \n",
       "                                                   4063456299                                       NaN  \n",
       "{'name': 'Algorithm', 'type': 'function'}          3785253476                                       NaN  \n",
       "                                                   4063456299                                       NaN  \n",
       "{'name': 'Algorithm_MEMCPY', 'type': 'function'}   3785253476                                       NaN  \n",
       "                                                   4063456299                                       NaN  \n",
       "{'name': 'cudaDeviceSynchronize', 'type': 'func... 3785253476                                       NaN  \n",
       "                                                   4063456299                                       NaN  \n",
       "{'name': 'cudaLaunchKernel', 'type': 'function'}   3785253476                                       NaN  \n",
       "                                                   4063456299                                       NaN  \n",
       "{'name': 'void RAJA::policy::cuda::impl::forall... 3785253476                                      16.0  \n",
       "                                                   4063456299                                      16.0  \n",
       "{'name': 'Algorithm_MEMSET', 'type': 'function'}   3785253476                                       NaN  \n",
       "                                                   4063456299                                       NaN  \n",
       "{'name': 'cudaDeviceSynchronize', 'type': 'func... 3785253476                                       NaN  \n",
       "                                                   4063456299                                       NaN  \n",
       "{'name': 'cudaLaunchKernel', 'type': 'function'}   3785253476                                       NaN  \n",
       "                                                   4063456299                                       NaN  \n",
       "{'name': 'void RAJA::policy::cuda::impl::forall... 3785253476                                      16.0  \n",
       "                                                   4063456299                                      16.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add NCU to thicket\n",
    "ncu_metrics = [\n",
    "    \"gpu__time_duration.sum\",\n",
    "    \"sm__throughput.avg.pct_of_peak_sustained_elapsed\",\n",
    "    \"smsp__maximum_warps_avg_per_active_cycle\",\n",
    "]\n",
    "# Add in metrics\n",
    "tk_cap.add_ncu(\n",
    "    ncu_report_mapping=ncu_report_mapping, \n",
    "    chosen_metrics=ncu_metrics,\n",
    ")\n",
    "tk_cap.dataframe.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e55c3a",
   "metadata": {},
   "source": [
    "## 5. Add Problem Size to the Index\n",
    "\n",
    "We can add the problem size to the performance data index for clarity about which profile we are looking at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b06b7e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>profile</th>\n",
       "      <th>nid</th>\n",
       "      <th>time</th>\n",
       "      <th>time (gpu)</th>\n",
       "      <th>name</th>\n",
       "      <th>gpu__time_duration.sum</th>\n",
       "      <th>sm__throughput.avg.pct_of_peak_sustained_elapsed</th>\n",
       "      <th>smsp__maximum_warps_avg_per_active_cycle</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>node</th>\n",
       "      <th>ProblemSizeRunParam</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">{'name': 'RAJAPerf', 'type': 'function'}</th>\n",
       "      <th>2097152</th>\n",
       "      <td>3785253476</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.000606</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RAJAPerf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048576</th>\n",
       "      <td>4063456299</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RAJAPerf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">{'name': 'Algorithm', 'type': 'function'}</th>\n",
       "      <th>2097152</th>\n",
       "      <td>3785253476</td>\n",
       "      <td>164.0</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Algorithm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048576</th>\n",
       "      <td>4063456299</td>\n",
       "      <td>164.0</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Algorithm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">{'name': 'Algorithm_MEMCPY', 'type': 'function'}</th>\n",
       "      <th>2097152</th>\n",
       "      <td>3785253476</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Algorithm_MEMCPY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048576</th>\n",
       "      <td>4063456299</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Algorithm_MEMCPY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">{'name': 'cudaDeviceSynchronize', 'type': 'function'}</th>\n",
       "      <th>2097152</th>\n",
       "      <td>3785253476</td>\n",
       "      <td>170.0</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cudaDeviceSynchronize</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048576</th>\n",
       "      <td>4063456299</td>\n",
       "      <td>170.0</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cudaDeviceSynchronize</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">{'name': 'cudaLaunchKernel', 'type': 'function'}</th>\n",
       "      <th>2097152</th>\n",
       "      <td>3785253476</td>\n",
       "      <td>169.0</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cudaLaunchKernel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048576</th>\n",
       "      <td>4063456299</td>\n",
       "      <td>169.0</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cudaLaunchKernel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">{'name': 'void RAJA::policy::cuda::impl::forall_cuda_kernel&lt;RAJA::policy::cuda::cuda_exec_explicit&lt;RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal&lt;(RAJA::named_dim)0, 128, 0&gt;, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true&gt;, 1ul, RAJA::Iterators::numeric_iterator&lt;long, long, long*&gt;, void rajaperf::algorithm::MEMCPY::runCudaVariantBlock&lt;128ul&gt;(rajaperf::VariantID)::{lambda(long)#2}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal&lt;(RAJA::named_dim)0, 128, 0&gt;, 128ul&gt;(void rajaperf::algorithm::MEMCPY::runCudaVariantBlock&lt;128ul&gt;(rajaperf::VariantID)::{lambda(long)#2}, RAJA::Iterators::numeric_iterator&lt;long, long, long*&gt;, long)', 'type': 'kernel'}</th>\n",
       "      <th>2097152</th>\n",
       "      <td>3785253476</td>\n",
       "      <td>225.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>void RAJA::policy::cuda::impl::forall_cuda_ker...</td>\n",
       "      <td>43424.0</td>\n",
       "      <td>7.243855</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048576</th>\n",
       "      <td>4063456299</td>\n",
       "      <td>225.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>void RAJA::policy::cuda::impl::forall_cuda_ker...</td>\n",
       "      <td>22752.0</td>\n",
       "      <td>6.849288</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">{'name': 'Algorithm_MEMSET', 'type': 'function'}</th>\n",
       "      <th>2097152</th>\n",
       "      <td>3785253476</td>\n",
       "      <td>165.0</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Algorithm_MEMSET</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048576</th>\n",
       "      <td>4063456299</td>\n",
       "      <td>165.0</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Algorithm_MEMSET</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">{'name': 'cudaDeviceSynchronize', 'type': 'function'}</th>\n",
       "      <th>2097152</th>\n",
       "      <td>3785253476</td>\n",
       "      <td>167.0</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cudaDeviceSynchronize</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048576</th>\n",
       "      <td>4063456299</td>\n",
       "      <td>167.0</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cudaDeviceSynchronize</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">{'name': 'cudaLaunchKernel', 'type': 'function'}</th>\n",
       "      <th>2097152</th>\n",
       "      <td>3785253476</td>\n",
       "      <td>166.0</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cudaLaunchKernel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048576</th>\n",
       "      <td>4063456299</td>\n",
       "      <td>166.0</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cudaLaunchKernel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">{'name': 'void RAJA::policy::cuda::impl::forall_cuda_kernel&lt;RAJA::policy::cuda::cuda_exec_explicit&lt;RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal&lt;(RAJA::named_dim)0, 128, 0&gt;, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true&gt;, 1ul, RAJA::Iterators::numeric_iterator&lt;long, long, long*&gt;, void rajaperf::algorithm::MEMSET::runCudaVariantBlock&lt;128ul&gt;(rajaperf::VariantID)::{lambda(long)#2}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal&lt;(RAJA::named_dim)0, 128, 0&gt;, 128ul&gt;(void rajaperf::algorithm::MEMSET::runCudaVariantBlock&lt;128ul&gt;(rajaperf::VariantID)::{lambda(long)#2}, RAJA::Iterators::numeric_iterator&lt;long, long, long*&gt;, long)', 'type': 'kernel'}</th>\n",
       "      <th>2097152</th>\n",
       "      <td>3785253476</td>\n",
       "      <td>224.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>void RAJA::policy::cuda::impl::forall_cuda_ker...</td>\n",
       "      <td>31520.0</td>\n",
       "      <td>8.552663</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048576</th>\n",
       "      <td>4063456299</td>\n",
       "      <td>224.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>void RAJA::policy::cuda::impl::forall_cuda_ker...</td>\n",
       "      <td>18080.0</td>\n",
       "      <td>7.554598</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                           profile  \\\n",
       "node                                               ProblemSizeRunParam               \n",
       "{'name': 'RAJAPerf', 'type': 'function'}           2097152              3785253476   \n",
       "                                                   1048576              4063456299   \n",
       "{'name': 'Algorithm', 'type': 'function'}          2097152              3785253476   \n",
       "                                                   1048576              4063456299   \n",
       "{'name': 'Algorithm_MEMCPY', 'type': 'function'}   2097152              3785253476   \n",
       "                                                   1048576              4063456299   \n",
       "{'name': 'cudaDeviceSynchronize', 'type': 'func... 2097152              3785253476   \n",
       "                                                   1048576              4063456299   \n",
       "{'name': 'cudaLaunchKernel', 'type': 'function'}   2097152              3785253476   \n",
       "                                                   1048576              4063456299   \n",
       "{'name': 'void RAJA::policy::cuda::impl::forall... 2097152              3785253476   \n",
       "                                                   1048576              4063456299   \n",
       "{'name': 'Algorithm_MEMSET', 'type': 'function'}   2097152              3785253476   \n",
       "                                                   1048576              4063456299   \n",
       "{'name': 'cudaDeviceSynchronize', 'type': 'func... 2097152              3785253476   \n",
       "                                                   1048576              4063456299   \n",
       "{'name': 'cudaLaunchKernel', 'type': 'function'}   2097152              3785253476   \n",
       "                                                   1048576              4063456299   \n",
       "{'name': 'void RAJA::policy::cuda::impl::forall... 2097152              3785253476   \n",
       "                                                   1048576              4063456299   \n",
       "\n",
       "                                                                          nid  \\\n",
       "node                                               ProblemSizeRunParam          \n",
       "{'name': 'RAJAPerf', 'type': 'function'}           2097152               23.0   \n",
       "                                                   1048576               23.0   \n",
       "{'name': 'Algorithm', 'type': 'function'}          2097152              164.0   \n",
       "                                                   1048576              164.0   \n",
       "{'name': 'Algorithm_MEMCPY', 'type': 'function'}   2097152              168.0   \n",
       "                                                   1048576              168.0   \n",
       "{'name': 'cudaDeviceSynchronize', 'type': 'func... 2097152              170.0   \n",
       "                                                   1048576              170.0   \n",
       "{'name': 'cudaLaunchKernel', 'type': 'function'}   2097152              169.0   \n",
       "                                                   1048576              169.0   \n",
       "{'name': 'void RAJA::policy::cuda::impl::forall... 2097152              225.0   \n",
       "                                                   1048576              225.0   \n",
       "{'name': 'Algorithm_MEMSET', 'type': 'function'}   2097152              165.0   \n",
       "                                                   1048576              165.0   \n",
       "{'name': 'cudaDeviceSynchronize', 'type': 'func... 2097152              167.0   \n",
       "                                                   1048576              167.0   \n",
       "{'name': 'cudaLaunchKernel', 'type': 'function'}   2097152              166.0   \n",
       "                                                   1048576              166.0   \n",
       "{'name': 'void RAJA::policy::cuda::impl::forall... 2097152              224.0   \n",
       "                                                   1048576              224.0   \n",
       "\n",
       "                                                                            time  \\\n",
       "node                                               ProblemSizeRunParam             \n",
       "{'name': 'RAJAPerf', 'type': 'function'}           2097152              0.000606   \n",
       "                                                   1048576              0.000590   \n",
       "{'name': 'Algorithm', 'type': 'function'}          2097152              0.000023   \n",
       "                                                   1048576              0.000023   \n",
       "{'name': 'Algorithm_MEMCPY', 'type': 'function'}   2097152              0.000017   \n",
       "                                                   1048576              0.000017   \n",
       "{'name': 'cudaDeviceSynchronize', 'type': 'func... 2097152              0.000059   \n",
       "                                                   1048576              0.000038   \n",
       "{'name': 'cudaLaunchKernel', 'type': 'function'}   2097152              0.000032   \n",
       "                                                   1048576              0.000032   \n",
       "{'name': 'void RAJA::policy::cuda::impl::forall... 2097152                   NaN   \n",
       "                                                   1048576                   NaN   \n",
       "{'name': 'Algorithm_MEMSET', 'type': 'function'}   2097152              0.000015   \n",
       "                                                   1048576              0.000015   \n",
       "{'name': 'cudaDeviceSynchronize', 'type': 'func... 2097152              0.000042   \n",
       "                                                   1048576              0.000029   \n",
       "{'name': 'cudaLaunchKernel', 'type': 'function'}   2097152              0.000040   \n",
       "                                                   1048576              0.000033   \n",
       "{'name': 'void RAJA::policy::cuda::impl::forall... 2097152                   NaN   \n",
       "                                                   1048576                   NaN   \n",
       "\n",
       "                                                                        time (gpu)  \\\n",
       "node                                               ProblemSizeRunParam               \n",
       "{'name': 'RAJAPerf', 'type': 'function'}           2097152                     NaN   \n",
       "                                                   1048576                     NaN   \n",
       "{'name': 'Algorithm', 'type': 'function'}          2097152                     NaN   \n",
       "                                                   1048576                     NaN   \n",
       "{'name': 'Algorithm_MEMCPY', 'type': 'function'}   2097152                     NaN   \n",
       "                                                   1048576                     NaN   \n",
       "{'name': 'cudaDeviceSynchronize', 'type': 'func... 2097152                     NaN   \n",
       "                                                   1048576                     NaN   \n",
       "{'name': 'cudaLaunchKernel', 'type': 'function'}   2097152                     NaN   \n",
       "                                                   1048576                     NaN   \n",
       "{'name': 'void RAJA::policy::cuda::impl::forall... 2097152                0.000051   \n",
       "                                                   1048576                0.000030   \n",
       "{'name': 'Algorithm_MEMSET', 'type': 'function'}   2097152                     NaN   \n",
       "                                                   1048576                     NaN   \n",
       "{'name': 'cudaDeviceSynchronize', 'type': 'func... 2097152                     NaN   \n",
       "                                                   1048576                     NaN   \n",
       "{'name': 'cudaLaunchKernel', 'type': 'function'}   2097152                     NaN   \n",
       "                                                   1048576                     NaN   \n",
       "{'name': 'void RAJA::policy::cuda::impl::forall... 2097152                0.000034   \n",
       "                                                   1048576                0.000020   \n",
       "\n",
       "                                                                                                                     name  \\\n",
       "node                                               ProblemSizeRunParam                                                      \n",
       "{'name': 'RAJAPerf', 'type': 'function'}           2097152                                                       RAJAPerf   \n",
       "                                                   1048576                                                       RAJAPerf   \n",
       "{'name': 'Algorithm', 'type': 'function'}          2097152                                                      Algorithm   \n",
       "                                                   1048576                                                      Algorithm   \n",
       "{'name': 'Algorithm_MEMCPY', 'type': 'function'}   2097152                                               Algorithm_MEMCPY   \n",
       "                                                   1048576                                               Algorithm_MEMCPY   \n",
       "{'name': 'cudaDeviceSynchronize', 'type': 'func... 2097152                                          cudaDeviceSynchronize   \n",
       "                                                   1048576                                          cudaDeviceSynchronize   \n",
       "{'name': 'cudaLaunchKernel', 'type': 'function'}   2097152                                               cudaLaunchKernel   \n",
       "                                                   1048576                                               cudaLaunchKernel   \n",
       "{'name': 'void RAJA::policy::cuda::impl::forall... 2097152              void RAJA::policy::cuda::impl::forall_cuda_ker...   \n",
       "                                                   1048576              void RAJA::policy::cuda::impl::forall_cuda_ker...   \n",
       "{'name': 'Algorithm_MEMSET', 'type': 'function'}   2097152                                               Algorithm_MEMSET   \n",
       "                                                   1048576                                               Algorithm_MEMSET   \n",
       "{'name': 'cudaDeviceSynchronize', 'type': 'func... 2097152                                          cudaDeviceSynchronize   \n",
       "                                                   1048576                                          cudaDeviceSynchronize   \n",
       "{'name': 'cudaLaunchKernel', 'type': 'function'}   2097152                                               cudaLaunchKernel   \n",
       "                                                   1048576                                               cudaLaunchKernel   \n",
       "{'name': 'void RAJA::policy::cuda::impl::forall... 2097152              void RAJA::policy::cuda::impl::forall_cuda_ker...   \n",
       "                                                   1048576              void RAJA::policy::cuda::impl::forall_cuda_ker...   \n",
       "\n",
       "                                                                        gpu__time_duration.sum  \\\n",
       "node                                               ProblemSizeRunParam                           \n",
       "{'name': 'RAJAPerf', 'type': 'function'}           2097152                                 NaN   \n",
       "                                                   1048576                                 NaN   \n",
       "{'name': 'Algorithm', 'type': 'function'}          2097152                                 NaN   \n",
       "                                                   1048576                                 NaN   \n",
       "{'name': 'Algorithm_MEMCPY', 'type': 'function'}   2097152                                 NaN   \n",
       "                                                   1048576                                 NaN   \n",
       "{'name': 'cudaDeviceSynchronize', 'type': 'func... 2097152                                 NaN   \n",
       "                                                   1048576                                 NaN   \n",
       "{'name': 'cudaLaunchKernel', 'type': 'function'}   2097152                                 NaN   \n",
       "                                                   1048576                                 NaN   \n",
       "{'name': 'void RAJA::policy::cuda::impl::forall... 2097152                             43424.0   \n",
       "                                                   1048576                             22752.0   \n",
       "{'name': 'Algorithm_MEMSET', 'type': 'function'}   2097152                                 NaN   \n",
       "                                                   1048576                                 NaN   \n",
       "{'name': 'cudaDeviceSynchronize', 'type': 'func... 2097152                                 NaN   \n",
       "                                                   1048576                                 NaN   \n",
       "{'name': 'cudaLaunchKernel', 'type': 'function'}   2097152                                 NaN   \n",
       "                                                   1048576                                 NaN   \n",
       "{'name': 'void RAJA::policy::cuda::impl::forall... 2097152                             31520.0   \n",
       "                                                   1048576                             18080.0   \n",
       "\n",
       "                                                                        sm__throughput.avg.pct_of_peak_sustained_elapsed  \\\n",
       "node                                               ProblemSizeRunParam                                                     \n",
       "{'name': 'RAJAPerf', 'type': 'function'}           2097152                                                           NaN   \n",
       "                                                   1048576                                                           NaN   \n",
       "{'name': 'Algorithm', 'type': 'function'}          2097152                                                           NaN   \n",
       "                                                   1048576                                                           NaN   \n",
       "{'name': 'Algorithm_MEMCPY', 'type': 'function'}   2097152                                                           NaN   \n",
       "                                                   1048576                                                           NaN   \n",
       "{'name': 'cudaDeviceSynchronize', 'type': 'func... 2097152                                                           NaN   \n",
       "                                                   1048576                                                           NaN   \n",
       "{'name': 'cudaLaunchKernel', 'type': 'function'}   2097152                                                           NaN   \n",
       "                                                   1048576                                                           NaN   \n",
       "{'name': 'void RAJA::policy::cuda::impl::forall... 2097152                                                      7.243855   \n",
       "                                                   1048576                                                      6.849288   \n",
       "{'name': 'Algorithm_MEMSET', 'type': 'function'}   2097152                                                           NaN   \n",
       "                                                   1048576                                                           NaN   \n",
       "{'name': 'cudaDeviceSynchronize', 'type': 'func... 2097152                                                           NaN   \n",
       "                                                   1048576                                                           NaN   \n",
       "{'name': 'cudaLaunchKernel', 'type': 'function'}   2097152                                                           NaN   \n",
       "                                                   1048576                                                           NaN   \n",
       "{'name': 'void RAJA::policy::cuda::impl::forall... 2097152                                                      8.552663   \n",
       "                                                   1048576                                                      7.554598   \n",
       "\n",
       "                                                                        smsp__maximum_warps_avg_per_active_cycle  \n",
       "node                                               ProblemSizeRunParam                                            \n",
       "{'name': 'RAJAPerf', 'type': 'function'}           2097152                                                   NaN  \n",
       "                                                   1048576                                                   NaN  \n",
       "{'name': 'Algorithm', 'type': 'function'}          2097152                                                   NaN  \n",
       "                                                   1048576                                                   NaN  \n",
       "{'name': 'Algorithm_MEMCPY', 'type': 'function'}   2097152                                                   NaN  \n",
       "                                                   1048576                                                   NaN  \n",
       "{'name': 'cudaDeviceSynchronize', 'type': 'func... 2097152                                                   NaN  \n",
       "                                                   1048576                                                   NaN  \n",
       "{'name': 'cudaLaunchKernel', 'type': 'function'}   2097152                                                   NaN  \n",
       "                                                   1048576                                                   NaN  \n",
       "{'name': 'void RAJA::policy::cuda::impl::forall... 2097152                                                  16.0  \n",
       "                                                   1048576                                                  16.0  \n",
       "{'name': 'Algorithm_MEMSET', 'type': 'function'}   2097152                                                   NaN  \n",
       "                                                   1048576                                                   NaN  \n",
       "{'name': 'cudaDeviceSynchronize', 'type': 'func... 2097152                                                   NaN  \n",
       "                                                   1048576                                                   NaN  \n",
       "{'name': 'cudaLaunchKernel', 'type': 'function'}   2097152                                                   NaN  \n",
       "                                                   1048576                                                   NaN  \n",
       "{'name': 'void RAJA::policy::cuda::impl::forall... 2097152                                                  16.0  \n",
       "                                                   1048576                                                  16.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk_cap.metadata_column_to_perfdata(\"ProblemSizeRunParam\")\n",
    "tk_cap.dataframe = tk_cap.dataframe.reset_index().set_index([\"node\", \"ProblemSizeRunParam\"])\n",
    "tk_cap.dataframe.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69ba22d",
   "metadata": {},
   "source": [
    "## 6. Visualize the NCU Performance Data on the Calltree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "877c1044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  _____ _     _      _        _   \n",
      " |_   _| |__ (_) ___| | _____| |_ \n",
      "   | | | '_ \\| |/ __| |/ / _ \\ __|\n",
      "   | | | | | | | (__|   <  __/ |_ \n",
      "   |_| |_| |_|_|\\___|_|\\_\\___|\\__|  v2024.1.0\n",
      "\n",
      "\u001b[34mnan\u001b[0m RAJAPerf\u001b[0m\n",
      "├─ \u001b[34mnan\u001b[0m Algorithm\u001b[0m\n",
      "│  ├─ \u001b[34mnan\u001b[0m Algorithm_MEMCPY\u001b[0m\n",
      "│  │  ├─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│  │  └─ \u001b[34mnan\u001b[0m cudaLaunchKernel\u001b[0m\n",
      "│  │     └─ \u001b[38;5;34m7.244\u001b[0m void RAJA::policy::cuda::impl::forall_cuda_kernel<RAJA::policy::cuda::cuda_exec_explicit<RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true>, 1ul, RAJA::Iterators::numeric_iterator<long, long, long*>, void rajaperf::algorithm::MEMCPY::runCudaVariantBlock<128ul>(rajaperf::VariantID)::{lambda(long)#2}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, 128ul>(void rajaperf::algorithm::MEMCPY::runCudaVariantBlock<128ul>(rajaperf::VariantID)::{lambda(long)#2}, RAJA::Iterators::numeric_iterator<long, long, long*>, long)\u001b[0m\n",
      "│  └─ \u001b[34mnan\u001b[0m Algorithm_MEMSET\u001b[0m\n",
      "│     ├─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│     └─ \u001b[34mnan\u001b[0m cudaLaunchKernel\u001b[0m\n",
      "│        └─ \u001b[38;5;34m8.553\u001b[0m void RAJA::policy::cuda::impl::forall_cuda_kernel<RAJA::policy::cuda::cuda_exec_explicit<RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true>, 1ul, RAJA::Iterators::numeric_iterator<long, long, long*>, void rajaperf::algorithm::MEMSET::runCudaVariantBlock<128ul>(rajaperf::VariantID)::{lambda(long)#2}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, 128ul>(void rajaperf::algorithm::MEMSET::runCudaVariantBlock<128ul>(rajaperf::VariantID)::{lambda(long)#2}, RAJA::Iterators::numeric_iterator<long, long, long*>, long)\u001b[0m\n",
      "├─ \u001b[34mnan\u001b[0m Apps\u001b[0m\n",
      "│  ├─ \u001b[34mnan\u001b[0m Apps_DEL_DOT_VEC_2D\u001b[0m\n",
      "│  │  ├─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│  │  └─ \u001b[34mnan\u001b[0m cudaLaunchKernel\u001b[0m\n",
      "│  │     └─ \u001b[38;5;220m27.251\u001b[0m void RAJA::policy::cuda::impl::forall_cuda_kernel<RAJA::policy::cuda::cuda_exec_explicit<RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true>, 1ul, long*, void rajaperf::apps::DEL_DOT_VEC_2D::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#2}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, 128ul>(void rajaperf::apps::DEL_DOT_VEC_2D::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#2}, long*, long)\u001b[0m\n",
      "│  ├─ \u001b[34mnan\u001b[0m Apps_EDGE3D\u001b[0m\n",
      "│  │  └─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│  ├─ \u001b[34mnan\u001b[0m Apps_ENERGY\u001b[0m\n",
      "│  │  ├─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│  │  └─ \u001b[34mnan\u001b[0m cudaLaunchKernel\u001b[0m\n",
      "│  │     ├─ \u001b[38;5;22m4.543\u001b[0m void RAJA::policy::cuda::impl::forall_cuda_kernel<RAJA::policy::cuda::cuda_exec_explicit<RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true>, 1ul, RAJA::Iterators::numeric_iterator<long, long, long*>, void rajaperf::apps::ENERGY::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(long)#1}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, 128ul>(void rajaperf::apps::ENERGY::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(long)#1}, RAJA::Iterators::numeric_iterator<long, long, long*>, long)\u001b[0m\n",
      "│  │     ├─ \u001b[38;5;34m8.279\u001b[0m void RAJA::policy::cuda::impl::forall_cuda_kernel<RAJA::policy::cuda::cuda_exec_explicit<RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true>, 1ul, RAJA::Iterators::numeric_iterator<long, long, long*>, void rajaperf::apps::ENERGY::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(long)#2}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, 128ul>(void rajaperf::apps::ENERGY::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(long)#2}, RAJA::Iterators::numeric_iterator<long, long, long*>, long)\u001b[0m\n",
      "│  │     ├─ \u001b[38;5;22m4.247\u001b[0m void RAJA::policy::cuda::impl::forall_cuda_kernel<RAJA::policy::cuda::cuda_exec_explicit<RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true>, 1ul, RAJA::Iterators::numeric_iterator<long, long, long*>, void rajaperf::apps::ENERGY::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(long)#3}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, 128ul>(void rajaperf::apps::ENERGY::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(long)#3}, RAJA::Iterators::numeric_iterator<long, long, long*>, long)\u001b[0m\n",
      "│  │     ├─ \u001b[38;5;34m6.867\u001b[0m void RAJA::policy::cuda::impl::forall_cuda_kernel<RAJA::policy::cuda::cuda_exec_explicit<RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true>, 1ul, RAJA::Iterators::numeric_iterator<long, long, long*>, void rajaperf::apps::ENERGY::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(long)#4}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, 128ul>(void rajaperf::apps::ENERGY::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(long)#4}, RAJA::Iterators::numeric_iterator<long, long, long*>, long)\u001b[0m\n",
      "│  │     ├─ \u001b[38;5;34m7.101\u001b[0m void RAJA::policy::cuda::impl::forall_cuda_kernel<RAJA::policy::cuda::cuda_exec_explicit<RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true>, 1ul, RAJA::Iterators::numeric_iterator<long, long, long*>, void rajaperf::apps::ENERGY::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(long)#5}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, 128ul>(void rajaperf::apps::ENERGY::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(long)#5}, RAJA::Iterators::numeric_iterator<long, long, long*>, long)\u001b[0m\n",
      "│  │     └─ \u001b[38;5;34m9.109\u001b[0m void RAJA::policy::cuda::impl::forall_cuda_kernel<RAJA::policy::cuda::cuda_exec_explicit<RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true>, 1ul, RAJA::Iterators::numeric_iterator<long, long, long*>, void rajaperf::apps::ENERGY::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(long)#6}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, 128ul>(void rajaperf::apps::ENERGY::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(long)#6}, RAJA::Iterators::numeric_iterator<long, long, long*>, long)\u001b[0m\n",
      "│  ├─ \u001b[34mnan\u001b[0m Apps_FIR\u001b[0m\n",
      "│  │  ├─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│  │  └─ \u001b[34mnan\u001b[0m cudaLaunchKernel\u001b[0m\n",
      "│  │     └─ \u001b[38;5;196m45.784\u001b[0m void RAJA::policy::cuda::impl::forall_cuda_kernel<RAJA::policy::cuda::cuda_exec_explicit<RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true>, 1ul, RAJA::Iterators::numeric_iterator<long, long, long*>, void rajaperf::apps::FIR::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#1}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, 128ul>(void rajaperf::apps::FIR::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#1}, RAJA::Iterators::numeric_iterator<long, long, long*>, long)\u001b[0m\n",
      "│  ├─ \u001b[34mnan\u001b[0m Apps_LTIMES\u001b[0m\n",
      "│  │  └─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│  ├─ \u001b[34mnan\u001b[0m Apps_LTIMES_NOVIEW\u001b[0m\n",
      "│  │  └─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│  ├─ \u001b[34mnan\u001b[0m Apps_MATVEC_3D_STENCIL\u001b[0m\n",
      "│  │  ├─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│  │  └─ \u001b[34mnan\u001b[0m cudaLaunchKernel\u001b[0m\n",
      "│  │     └─ \u001b[38;5;34m9.664\u001b[0m void RAJA::policy::cuda::impl::forall_cuda_kernel<RAJA::policy::cuda::cuda_exec_explicit<RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true>, 1ul, long*, void rajaperf::apps::MATVEC_3D_STENCIL::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#1}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, 128ul>(void rajaperf::apps::MATVEC_3D_STENCIL::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#1}, long*, long)\u001b[0m\n",
      "│  ├─ \u001b[34mnan\u001b[0m Apps_NODAL_ACCUMULATION_3D\u001b[0m\n",
      "│  │  ├─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│  │  └─ \u001b[34mnan\u001b[0m cudaLaunchKernel\u001b[0m\n",
      "│  │     └─ \u001b[38;5;34m7.376\u001b[0m void RAJA::policy::cuda::impl::forall_cuda_kernel<RAJA::policy::cuda::cuda_exec_explicit<RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true>, 1ul, long*, void rajaperf::apps::NODAL_ACCUMULATION_3D::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#1}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, 128ul>(void rajaperf::apps::NODAL_ACCUMULATION_3D::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#1}, long*, long)\u001b[0m\n",
      "│  ├─ \u001b[34mnan\u001b[0m Apps_PRESSURE\u001b[0m\n",
      "│  │  ├─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│  │  └─ \u001b[34mnan\u001b[0m cudaLaunchKernel\u001b[0m\n",
      "│  │     ├─ \u001b[38;5;34m8.256\u001b[0m void RAJA::policy::cuda::impl::forall_cuda_kernel<RAJA::policy::cuda::cuda_exec_explicit<RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true>, 1ul, RAJA::Iterators::numeric_iterator<long, long, long*>, void rajaperf::apps::PRESSURE::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(long)#1}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, 128ul>(void rajaperf::apps::PRESSURE::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(long)#1}, RAJA::Iterators::numeric_iterator<long, long, long*>, long)\u001b[0m\n",
      "│  │     └─ \u001b[38;5;34m7.427\u001b[0m void RAJA::policy::cuda::impl::forall_cuda_kernel<RAJA::policy::cuda::cuda_exec_explicit<RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true>, 1ul, RAJA::Iterators::numeric_iterator<long, long, long*>, void rajaperf::apps::PRESSURE::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(long)#2}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, 128ul>(void rajaperf::apps::PRESSURE::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(long)#2}, RAJA::Iterators::numeric_iterator<long, long, long*>, long)\u001b[0m\n",
      "│  ├─ \u001b[34mnan\u001b[0m Apps_VOL3D\u001b[0m\n",
      "│  │  ├─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│  │  └─ \u001b[34mnan\u001b[0m cudaLaunchKernel\u001b[0m\n",
      "│  │     └─ \u001b[38;5;208m39.563\u001b[0m void RAJA::policy::cuda::impl::forall_cuda_kernel<RAJA::policy::cuda::cuda_exec_explicit<RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true>, 1ul, RAJA::Iterators::numeric_iterator<long, long, long*>, void rajaperf::apps::VOL3D::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#1}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, 128ul>(void rajaperf::apps::VOL3D::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#1}, RAJA::Iterators::numeric_iterator<long, long, long*>, long)\u001b[0m\n",
      "│  └─ \u001b[34mnan\u001b[0m Apps_ZONAL_ACCUMULATION_3D\u001b[0m\n",
      "│     ├─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│     └─ \u001b[34mnan\u001b[0m cudaLaunchKernel\u001b[0m\n",
      "│        └─ \u001b[38;5;34m11.433\u001b[0m void RAJA::policy::cuda::impl::forall_cuda_kernel<RAJA::policy::cuda::cuda_exec_explicit<RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true>, 1ul, long*, void rajaperf::apps::ZONAL_ACCUMULATION_3D::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#1}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, 128ul>(void rajaperf::apps::ZONAL_ACCUMULATION_3D::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#1}, long*, long)\u001b[0m\n",
      "├─ \u001b[34mnan\u001b[0m Basic\u001b[0m\n",
      "│  ├─ \u001b[34mnan\u001b[0m Basic_ARRAY_OF_PTRS\u001b[0m\n",
      "│  │  └─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│  ├─ \u001b[34mnan\u001b[0m Basic_COPY8\u001b[0m\n",
      "│  │  └─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│  ├─ \u001b[34mnan\u001b[0m Basic_DAXPY\u001b[0m\n",
      "│  │  ├─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│  │  └─ \u001b[34mnan\u001b[0m cudaLaunchKernel\u001b[0m\n",
      "│  │     └─ \u001b[38;5;34m5.766\u001b[0m void RAJA::policy::cuda::impl::forall_cuda_kernel<RAJA::policy::cuda::cuda_exec_explicit<RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true>, 1ul, RAJA::Iterators::numeric_iterator<long, long, long*>, void rajaperf::basic::DAXPY::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#2}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, 128ul>(void rajaperf::basic::DAXPY::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#2}, RAJA::Iterators::numeric_iterator<long, long, long*>, long)\u001b[0m\n",
      "│  ├─ \u001b[34mnan\u001b[0m Basic_DAXPY_ATOMIC\u001b[0m\n",
      "│  │  ├─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│  │  └─ \u001b[34mnan\u001b[0m cudaLaunchKernel\u001b[0m\n",
      "│  │     └─ \u001b[38;5;34m5.511\u001b[0m void RAJA::policy::cuda::impl::forall_cuda_kernel<RAJA::policy::cuda::cuda_exec_explicit<RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true>, 1ul, RAJA::Iterators::numeric_iterator<long, long, long*>, void rajaperf::basic::DAXPY_ATOMIC::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#2}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, 128ul>(void rajaperf::basic::DAXPY_ATOMIC::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#2}, RAJA::Iterators::numeric_iterator<long, long, long*>, long)\u001b[0m\n",
      "│  ├─ \u001b[34mnan\u001b[0m Basic_IF_QUAD\u001b[0m\n",
      "│  │  ├─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│  │  └─ \u001b[34mnan\u001b[0m cudaLaunchKernel\u001b[0m\n",
      "│  │     └─ \u001b[38;5;34m11.947\u001b[0m void RAJA::policy::cuda::impl::forall_cuda_kernel<RAJA::policy::cuda::cuda_exec_explicit<RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true>, 1ul, RAJA::Iterators::numeric_iterator<long, long, long*>, void rajaperf::basic::IF_QUAD::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#2}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, 128ul>(void rajaperf::basic::IF_QUAD::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#2}, RAJA::Iterators::numeric_iterator<long, long, long*>, long)\u001b[0m\n",
      "│  ├─ \u001b[34mnan\u001b[0m Basic_INIT3\u001b[0m\n",
      "│  │  ├─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│  │  └─ \u001b[34mnan\u001b[0m cudaLaunchKernel\u001b[0m\n",
      "│  │     └─ \u001b[38;5;22m4.536\u001b[0m void RAJA::policy::cuda::impl::forall_cuda_kernel<RAJA::policy::cuda::cuda_exec_explicit<RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true>, 1ul, RAJA::Iterators::numeric_iterator<long, long, long*>, void rajaperf::basic::INIT3::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#2}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, 128ul>(void rajaperf::basic::INIT3::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#2}, RAJA::Iterators::numeric_iterator<long, long, long*>, long)\u001b[0m\n",
      "│  ├─ \u001b[34mnan\u001b[0m Basic_INIT_VIEW1D\u001b[0m\n",
      "│  │  ├─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│  │  └─ \u001b[34mnan\u001b[0m cudaLaunchKernel\u001b[0m\n",
      "│  │     └─ \u001b[38;5;34m9.915\u001b[0m void RAJA::policy::cuda::impl::forall_cuda_kernel<RAJA::policy::cuda::cuda_exec_explicit<RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true>, 1ul, RAJA::Iterators::numeric_iterator<long, long, long*>, void rajaperf::basic::INIT_VIEW1D::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#2}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, 128ul>(void rajaperf::basic::INIT_VIEW1D::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#2}, RAJA::Iterators::numeric_iterator<long, long, long*>, long)\u001b[0m\n",
      "│  ├─ \u001b[34mnan\u001b[0m Basic_INIT_VIEW1D_OFFSET\u001b[0m\n",
      "│  │  ├─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│  │  └─ \u001b[34mnan\u001b[0m cudaLaunchKernel\u001b[0m\n",
      "│  │     └─ \u001b[38;5;34m11.991\u001b[0m void RAJA::policy::cuda::impl::forall_cuda_kernel<RAJA::policy::cuda::cuda_exec_explicit<RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true>, 1ul, RAJA::Iterators::numeric_iterator<long, long, long*>, void rajaperf::basic::INIT_VIEW1D_OFFSET::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#2}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, 128ul>(void rajaperf::basic::INIT_VIEW1D_OFFSET::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#2}, RAJA::Iterators::numeric_iterator<long, long, long*>, long)\u001b[0m\n",
      "│  ├─ \u001b[34mnan\u001b[0m Basic_MULADDSUB\u001b[0m\n",
      "│  │  ├─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│  │  └─ \u001b[34mnan\u001b[0m cudaLaunchKernel\u001b[0m\n",
      "│  │     └─ \u001b[38;5;34m6.420\u001b[0m void RAJA::policy::cuda::impl::forall_cuda_kernel<RAJA::policy::cuda::cuda_exec_explicit<RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true>, 1ul, RAJA::Iterators::numeric_iterator<long, long, long*>, void rajaperf::basic::MULADDSUB::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#2}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, 128ul>(void rajaperf::basic::MULADDSUB::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#2}, RAJA::Iterators::numeric_iterator<long, long, long*>, long)\u001b[0m\n",
      "│  ├─ \u001b[34mnan\u001b[0m Basic_NESTED_INIT\u001b[0m\n",
      "│  │  ├─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│  │  └─ \u001b[34mnan\u001b[0m cudaLaunchKernel\u001b[0m\n",
      "│  │     └─ \u001b[38;5;220m27.839\u001b[0m void RAJA::internal::CudaKernelLauncherFixed<128, 1, RAJA::internal::LoopData<camp::tuple<RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long>, RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long>, RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long> >, camp::tuple<>, camp::resources::v1::Cuda, void rajaperf::basic::NESTED_INIT::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, long, long)#2}>, RAJA::internal::CudaStatementListExecutor<void rajaperf::basic::NESTED_INIT::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, long, long)#2}, camp::list<RAJA::statement::For<2l, RAJA::policy::cuda::cuda_indexer<RAJA::iteration_mapping::Direct, (RAJA::kernel_sync_requirement)0, RAJA::cuda::IndexGlobal<(RAJA::named_dim)2, -1, 0> >, RAJA::statement<1l, RAJA::policy::cuda<RAJA::iteration_mapping, (RAJA::iteration_mapping::Direct)0, RAJA::cuda<(RAJA::cuda::IndexGlobal)1, 4, 0> >, RAJA::statement<0l, RAJA::policy::cuda<RAJA::iteration_mapping, (RAJA::iteration_mapping::Direct)0, RAJA::cuda<(RAJA::cuda::IndexGlobal)0, 32, 0> >, camp::list::Lambda<0l> > > > >, RAJA::internal::LoopTypes<RAJA::internal::CudaStatementListExecutor<void, void, void>, RAJA::internal::LoopTypes> > >(RAJA::internal::LoopData<camp::tuple<RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long>, RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long>, RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long> >, camp::tuple<>, camp::resources::v1::Cuda, void rajaperf::basic::NESTED_INIT::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, long, long)#2}>)\u001b[0m\n",
      "│  └─ \u001b[34mnan\u001b[0m Basic_PI_ATOMIC\u001b[0m\n",
      "│     └─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "├─ \u001b[34mnan\u001b[0m Comm\u001b[0m\n",
      "│  └─ \u001b[34mnan\u001b[0m Comm_HALO_PACKING\u001b[0m\n",
      "│     ├─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│     ├─ \u001b[34mnan\u001b[0m cudaLaunchKernel\u001b[0m\n",
      "│     │  ├─ \u001b[38;5;22m0.142\u001b[0m void RAJA::policy::cuda::impl::forall_cuda_kernel<RAJA::policy::cuda::cuda_exec_explicit<RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true>, 1ul, RAJA::Iterators::numeric_iterator<long, long, long*>, void rajaperf::comm::HALO_PACKING::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#1}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, 128ul>(void rajaperf::comm::HALO_PACKING::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#1}, RAJA::Iterators::numeric_iterator<long, long, long*>, long)\u001b[0m\n",
      "│     │  └─ \u001b[38;5;22m0.167\u001b[0m void RAJA::policy::cuda::impl::forall_cuda_kernel<RAJA::policy::cuda::cuda_exec_explicit<RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true>, 1ul, RAJA::Iterators::numeric_iterator<long, long, long*>, void rajaperf::comm::HALO_PACKING::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#2}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, 128ul>(void rajaperf::comm::HALO_PACKING::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#2}, RAJA::Iterators::numeric_iterator<long, long, long*>, long)\u001b[0m\n",
      "│     └─ \u001b[34mnan\u001b[0m cudaStreamSynchronize\u001b[0m\n",
      "├─ \u001b[34mnan\u001b[0m Lcals\u001b[0m\n",
      "│  ├─ \u001b[34mnan\u001b[0m Lcals_DIFF_PREDICT\u001b[0m\n",
      "│  │  ├─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│  │  └─ \u001b[34mnan\u001b[0m cudaLaunchKernel\u001b[0m\n",
      "│  │     └─ \u001b[38;5;22m2.247\u001b[0m void RAJA::policy::cuda::impl::forall_cuda_kernel<RAJA::policy::cuda::cuda_exec_explicit<RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true>, 1ul, RAJA::Iterators::numeric_iterator<long, long, long*>, void rajaperf::lcals::DIFF_PREDICT::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#1}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, 128ul>(void rajaperf::lcals::DIFF_PREDICT::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#1}, RAJA::Iterators::numeric_iterator<long, long, long*>, long)\u001b[0m\n",
      "│  ├─ \u001b[34mnan\u001b[0m Lcals_EOS\u001b[0m\n",
      "│  │  ├─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│  │  └─ \u001b[34mnan\u001b[0m cudaLaunchKernel\u001b[0m\n",
      "│  │     └─ \u001b[38;5;34m9.165\u001b[0m void RAJA::policy::cuda::impl::forall_cuda_kernel<RAJA::policy::cuda::cuda_exec_explicit<RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true>, 1ul, RAJA::Iterators::numeric_iterator<long, long, long*>, void rajaperf::lcals::EOS::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#1}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, 128ul>(void rajaperf::lcals::EOS::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#1}, RAJA::Iterators::numeric_iterator<long, long, long*>, long)\u001b[0m\n",
      "│  ├─ \u001b[34mnan\u001b[0m Lcals_FIRST_DIFF\u001b[0m\n",
      "│  │  ├─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│  │  └─ \u001b[34mnan\u001b[0m cudaLaunchKernel\u001b[0m\n",
      "│  │     └─ \u001b[38;5;34m8.252\u001b[0m void RAJA::policy::cuda::impl::forall_cuda_kernel<RAJA::policy::cuda::cuda_exec_explicit<RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true>, 1ul, RAJA::Iterators::numeric_iterator<long, long, long*>, void rajaperf::lcals::FIRST_DIFF::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#1}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, 128ul>(void rajaperf::lcals::FIRST_DIFF::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#1}, RAJA::Iterators::numeric_iterator<long, long, long*>, long)\u001b[0m\n",
      "│  ├─ \u001b[34mnan\u001b[0m Lcals_FIRST_SUM\u001b[0m\n",
      "│  │  ├─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│  │  └─ \u001b[34mnan\u001b[0m cudaLaunchKernel\u001b[0m\n",
      "│  │     └─ \u001b[38;5;34m7.818\u001b[0m void RAJA::policy::cuda::impl::forall_cuda_kernel<RAJA::policy::cuda::cuda_exec_explicit<RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true>, 1ul, RAJA::Iterators::numeric_iterator<long, long, long*>, void rajaperf::lcals::FIRST_SUM::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#1}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, 128ul>(void rajaperf::lcals::FIRST_SUM::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#1}, RAJA::Iterators::numeric_iterator<long, long, long*>, long)\u001b[0m\n",
      "│  ├─ \u001b[34mnan\u001b[0m Lcals_GEN_LIN_RECUR\u001b[0m\n",
      "│  │  ├─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│  │  └─ \u001b[34mnan\u001b[0m cudaLaunchKernel\u001b[0m\n",
      "│  │     ├─ \u001b[38;5;34m4.975\u001b[0m void RAJA::policy::cuda::impl::forall_cuda_kernel<RAJA::policy::cuda::cuda_exec_explicit<RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true>, 1ul, RAJA::Iterators::numeric_iterator<long, long, long*>, void rajaperf::lcals::GEN_LIN_RECUR::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#1}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, 128ul>(void rajaperf::lcals::GEN_LIN_RECUR::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#1}, RAJA::Iterators::numeric_iterator<long, long, long*>, long)\u001b[0m\n",
      "│  │     └─ \u001b[38;5;34m5.284\u001b[0m void RAJA::policy::cuda::impl::forall_cuda_kernel<RAJA::policy::cuda::cuda_exec_explicit<RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true>, 1ul, RAJA::Iterators::numeric_iterator<long, long, long*>, void rajaperf::lcals::GEN_LIN_RECUR::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#2}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, 128ul>(void rajaperf::lcals::GEN_LIN_RECUR::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#2}, RAJA::Iterators::numeric_iterator<long, long, long*>, long)\u001b[0m\n",
      "│  ├─ \u001b[34mnan\u001b[0m Lcals_HYDRO_1D\u001b[0m\n",
      "│  │  ├─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│  │  └─ \u001b[34mnan\u001b[0m cudaLaunchKernel\u001b[0m\n",
      "│  │     └─ \u001b[38;5;34m7.053\u001b[0m void RAJA::policy::cuda::impl::forall_cuda_kernel<RAJA::policy::cuda::cuda_exec_explicit<RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true>, 1ul, RAJA::Iterators::numeric_iterator<long, long, long*>, void rajaperf::lcals::HYDRO_1D::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#1}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, 128ul>(void rajaperf::lcals::HYDRO_1D::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#1}, RAJA::Iterators::numeric_iterator<long, long, long*>, long)\u001b[0m\n",
      "│  ├─ \u001b[34mnan\u001b[0m Lcals_HYDRO_2D\u001b[0m\n",
      "│  │  ├─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│  │  └─ \u001b[34mnan\u001b[0m cudaLaunchKernel\u001b[0m\n",
      "│  │     ├─ \u001b[38;5;46m16.915\u001b[0m void RAJA::internal::CudaKernelLauncherFixed<128, 1, RAJA::internal::LoopData<camp::tuple<RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long>, RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long> >, camp::tuple<>, camp::resources::v1::Cuda, void rajaperf::lcals::HYDRO_2D::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, long)#1}>, RAJA::internal::CudaStatementListExecutor<void rajaperf::lcals::HYDRO_2D::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, long)#1}, camp::list<RAJA::statement::For<0l, RAJA::policy::cuda::cuda_indexer<RAJA::iteration_mapping::Direct, (RAJA::kernel_sync_requirement)0, RAJA::cuda::IndexGlobal<(RAJA::named_dim)1, 4, 0> >, RAJA::statement<1l, RAJA::policy::cuda<RAJA::iteration_mapping, (RAJA::iteration_mapping::Direct)0, RAJA::cuda<(RAJA::cuda::IndexGlobal)0, 32, 0> >, camp::list::Lambda<0l> > > >, RAJA::internal::LoopTypes<RAJA::internal::CudaStatementListExecutor<void, void>, RAJA::internal::LoopTypes> > >(RAJA::internal::LoopData<camp::tuple<RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long>, RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long> >, camp::tuple<>, camp::resources::v1::Cuda, void rajaperf::lcals::HYDRO_2D::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, long)#1}>)\u001b[0m\n",
      "│  │     ├─ \u001b[38;5;34m12.606\u001b[0m void RAJA::internal::CudaKernelLauncherFixed<128, 1, RAJA::internal::LoopData<camp::tuple<RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long>, RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long> >, camp::tuple<>, camp::resources::v1::Cuda, void rajaperf::lcals::HYDRO_2D::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, long)#2}>, RAJA::internal::CudaStatementListExecutor<void rajaperf::lcals::HYDRO_2D::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, long)#2}, camp::list<RAJA::statement::For<0l, RAJA::policy::cuda::cuda_indexer<RAJA::iteration_mapping::Direct, (RAJA::kernel_sync_requirement)0, RAJA::cuda::IndexGlobal<(RAJA::named_dim)1, 4, 0> >, RAJA::statement<1l, RAJA::policy::cuda<RAJA::iteration_mapping, (RAJA::iteration_mapping::Direct)0, RAJA::cuda<(RAJA::cuda::IndexGlobal)0, 32, 0> >, camp::list::Lambda<0l> > > >, RAJA::internal::LoopTypes<RAJA::internal::CudaStatementListExecutor<void, void>, RAJA::internal::LoopTypes> > >(RAJA::internal::LoopData<camp::tuple<RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long>, RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long> >, camp::tuple<>, camp::resources::v1::Cuda, void rajaperf::lcals::HYDRO_2D::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, long)#2}>)\u001b[0m\n",
      "│  │     └─ \u001b[38;5;34m9.108\u001b[0m void RAJA::internal::CudaKernelLauncherFixed<128, 1, RAJA::internal::LoopData<camp::tuple<RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long>, RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long> >, camp::tuple<>, camp::resources::v1::Cuda, void rajaperf::lcals::HYDRO_2D::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, long)#3}>, RAJA::internal::CudaStatementListExecutor<void rajaperf::lcals::HYDRO_2D::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, long)#3}, camp::list<RAJA::statement::For<0l, RAJA::policy::cuda::cuda_indexer<RAJA::iteration_mapping::Direct, (RAJA::kernel_sync_requirement)0, RAJA::cuda::IndexGlobal<(RAJA::named_dim)1, 4, 0> >, RAJA::statement<1l, RAJA::policy::cuda<RAJA::iteration_mapping, (RAJA::iteration_mapping::Direct)0, RAJA::cuda<(RAJA::cuda::IndexGlobal)0, 32, 0> >, camp::list::Lambda<0l> > > >, RAJA::internal::LoopTypes<RAJA::internal::CudaStatementListExecutor<void, void>, RAJA::internal::LoopTypes> > >(RAJA::internal::LoopData<camp::tuple<RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long>, RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long> >, camp::tuple<>, camp::resources::v1::Cuda, void rajaperf::lcals::HYDRO_2D::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, long)#3}>)\u001b[0m\n",
      "│  ├─ \u001b[34mnan\u001b[0m Lcals_INT_PREDICT\u001b[0m\n",
      "│  │  ├─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│  │  └─ \u001b[34mnan\u001b[0m cudaLaunchKernel\u001b[0m\n",
      "│  │     └─ \u001b[38;5;34m5.662\u001b[0m void RAJA::policy::cuda::impl::forall_cuda_kernel<RAJA::policy::cuda::cuda_exec_explicit<RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true>, 1ul, RAJA::Iterators::numeric_iterator<long, long, long*>, void rajaperf::lcals::INT_PREDICT::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#1}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, 128ul>(void rajaperf::lcals::INT_PREDICT::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#1}, RAJA::Iterators::numeric_iterator<long, long, long*>, long)\u001b[0m\n",
      "│  ├─ \u001b[34mnan\u001b[0m Lcals_PLANCKIAN\u001b[0m\n",
      "│  │  └─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│  └─ \u001b[34mnan\u001b[0m Lcals_TRIDIAG_ELIM\u001b[0m\n",
      "│     ├─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│     └─ \u001b[34mnan\u001b[0m cudaLaunchKernel\u001b[0m\n",
      "│        └─ \u001b[38;5;34m5.353\u001b[0m void RAJA::policy::cuda::impl::forall_cuda_kernel<RAJA::policy::cuda::cuda_exec_explicit<RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true>, 1ul, RAJA::Iterators::numeric_iterator<long, long, long*>, void rajaperf::lcals::TRIDIAG_ELIM::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#1}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, 128ul>(void rajaperf::lcals::TRIDIAG_ELIM::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#1}, RAJA::Iterators::numeric_iterator<long, long, long*>, long)\u001b[0m\n",
      "├─ \u001b[34mnan\u001b[0m Polybench\u001b[0m\n",
      "│  ├─ \u001b[34mnan\u001b[0m Polybench_2MM\u001b[0m\n",
      "│  │  └─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│  ├─ \u001b[34mnan\u001b[0m Polybench_3MM\u001b[0m\n",
      "│  │  └─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│  ├─ \u001b[34mnan\u001b[0m Polybench_ADI\u001b[0m\n",
      "│  │  └─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│  ├─ \u001b[34mnan\u001b[0m Polybench_ATAX\u001b[0m\n",
      "│  │  ├─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│  │  └─ \u001b[34mnan\u001b[0m cudaLaunchKernel\u001b[0m\n",
      "│  │     ├─ \u001b[38;5;22m0.742\u001b[0m void RAJA::internal::CudaKernelLauncherFixed<128, 1, RAJA::internal::LoopData<camp::tuple<RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long>, RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long> >, camp::tuple<double>, camp::resources::v1::Cuda, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#1}, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, long, double&)#1}, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#2}>, RAJA::internal::CudaStatementListExecutor<void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, long, double&)#1}, camp::list<RAJA::statement::For<0l, RAJA::policy::cuda::cuda_indexer<RAJA::iteration_mapping::Direct, (RAJA::kernel_sync_requirement)0, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0> >, RAJA::internal::LoopData<camp::tuple<RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long>, RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long> >, camp::tuple<double>, camp::resources::v1::Cuda, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#1}, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, long, double&)#1}, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#2}>::Lambda<0l, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#2}<RAJA::internal::LambdaArg<RAJA::internal::lambda_arg_seg_t, 0l> >, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#2}<RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0><RAJA::internal::lambda_arg_param_t, 0l> > >, RAJA::internal::CudaStatementListExecutor<1l, camp::list::sequential::seq_exec, RAJA::named_dim<1l, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#2}<RAJA::internal::LoopData<camp::tuple<RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long>, RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long> >, camp::tuple<double>, camp::resources::v1::Cuda, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#1}, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, long, double&)#1}, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#2}>::Lambda, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0><RAJA::policy::cuda::cuda_indexer<RAJA::iteration_mapping::Direct, (RAJA::kernel_sync_requirement)0, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0> >, 1l> >, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#2}<RAJA::internal::LambdaArg<RAJA::internal::lambda_arg_seg_t, 0l> > > >, RAJA::named_dim<2l, RAJA::internal::LambdaArg, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#2}<RAJA::internal::LambdaArg<RAJA::internal::lambda_arg_seg_t, 0l> > > > >, RAJA::internal::LoopTypes<void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#2}<void, void>, RAJA::statement::For<0l, RAJA::policy::cuda::cuda_indexer<RAJA::iteration_mapping::Direct, (RAJA::kernel_sync_requirement)0, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0> >, RAJA::internal::LoopData<camp::tuple<RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long>, RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long> >, camp::tuple<double>, camp::resources::v1::Cuda, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#1}, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, long, double&)#1}, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#2}>::Lambda<0l, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#2}<RAJA::internal::LambdaArg<RAJA::internal::lambda_arg_seg_t, 0l> >, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#2}<RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0><RAJA::internal::lambda_arg_param_t, 0l> > >, RAJA::internal::CudaStatementListExecutor<1l, camp::list::sequential::seq_exec, RAJA::named_dim<1l, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#2}<RAJA::internal::LoopData<camp::tuple<RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long>, RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long> >, camp::tuple<double>, camp::resources::v1::Cuda, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#1}, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, long, double&)#1}, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#2}>::Lambda, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0><RAJA::policy::cuda::cuda_indexer<RAJA::iteration_mapping::Direct, (RAJA::kernel_sync_requirement)0, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0> >, 1l> >, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#2}<RAJA::internal::LambdaArg<RAJA::internal::lambda_arg_seg_t, 0l> > > >, RAJA::named_dim<2l, RAJA::internal::LambdaArg, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#2}<RAJA::internal::LambdaArg<RAJA::internal::lambda_arg_seg_t, 0l> > > > > > >(RAJA::internal::LoopData<camp::tuple<RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long>, RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long> >, camp::tuple<double>, camp::resources::v1::Cuda, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#1}, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, long, double&)#1}, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#2}>)\u001b[0m\n",
      "│  │     └─ \u001b[38;5;22m1.960\u001b[0m void RAJA::internal::CudaKernelLauncherFixed<128, 1, RAJA::internal::LoopData<camp::tuple<RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long>, RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long> >, camp::tuple<double>, camp::resources::v1::Cuda, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#3}, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, long, double&)#2}, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#4}>, RAJA::internal::CudaStatementListExecutor<void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, long, double&)#2}, camp::list<RAJA::statement::For<1l, RAJA::policy::cuda::cuda_indexer<RAJA::iteration_mapping::Direct, (RAJA::kernel_sync_requirement)0, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0> >, RAJA::internal::LoopData<camp::tuple<RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long>, RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long> >, camp::tuple<double>, camp::resources::v1::Cuda, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#3}, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, long, double&)#2}, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#4}>::Lambda<0l, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#4}<RAJA::internal::LambdaArg<RAJA::internal::lambda_arg_seg_t, 1l> >, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#4}<RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0><RAJA::internal::lambda_arg_param_t, 0l> > >, RAJA::internal::CudaStatementListExecutor<0l, camp::list::sequential::seq_exec, RAJA::named_dim<1l, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#4}<RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0><RAJA::policy::cuda::cuda_indexer<RAJA::iteration_mapping::Direct, (RAJA::kernel_sync_requirement)0, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0> >, 0l>, RAJA::internal::LoopData<camp::tuple<RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long>, RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long> >, camp::tuple<double>, camp::resources::v1::Cuda, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#3}, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, long, double&)#2}, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#4}>::Lambda>, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#4}<RAJA::internal::LambdaArg<RAJA::internal::lambda_arg_seg_t, 1l> > > >, RAJA::named_dim<2l, RAJA::internal::LambdaArg, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#4}<RAJA::internal::LambdaArg<RAJA::internal::lambda_arg_seg_t, 1l> > > > >, RAJA::internal::LoopTypes<void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#4}<void, void>, RAJA::statement::For<1l, RAJA::policy::cuda::cuda_indexer<RAJA::iteration_mapping::Direct, (RAJA::kernel_sync_requirement)0, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0> >, RAJA::internal::LoopData<camp::tuple<RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long>, RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long> >, camp::tuple<double>, camp::resources::v1::Cuda, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#3}, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, long, double&)#2}, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#4}>::Lambda<0l, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#4}<RAJA::internal::LambdaArg<RAJA::internal::lambda_arg_seg_t, 1l> >, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#4}<RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0><RAJA::internal::lambda_arg_param_t, 0l> > >, RAJA::internal::CudaStatementListExecutor<0l, camp::list::sequential::seq_exec, RAJA::named_dim<1l, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#4}<RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0><RAJA::policy::cuda::cuda_indexer<RAJA::iteration_mapping::Direct, (RAJA::kernel_sync_requirement)0, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0> >, 0l>, RAJA::internal::LoopData<camp::tuple<RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long>, RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long> >, camp::tuple<double>, camp::resources::v1::Cuda, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#3}, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, long, double&)#2}, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#4}>::Lambda>, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#4}<RAJA::internal::LambdaArg<RAJA::internal::lambda_arg_seg_t, 1l> > > >, RAJA::named_dim<2l, RAJA::internal::LambdaArg, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#4}<RAJA::internal::LambdaArg<RAJA::internal::lambda_arg_seg_t, 1l> > > > > > >(RAJA::internal::LoopData<camp::tuple<RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long>, RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long> >, camp::tuple<double>, camp::resources::v1::Cuda, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#3}, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, long, double&)#2}, void rajaperf::polybench::POLYBENCH_ATAX::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&)#4}>)\u001b[0m\n",
      "│  ├─ \u001b[34mnan\u001b[0m Polybench_FDTD_2D\u001b[0m\n",
      "│  │  └─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│  ├─ \u001b[34mnan\u001b[0m Polybench_FLOYD_WARSHALL\u001b[0m\n",
      "│  │  └─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│  ├─ \u001b[34mnan\u001b[0m Polybench_GEMM\u001b[0m\n",
      "│  │  └─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│  ├─ \u001b[34mnan\u001b[0m Polybench_GEMVER\u001b[0m\n",
      "│  │  └─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│  ├─ \u001b[34mnan\u001b[0m Polybench_GESUMMV\u001b[0m\n",
      "│  │  ├─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│  │  └─ \u001b[34mnan\u001b[0m cudaLaunchKernel\u001b[0m\n",
      "│  │     └─ \u001b[38;5;22m0.770\u001b[0m void RAJA::internal::CudaKernelLauncherFixed<128, 1, RAJA::internal::LoopData<camp::tuple<RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long>, RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long> >, camp::tuple<double, double>, camp::resources::v1::Cuda, void rajaperf::polybench::POLYBENCH_GESUMMV::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(double&, double&)#1}, void rajaperf::polybench::POLYBENCH_GESUMMV::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, long, double&, double&)#1}, void rajaperf::polybench::POLYBENCH_GESUMMV::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&, double&)#1}>, RAJA::internal::CudaStatementListExecutor<void rajaperf::polybench::POLYBENCH_GESUMMV::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, long, double&, double&)#1}, camp::list<RAJA::statement::For<0l, RAJA::policy::cuda::cuda_indexer<RAJA::iteration_mapping::Direct, (RAJA::kernel_sync_requirement)0, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0> >, RAJA::internal::LoopData<camp::tuple<RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long>, RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long> >, camp::tuple<double, double>, camp::resources::v1::Cuda, void rajaperf::polybench::POLYBENCH_GESUMMV::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(double&, double&)#1}, void rajaperf::polybench::POLYBENCH_GESUMMV::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, long, double&, double&)#1}, void rajaperf::polybench::POLYBENCH_GESUMMV::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&, double&)#1}>::Lambda<0l, void rajaperf::polybench::POLYBENCH_GESUMMV::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&, double&)#1}<RAJA::internal::LambdaArg<RAJA::internal::lambda_arg_param_t, 0l>, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0><RAJA::policy::cuda::cuda_indexer<RAJA::iteration_mapping::Direct, (RAJA::kernel_sync_requirement)0, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0> >, 1l> > >, RAJA::internal::CudaStatementListExecutor<1l, camp::list::sequential::seq_exec, RAJA::named_dim<1l, void rajaperf::polybench::POLYBENCH_GESUMMV::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&, double&)#1}<RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0><RAJA::internal::lambda_arg_seg_t, 0l>, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0><RAJA::internal::LoopData<camp::tuple<RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long>, RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long> >, camp::tuple<double, double>, camp::resources::v1::Cuda, void rajaperf::polybench::POLYBENCH_GESUMMV::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(double&, double&)#1}, void rajaperf::polybench::POLYBENCH_GESUMMV::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, long, double&, double&)#1}, void rajaperf::polybench::POLYBENCH_GESUMMV::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&, double&)#1}>::Lambda<0l, void rajaperf::polybench::POLYBENCH_GESUMMV::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&, double&)#1}<RAJA::internal::LambdaArg<RAJA::internal::lambda_arg_param_t, 0l>, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0><RAJA::policy::cuda::cuda_indexer<RAJA::iteration_mapping::Direct, (RAJA::kernel_sync_requirement)0, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0> >, 1l> > >, 1l> >, RAJA::internal::lambda_arg_param_t> >, RAJA::named_dim<2l, void rajaperf::polybench::POLYBENCH_GESUMMV::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&, double&)#1}<camp::list::sequential>, RAJA::internal::lambda_arg_param_t> > >, RAJA::internal::LoopTypes<void rajaperf::polybench::POLYBENCH_GESUMMV::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&, double&)#1}<void, void>, RAJA::statement::For<0l, RAJA::policy::cuda::cuda_indexer<RAJA::iteration_mapping::Direct, (RAJA::kernel_sync_requirement)0, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0> >, RAJA::internal::LoopData<camp::tuple<RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long>, RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long> >, camp::tuple<double, double>, camp::resources::v1::Cuda, void rajaperf::polybench::POLYBENCH_GESUMMV::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(double&, double&)#1}, void rajaperf::polybench::POLYBENCH_GESUMMV::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, long, double&, double&)#1}, void rajaperf::polybench::POLYBENCH_GESUMMV::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&, double&)#1}>::Lambda<0l, void rajaperf::polybench::POLYBENCH_GESUMMV::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&, double&)#1}<RAJA::internal::LambdaArg<RAJA::internal::lambda_arg_param_t, 0l>, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0><RAJA::policy::cuda::cuda_indexer<RAJA::iteration_mapping::Direct, (RAJA::kernel_sync_requirement)0, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0> >, 1l> > >, RAJA::internal::CudaStatementListExecutor<1l, camp::list::sequential::seq_exec, RAJA::named_dim<1l, void rajaperf::polybench::POLYBENCH_GESUMMV::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&, double&)#1}<RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0><RAJA::internal::lambda_arg_seg_t, 0l>, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0><RAJA::internal::LoopData<camp::tuple<RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long>, RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long> >, camp::tuple<double, double>, camp::resources::v1::Cuda, void rajaperf::polybench::POLYBENCH_GESUMMV::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(double&, double&)#1}, void rajaperf::polybench::POLYBENCH_GESUMMV::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, long, double&, double&)#1}, void rajaperf::polybench::POLYBENCH_GESUMMV::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&, double&)#1}>::Lambda<0l, void rajaperf::polybench::POLYBENCH_GESUMMV::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&, double&)#1}<RAJA::internal::LambdaArg<RAJA::internal::lambda_arg_param_t, 0l>, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0><RAJA::policy::cuda::cuda_indexer<RAJA::iteration_mapping::Direct, (RAJA::kernel_sync_requirement)0, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0> >, 1l> > >, 1l> >, RAJA::internal::lambda_arg_param_t> >, RAJA::named_dim<2l, void rajaperf::polybench::POLYBENCH_GESUMMV::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&, double&)#1}<camp::list::sequential>, RAJA::internal::lambda_arg_param_t> > > > >(RAJA::internal::LoopData<camp::tuple<RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long>, RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long> >, camp::tuple<double, double>, camp::resources::v1::Cuda, void rajaperf::polybench::POLYBENCH_GESUMMV::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(double&, double&)#1}, void rajaperf::polybench::POLYBENCH_GESUMMV::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, long, double&, double&)#1}, void rajaperf::polybench::POLYBENCH_GESUMMV::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long, double&, double&)#1}>)\u001b[0m\n",
      "│  ├─ \u001b[34mnan\u001b[0m Polybench_HEAT_3D\u001b[0m\n",
      "│  │  └─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│  ├─ \u001b[34mnan\u001b[0m Polybench_JACOBI_1D\u001b[0m\n",
      "│  │  ├─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│  │  └─ \u001b[34mnan\u001b[0m cudaLaunchKernel\u001b[0m\n",
      "│  │     ├─ \u001b[38;5;34m8.887\u001b[0m void RAJA::policy::cuda::impl::forall_cuda_kernel<RAJA::policy::cuda::cuda_exec_explicit<RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true>, 1ul, RAJA::Iterators::numeric_iterator<long, long, long*>, void rajaperf::polybench::POLYBENCH_JACOBI_1D::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#1}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, 128ul>(void rajaperf::polybench::POLYBENCH_JACOBI_1D::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#1}, RAJA::Iterators::numeric_iterator<long, long, long*>, long)\u001b[0m\n",
      "│  │     └─ \u001b[38;5;34m8.887\u001b[0m void RAJA::policy::cuda::impl::forall_cuda_kernel<RAJA::policy::cuda::cuda_exec_explicit<RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true>, 1ul, RAJA::Iterators::numeric_iterator<long, long, long*>, void rajaperf::polybench::POLYBENCH_JACOBI_1D::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#2}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, 128ul>(void rajaperf::polybench::POLYBENCH_JACOBI_1D::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#2}, RAJA::Iterators::numeric_iterator<long, long, long*>, long)\u001b[0m\n",
      "│  ├─ \u001b[34mnan\u001b[0m Polybench_JACOBI_2D\u001b[0m\n",
      "│  │  └─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│  └─ \u001b[34mnan\u001b[0m Polybench_MVT\u001b[0m\n",
      "│     ├─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "│     └─ \u001b[34mnan\u001b[0m cudaLaunchKernel\u001b[0m\n",
      "│        ├─ \u001b[38;5;22m0.740\u001b[0m void RAJA::internal::CudaKernelLauncherFixed<128, 1, RAJA::internal::LoopData<camp::tuple<RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long>, RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long> >, camp::tuple<double>, camp::resources::v1::Cuda, void rajaperf::polybench::POLYBENCH_MVT::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(double&)#1}, void rajaperf::polybench::POLYBENCH_MVT::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(long, long, {lambda()#1})#1}, void rajaperf::polybench::POLYBENCH_MVT::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(long, {lambda()#1})#1}>, RAJA::internal::CudaStatementListExecutor<{lambda(long, long, {lambda()#1})#1}, camp::list<RAJA::statement::For<0l, RAJA::policy::cuda::cuda_indexer<RAJA::iteration_mapping::Direct, (RAJA::kernel_sync_requirement)0, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0> >, void rajaperf::polybench::POLYBENCH_MVT::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(long, {lambda()#1})#1}::Lambda<0l, {lambda(long, {lambda()#1})#1}<RAJA::internal::LambdaArg<RAJA::internal::lambda_arg_param_t, 0l> > >, RAJA::internal::LoopData<camp::tuple<RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long>, RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long> >, camp::tuple<double>, camp::resources::v1::Cuda, void rajaperf::polybench::POLYBENCH_MVT::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(double&)#1}, void rajaperf::polybench::POLYBENCH_MVT::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(long, long, {lambda()#1})#1}, void rajaperf::polybench::POLYBENCH_MVT::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(long, {lambda()#1})#1}><1l, RAJA::internal::CudaStatementListExecutor::sequential::seq_exec, RAJA::cuda::IndexGlobal<1l, {lambda(long, {lambda()#1})#1}<RAJA::named_dim<RAJA::internal::lambda_arg_seg_t, 0l>, RAJA::named_dim<{lambda(long, {lambda()#1})#1}<RAJA::internal::LambdaArg<RAJA::internal::lambda_arg_param_t, 0l> >, 1l> >, void rajaperf::polybench::POLYBENCH_MVT::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(long, {lambda()#1})#1}::Lambda> >, RAJA::cuda::IndexGlobal<2l, {lambda(long, {lambda()#1})#1}<void rajaperf::polybench::POLYBENCH_MVT::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(long, {lambda()#1})#1}::Lambda<0l, {lambda(long, {lambda()#1})#1}<RAJA::internal::LambdaArg<RAJA::internal::lambda_arg_param_t, 0l> > > >, void rajaperf::polybench::POLYBENCH_MVT::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(long, {lambda()#1})#1}::Lambda> > >, RAJA::internal::LoopTypes<{lambda(long, {lambda()#1})#1}<void, void>, RAJA::cuda::IndexGlobal<2l, {lambda(long, {lambda()#1})#1}<void rajaperf::polybench::POLYBENCH_MVT::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(long, {lambda()#1})#1}::Lambda<0l, {lambda(long, {lambda()#1})#1}<RAJA::internal::LambdaArg<RAJA::internal::lambda_arg_param_t, 0l> > > >, void rajaperf::polybench::POLYBENCH_MVT::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(long, {lambda()#1})#1}::Lambda> > > >(RAJA::internal::LoopData<camp::tuple<RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long>, RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long> >, camp::tuple<double>, camp::resources::v1::Cuda, void rajaperf::polybench::POLYBENCH_MVT::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(double&)#1}, void rajaperf::polybench::POLYBENCH_MVT::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(long, long, {lambda()#1})#1}, void rajaperf::polybench::POLYBENCH_MVT::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(long, {lambda()#1})#1}>)\u001b[0m\n",
      "│        └─ \u001b[38;5;22m0.789\u001b[0m void RAJA::internal::CudaKernelLauncherFixed<128, 1, RAJA::internal::LoopData<camp::tuple<RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long>, RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long> >, camp::tuple<double>, camp::resources::v1::Cuda, void rajaperf::polybench::POLYBENCH_MVT::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(double&)#2}, void rajaperf::polybench::POLYBENCH_MVT::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(long, long, {lambda()#1})#2}, void rajaperf::polybench::POLYBENCH_MVT::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(long, {lambda()#1})#2}>, RAJA::internal::CudaStatementListExecutor<{lambda(long, long, {lambda()#1})#2}, camp::list<RAJA::statement::For<0l, RAJA::policy::cuda::cuda_indexer<RAJA::iteration_mapping::Direct, (RAJA::kernel_sync_requirement)0, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0> >, void rajaperf::polybench::POLYBENCH_MVT::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(long, {lambda()#1})#2}::Lambda<0l, {lambda(long, {lambda()#1})#2}<RAJA::internal::LambdaArg<RAJA::internal::lambda_arg_param_t, 0l> > >, RAJA::internal::LoopData<camp::tuple<RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long>, RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long> >, camp::tuple<double>, camp::resources::v1::Cuda, void rajaperf::polybench::POLYBENCH_MVT::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(double&)#2}, void rajaperf::polybench::POLYBENCH_MVT::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(long, long, {lambda()#1})#2}, void rajaperf::polybench::POLYBENCH_MVT::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(long, {lambda()#1})#2}><1l, RAJA::internal::CudaStatementListExecutor::sequential::seq_exec, RAJA::cuda::IndexGlobal<1l, {lambda(long, {lambda()#1})#2}<RAJA::named_dim<RAJA::internal::lambda_arg_seg_t, 0l>, RAJA::named_dim<{lambda(long, {lambda()#1})#2}<RAJA::internal::LambdaArg<RAJA::internal::lambda_arg_param_t, 0l> >, 1l> >, void rajaperf::polybench::POLYBENCH_MVT::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(long, {lambda()#1})#2}::Lambda> >, RAJA::cuda::IndexGlobal<2l, {lambda(long, {lambda()#1})#2}<void rajaperf::polybench::POLYBENCH_MVT::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(long, {lambda()#1})#2}::Lambda<0l, {lambda(long, {lambda()#1})#2}<RAJA::internal::LambdaArg<RAJA::internal::lambda_arg_param_t, 0l> > > >, void rajaperf::polybench::POLYBENCH_MVT::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(long, {lambda()#1})#2}::Lambda> > >, RAJA::internal::LoopTypes<{lambda(long, {lambda()#1})#2}<void, void>, RAJA::cuda::IndexGlobal<2l, {lambda(long, {lambda()#1})#2}<void rajaperf::polybench::POLYBENCH_MVT::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(long, {lambda()#1})#2}::Lambda<0l, {lambda(long, {lambda()#1})#2}<RAJA::internal::LambdaArg<RAJA::internal::lambda_arg_param_t, 0l> > > >, void rajaperf::polybench::POLYBENCH_MVT::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(long, {lambda()#1})#2}::Lambda> > > >(RAJA::internal::LoopData<camp::tuple<RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long>, RAJA::Span<RAJA::Iterators::numeric_iterator<long, long, long*>, long> >, camp::tuple<double>, camp::resources::v1::Cuda, void rajaperf::polybench::POLYBENCH_MVT::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(double&)#2}, void rajaperf::polybench::POLYBENCH_MVT::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(long, long, {lambda()#1})#2}, void rajaperf::polybench::POLYBENCH_MVT::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda()#1}::operator()() const::{lambda(long, {lambda()#1})#2}>)\u001b[0m\n",
      "└─ \u001b[34mnan\u001b[0m Stream\u001b[0m\n",
      "   ├─ \u001b[34mnan\u001b[0m Stream_ADD\u001b[0m\n",
      "   │  ├─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "   │  └─ \u001b[34mnan\u001b[0m cudaLaunchKernel\u001b[0m\n",
      "   │     └─ \u001b[38;5;34m6.272\u001b[0m void RAJA::policy::cuda::impl::forall_cuda_kernel<RAJA::policy::cuda::cuda_exec_explicit<RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true>, 1ul, RAJA::Iterators::numeric_iterator<long, long, long*>, void rajaperf::stream::ADD::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#2}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, 128ul>(void rajaperf::stream::ADD::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#2}, RAJA::Iterators::numeric_iterator<long, long, long*>, long)\u001b[0m\n",
      "   ├─ \u001b[34mnan\u001b[0m Stream_COPY\u001b[0m\n",
      "   │  ├─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "   │  └─ \u001b[34mnan\u001b[0m cudaLaunchKernel\u001b[0m\n",
      "   │     └─ \u001b[38;5;34m7.516\u001b[0m void RAJA::policy::cuda::impl::forall_cuda_kernel<RAJA::policy::cuda::cuda_exec_explicit<RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true>, 1ul, RAJA::Iterators::numeric_iterator<long, long, long*>, void rajaperf::stream::COPY::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#2}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, 128ul>(void rajaperf::stream::COPY::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#2}, RAJA::Iterators::numeric_iterator<long, long, long*>, long)\u001b[0m\n",
      "   ├─ \u001b[34mnan\u001b[0m Stream_MUL\u001b[0m\n",
      "   │  ├─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "   │  └─ \u001b[34mnan\u001b[0m cudaLaunchKernel\u001b[0m\n",
      "   │     └─ \u001b[38;5;34m7.909\u001b[0m void RAJA::policy::cuda::impl::forall_cuda_kernel<RAJA::policy::cuda::cuda_exec_explicit<RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true>, 1ul, RAJA::Iterators::numeric_iterator<long, long, long*>, void rajaperf::stream::MUL::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#2}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, 128ul>(void rajaperf::stream::MUL::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#2}, RAJA::Iterators::numeric_iterator<long, long, long*>, long)\u001b[0m\n",
      "   └─ \u001b[34mnan\u001b[0m Stream_TRIAD\u001b[0m\n",
      "      ├─ \u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "      └─ \u001b[34mnan\u001b[0m cudaLaunchKernel\u001b[0m\n",
      "         └─ \u001b[38;5;34m6.277\u001b[0m void RAJA::policy::cuda::impl::forall_cuda_kernel<RAJA::policy::cuda::cuda_exec_explicit<RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true>, 1ul, RAJA::Iterators::numeric_iterator<long, long, long*>, void rajaperf::stream::TRIAD::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#2}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, 128ul>(void rajaperf::stream::TRIAD::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#2}, RAJA::Iterators::numeric_iterator<long, long, long*>, long)\u001b[0m\n",
      "\u001b[34mnan\u001b[0m cudaDeviceSynchronize\u001b[0m\n",
      "\u001b[34mnan\u001b[0m cudaFree\u001b[0m\n",
      "\u001b[34mnan\u001b[0m cudaFreeHost\u001b[0m\n",
      "\u001b[34mnan\u001b[0m cudaGetDevice\u001b[0m\n",
      "\u001b[34mnan\u001b[0m cudaGetSymbolAddress\u001b[0m\n",
      "\u001b[34mnan\u001b[0m cudaHostAlloc\u001b[0m\n",
      "\u001b[34mnan\u001b[0m cudaLaunchKernel\u001b[0m\n",
      "└─ \u001b[34mnan\u001b[0m void RAJA::policy::cuda::impl::forall_cuda_kernel<RAJA::policy::cuda::cuda_exec_explicit<RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, RAJA::cuda::MaxOccupancyConcretizer, 1ul, true>, 1ul, RAJA::Iterators::numeric_iterator<long, long, long*>, void rajaperf::basic::DAXPY::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#2}, long, RAJA::iteration_mapping::Direct, RAJA::cuda::IndexGlobal<(RAJA::named_dim)0, 128, 0>, 128ul>(void rajaperf::basic::DAXPY::runCudaVariantImpl<128ul>(rajaperf::VariantID)::{lambda(long)#2}, RAJA::Iterators::numeric_iterator<long, long, long*>, long)\u001b[0m\n",
      "\u001b[34mnan\u001b[0m cudaMalloc\u001b[0m\n",
      "\u001b[34mnan\u001b[0m cudaMallocManaged\u001b[0m\n",
      "\u001b[34mnan\u001b[0m cudaMemAdvise\u001b[0m\n",
      "\u001b[34mnan\u001b[0m cudaMemcpy\u001b[0m\n",
      "└─ \u001b[34mnan\u001b[0m memcpy\u001b[0m\n",
      "\u001b[34mnan\u001b[0m cudaMemcpyAsync\u001b[0m\n",
      "└─ \u001b[34mnan\u001b[0m memcpy\u001b[0m\n",
      "\u001b[34mnan\u001b[0m cudaStreamCreate\u001b[0m\n",
      "\n",
      "\u001b[4mLegend\u001b[0m (Metric: sm__throughput.avg.pct_of_peak_sustained_elapsed Min: 0.14 Max: 45.78 indices: {'ProblemSizeRunParam': 2097152})\n",
      "\u001b[38;5;196m█ \u001b[0m41.22 - 45.78\n",
      "\u001b[38;5;208m█ \u001b[0m32.09 - 41.22\n",
      "\u001b[38;5;220m█ \u001b[0m22.96 - 32.09\n",
      "\u001b[38;5;46m█ \u001b[0m13.83 - 22.96\n",
      "\u001b[38;5;34m█ \u001b[0m4.71 - 13.83\n",
      "\u001b[38;5;22m█ \u001b[0m0.14 - 4.71\n",
      "\n",
      "name\u001b[0m User code    \u001b[38;5;160m◀ \u001b[0m Only in left graph    \u001b[38;5;28m▶ \u001b[0m Only in right graph\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tk_cap.tree(\n",
    "    metric_column=\"sm__throughput.avg.pct_of_peak_sustained_elapsed\",\n",
    "    expand_name=True,\n",
    "    ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3.570405,
   "end_time": "2024-03-26T22:19:29.810540",
   "environment_variables": {},
   "exception": null,
   "input_path": "01_thicket_tutorial.ipynb",
   "output_path": "01_thicket_tutorial.ipynb",
   "parameters": {},
   "start_time": "2024-03-26T22:19:26.240135",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
