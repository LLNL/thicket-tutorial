{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Roundtrip module could not be loaded. Requires jupyter notebook version <= 7.x.\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import pickle\n",
    "\n",
    "import hatchet as ht\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import thicket as th\n",
    "\n",
    "DATA_DIR = \"../data/parallel-sorting\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read files into Thicket\n",
    "\n",
    "- `glob`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 15734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(1/2) Reading Files: 100%|██████████| 15734/15734 [01:55<00:00, 136.47it/s]\n",
      "(2/2) Creating Thicket: 100%|██████████| 15733/15733 [02:33<00:00, 102.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape (157459, 16)\n",
      "Metadata shape: (15732, 65)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = glob(f\"{DATA_DIR}/**/*.cali\", recursive=True)\n",
    "print(f\"Total files: {len(data)}\")\n",
    "\n",
    "# Read caliper files without filling the profile index as it expensive and unnecessary in our case\n",
    "tk = th.Thicket.from_caliperreader(\n",
    "    data,\n",
    "    fill_perfdata=False\n",
    ")\n",
    "print(f\"DataFrame shape {tk.dataframe.shape}\")\n",
    "print(f\"Metadata shape: {tk.metadata.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Modify and Filter Metadata Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "META_FIX_DICT = {\n",
    "    \"Algorithm\": {\n",
    "        \"Quicksort\": \"QuickSort\",\n",
    "        \"bitonic_sort\": \"BitonicSort\",\n",
    "        \"merge_sort\": \"MergeSort\",\n",
    "        \"Merge Sort\": \"MergeSort\",\n",
    "        \"quicksort\": \"QuickSort\",\n",
    "        \"odd_even_sort\": \"OddEvenSort\",\n",
    "        \"Merge sort\": \"MergeSort\",\n",
    "        \"Quick Sort\": \"QuickSort\",\n",
    "        \"Sample Sort\": \"SampleSort\",\n",
    "        \"Bitonic_Sort\": \"BitonicSort\",\n",
    "        \"Merge_Sort\": \"MergeSort\",\n",
    "        \"Quick_Sort\": \"QuickSort\",\n",
    "        \"OddEvenTranspositionSort\": \"OddEvenSort\",\n",
    "        \"Bitonic Sort\": \"BitonicSort\",\n",
    "        \"Selection Sort\": \"SelectionSort\",\n",
    "        \"Bucketsort\": \"BucketSort\",\n",
    "        \"selection_sort\": \"SelectionSort\",\n",
    "        \"Mergesort\": \"MergeSort\",\n",
    "        \"mergesort\": \"MergeSort\",\n",
    "        \"oddEven\": \"OddEvenSort\",\n",
    "        \"Quick(Sample) Sort\": \"QuickSort\",\n",
    "        \"Odd Even Transposition Sort\": \"OddEvenSort\",\n",
    "        \"RadixSort Sort\": \"RadixSort\",\n",
    "        \"Bucket Sort\": \"BucketSort\",\n",
    "        \"Odd Even Sort\": \"OddEvenSort\",\n",
    "        \"Odd-Even Sort\": \"OddEvenSort\",\n",
    "        \"Countsort\": \"CountSort\",\n",
    "        \"OddevenSort\": \"OddEvenSort\",\n",
    "        \"oddeven_sort\": \"OddEvenSort\",\n",
    "        \"bucket\": \"BucketSort\",\n",
    "        \"Radix Sort\": \"RadixSort\",\n",
    "        \"Odd-Even Bubble Sort\": \"OddEvenSort\",\n",
    "        \"Bubble_Sort\": \"OddEvenSort\",\n",
    "        \"Bubblesort\": \"OddEvenSort\",\n",
    "        \"Bubble Sort(Odd/Even)\": \"OddEvenSort\",\n",
    "        \"Bubble/Odd-Even Sort\": \"OddEvenSort\",\n",
    "        \"Parallel Bubble Sort\": \"OddEvenSort\",\n",
    "        \"BubbleSort\": \"OddEvenSort\",\n",
    "        \"Radix\": \"RadixSort\",\n",
    "        \"Bitonic\": \"BitonicSort\",\n",
    "    },\n",
    "    \"InputType\": {\n",
    "        \"perturbed_array\": \"1%perturbed\",\n",
    "        \"sorted_array\": \"Sorted\",\n",
    "        \"random_array\": \"Random\",\n",
    "        \"ascending_array\": \"Sorted\",\n",
    "        \"descending_array\": \"Reverse\",\n",
    "        \"reversed_array\": \"Reverse\",\n",
    "        \"reversedSort\": \"Reverse\",\n",
    "        \"1% Perturbed\": \"1%perturbed\",\n",
    "        \"reverse_sorted\": \"Reverse\",\n",
    "        \"1perturbed\": \"1%perturbed\",\n",
    "        r\"1%%perturbed\": \"1%perturbed\",\n",
    "        \"1 Perturbed\": \"1%perturbed\",\n",
    "        \"1 perturbed\": \"1%perturbed\",\n",
    "        \"Reverse Sorted\": \"Reverse\",\n",
    "        \"1%Perturbed\": \"1%perturbed\",\n",
    "        \"1% perturbation\": \"1%perturbed\",\n",
    "        \"1percentperturbed\": \"1%perturbed\",\n",
    "        \"1 percent noise\": \"1%perturbed\",\n",
    "        \"reverse sorted\": \"Reverse\",\n",
    "        \"sorted_1%_perturbed\": \"1%perturbed\",\n",
    "        \"Reversesorted\": \"Reverse\",\n",
    "        \"ReverseSorted\": \"Reverse\",\n",
    "        \"Reverse_Sorted\": \"Reverse\",\n",
    "        \"ReversedSort\": \"Reverse\",\n",
    "        \"Sorted_1%_perturbed\": \"1%perturbed\",\n",
    "        \"Randomized\": \"Random\",\n",
    "        \"Reversed\": \"Reverse\",\n",
    "        \"reversed\": \"Reverse\",\n",
    "        \"sorted\": \"Sorted\",\n",
    "        \"random\": \"Random\",\n",
    "        \"nearly\": \"Nearly\",\n",
    "        \"reverse\": \"Reverse\",\n",
    "        \" Reverse sorted\": \"Reverse\",\n",
    "        \"Perturbed\": \"1%perturbed\",\n",
    "        \"perturbed\": \"1%perturbed\",\n",
    "    },\n",
    "    \"Datatype\": {\n",
    "        \"integer\": \"int\",\n",
    "        \"Int\": \"int\",\n",
    "        \"Integer\": \"int\",\n",
    "        \"Double\": \"double\",\n",
    "    },\n",
    "}\n",
    "\n",
    "META_WHITELIST_DICT = {\n",
    "    \"InputType\": [\"Random\", \"Sorted\", \"Reverse\", \"1%perturbed\", \"Nearly\"],\n",
    "    \"Algorithm\": [\n",
    "        \"BitonicSort\",\n",
    "        \"BucketSort\",\n",
    "        \"CountSort\",\n",
    "        \"EnumerationSort\",\n",
    "        \"MergeSort\",\n",
    "        \"OddEvenSort\",\n",
    "        \"QuickSort\",\n",
    "        \"RadixSort\",\n",
    "        \"SampleSort\",\n",
    "        \"SelectionSort\",\n",
    "    ],\n",
    "    \"Datatype\": [\"int\", \"float\", \"double\"],\n",
    "    \"num_procs\": [2, 4, 8, 16, 32, 64, 128, 256, 512, 1024],\n",
    "    \"InputSize\": [65536, 262144, 1048576, 4194304, 16777216, 67108864, 268435456],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2A. Modify Metadata Values to Match Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for meta_col, values in META_FIX_DICT.items():\n",
    "    tk.metadata[meta_col] = tk.metadata[meta_col].replace(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2B. Filter Metadata Values from Whitelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files before: 15734\n",
      "Total files after: 12641\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total files before: {len(tk.profile)}\")\n",
    "tk = tk.filter_metadata(lambda meta: all([meta[key] in META_WHITELIST_DICT[key] for key in META_WHITELIST_DICT.keys()]))\n",
    "print(f\"Total files after: {len(tk.profile)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2C. Filter Duplicate Metadata Values\n",
    "\n",
    "Indicates that one profile has incorrect metadata, since all profiles are assumed to be single-trial. Usually from user error (metadata is manually annotated in Adiak)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping ('RadixSort', 'Random', 'double', 2.0, 65536.0) (27 profiles) because it has duplicate num_procs\n",
      "Skipping ('RadixSort', 'Random', 'double', 2.0, 262144.0) (26 profiles) because it has duplicate num_procs\n",
      "Total files after removing duplicates: 12588\n"
     ]
    }
   ],
   "source": [
    "gb = tk.groupby([\"Algorithm\", \"InputType\", \"Datatype\", \"group_num\", \"InputSize\"])\n",
    "rm_profs = []\n",
    "for key, ttk in gb.items():\n",
    "    if ttk.metadata[\"num_procs\"].duplicated().any():\n",
    "        print(f\"Skipping {key} ({len(ttk.profile)} profiles) because it has duplicate num_procs\")\n",
    "        rm_profs += ttk.profile   \n",
    "tk = tk.filter_profile([p for p in tk.profile if p not in set(rm_profs)])\n",
    "print(f\"Total files after removing duplicates: {len(tk.profile)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3A. Query the Call Tree\n",
    "\n",
    "To get performance metrics per node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [\n",
    "    \"comp\",\n",
    "    \"comp_large\",\n",
    "    \"comm\",\n",
    "    \"comm_large\",\n",
    "    \"comp_small\",\n",
    "    \"comm_small\"\n",
    "]\n",
    "ntk_dict = {n: tk.query(\n",
    "    th.query.Query().match(\n",
    "        \"*\",\n",
    "        lambda row: row[\"name\"].apply(\n",
    "            lambda tn: tn == n\n",
    "        ).all()\n",
    "    )\n",
    ") for n in nodes}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3B. Compute Features from Performance Data using Queried Thickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = th.Thicket.concat_thickets(\n",
    "    thickets=list(ntk_dict.values()),\n",
    ")\n",
    "# Get mapping of node objects\n",
    "node_objects = {n.frame[\"name\"]: n for n in [n for n in tk.dataframe.index.get_level_values(\"node\").unique()]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate profiles from concat_thickets\n",
    "# Can't pass these cols in the check or error will be thrown. Won't change the outcome of the check\n",
    "unhashable_cols = [\"libraries\", \"cmdline\"]\n",
    "tk.metadata = tk.metadata.drop_duplicates(subset=[col for col in tk.metadata.columns if col not in unhashable_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodes not considered in the check. They are only used for their presence T/F\n",
    "not_considered = [\"comp_small\", \"comm_small\"]\n",
    "profiles_per_node = [set(ntk_dict[n].dataframe.index.get_level_values(\"profile\")) for n in ntk_dict.keys() if n not in not_considered]\n",
    "# Intersection of the profiles\n",
    "profile_truth = list(profiles_per_node[0].intersection(*profiles_per_node[1:]))\n",
    "# Filter the Thicket to only contain these profiles\n",
    "tk = tk.filter_profile(profile_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_cols = [\n",
    "    \"Variance time/rank\",\n",
    "    \"Min time/rank\",\n",
    "    \"Max time/rank\",\n",
    "    \"Avg time/rank\",\n",
    "    \"Total time\",\n",
    "]\n",
    "\n",
    "# Compute metric \"Node presence for each node in the performance data\"\n",
    "tk.dataframe[\"Node presence\"] = tk.dataframe[\"name\"].apply(lambda x: False if x is None else True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute comp/comm\n",
    "tk.add_root_node(attrs={\"name\": \"comp/comm\"})\n",
    "compcomm = [node for node in tk.graph.traverse() if node.frame[\"name\"] == \"comp/comm\"][0]\n",
    "tdf = tk.dataframe.loc[node_objects[\"comp\"], metric_cols].div(tk.dataframe.loc[node_objects[\"comm\"], metric_cols])\n",
    "for prof in tdf.index:\n",
    "    tk.dataframe.loc[(compcomm, prof), metric_cols] = tdf.loc[prof]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_idx = (\n",
    "    (\n",
    "        [\n",
    "            compcomm, \n",
    "            node_objects[\"comp_large\"],\n",
    "            node_objects[\"comm_large\"]\n",
    "        ]\n",
    "    ), metric_cols\n",
    ")\n",
    "\n",
    "presence_idx = (\n",
    "    (\n",
    "        [\n",
    "            node_objects[\"comp_small\"],\n",
    "            node_objects[\"comm_small\"],\n",
    "        ]\n",
    "    ), [\n",
    "        \"Node presence\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "feature_slices = [perf_idx, presence_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3C. Filter Features with NaN Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total profiles before dropping NaNs: 11348\n",
      "Total profiles after dropping NaNs: 10628\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total profiles before dropping NaNs: {len(tk.profile)}\")\n",
    "for idx in feature_slices:\n",
    "    any_nan_rows_series = tk.dataframe.loc[idx].isna().apply(lambda x: x.any(), axis=1)\n",
    "    nan_profs = tk.dataframe.loc[idx][any_nan_rows_series].index.get_level_values(\"profile\").unique()\n",
    "    tk = tk.filter_profile([p for p in tk.profile if p not in nan_profs])\n",
    "print(f\"Total profiles after dropping NaNs: {len(tk.profile)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Remove Anomalies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Omitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Write Model Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm: MergeSort [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 11.0, 13.0, 15.0, 16.0, 18.0, 20.0, 21.0, 24.0] has 2322 data points\n",
      "Algorithm: SampleSort [1.0, 6.0, 9.0, 10.0, 13.0, 14.0, 16.0, 22.0, 25.0] has 2285 data points\n",
      "Algorithm: OddEvenSort [1.0, 3.0, 5.0, 9.0, 11.0, 13.0, 15.0, 17.0, 18.0, 19.0, 20.0, 24.0] has 2078 data points\n",
      "Algorithm: BitonicSort [1.0, 4.0, 5.0, 6.0, 7.0, 10.0, 11.0, 13.0, 15.0, 16.0, 23.0] has 1761 data points\n",
      "Algorithm: QuickSort [3.0, 4.0, 5.0, 11.0, 17.0, 18.0] has 643 data points\n",
      "Algorithm: RadixSort [7.0, 9.0, 10.0, 23.0] has 591 data points\n",
      "Algorithm: EnumerationSort [15.0, 20.0] has 286 data points\n",
      "Algorithm: BucketSort [19.0] has 268 data points\n",
      "Algorithm: CountSort [19.0] has 266 data points\n",
      "Algorithm: SelectionSort [3.0, 16.0] has 128 data points\n",
      "Label: 'MergeSort' has 2322 data points\n",
      "Label: 'SampleSort' has 2285 data points\n",
      "Label: 'OddEvenSort' has 2078 data points\n",
      "Label: 'BitonicSort' has 1761 data points\n",
      "Label: 'RadixSort' has 591 data points\n",
      "Total data points: 9037\n"
     ]
    }
   ],
   "source": [
    "# Shuffle data\n",
    "tk.metadata = tk.metadata.sample(frac=1.0)\n",
    "\n",
    "dfs = {}\n",
    "algs = tk.metadata.reset_index().groupby(\"Algorithm\")\n",
    "for name, data in algs:\n",
    "    dfs[name] = pd.DataFrame(data)\n",
    "\n",
    "dfs = dict(sorted(dfs.items(), key=lambda item: len(item[1]), reverse=True))\n",
    "for name, data in dfs.items():\n",
    "    print(f\"Algorithm: {name} {sorted(list(data['group_num'].unique()))} has {len(data)} data points\")\n",
    "\n",
    "labels = [\n",
    "    'MergeSort',\n",
    "    'SampleSort',\n",
    "    'OddEvenSort',\n",
    "    'BitonicSort',\n",
    "    'RadixSort',\n",
    "]\n",
    "num_classes = len(labels)\n",
    "\n",
    "trim_dict = {}\n",
    "total_data=0\n",
    "min_data_points = min([len(dfs[l]) for l in labels])\n",
    "\n",
    "for label in labels:\n",
    "    print(f\"Label: '{label}' has {len(dfs[label])} data points\")\n",
    "    trim_dict[label] = dfs[label]\n",
    "    total_data += len(trim_dict[label])\n",
    "print(f\"Total data points: {total_data}\")\n",
    "\n",
    "total_df = pd.concat(trim_dict.values(), axis=0)\n",
    "total_df = total_df.set_index(\"profile\")\n",
    "\n",
    "tk = tk.filter_profile(list(total_df.index))\n",
    "\n",
    "# %% [markdown]\n",
    "# # 3. Write Thicket to File\n",
    "\n",
    "tk.feature_slices = feature_slices\n",
    "tk.node_objects = node_objects\n",
    "\n",
    "# %%\n",
    "pickle.dump(tk, open('thicket-modeldata.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-p3.11.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
