{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Composing & Modeling Parallel Sorting Performance Data\n",
    "## Part A: Composing Parallel Sorting Data\n",
    "\n",
    "The parallel sorting dataset consists of 8,747 MPI sorting algorithm performance profiles (collected with [Caliper](https://software.llnl.gov/Caliper/)) for 5 different algorithms and 51 implementations.\n",
    "We start with a dataset that includes over 10,000 performance profiles, and we show how to apply various filters and checks on the performance data to remove profiles that do not match our criteria.\n",
    "We use this data to show how we can train models to determine the algorithm from the performance data.\n",
    "\n",
    "\n",
    "| Algorithm | # Performance Profiles | # Implementations |\n",
    "| -------- | ------- | ------- |\n",
    "| Merge Sort | 2,239 | 15 |\n",
    "| Sample Sort | 2,231 | 9 |\n",
    "| Odd-Even Sort | 2,034 | 12 |\n",
    "| Bitonic Sort | 1,652 | 11 |\n",
    "| Radix Sort | 591 | 4 |\n",
    "| **Total** | **8,747** | **51** |\n",
    "\n",
    "## 1. Import Necessary Packages\n",
    "\n",
    "Import packages and point to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need a newer version of Thicket for this notebook\n",
    "! pip install -U git+https://github.com/LLNL/thicket.git@develop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import thicket as th\n",
    "\n",
    "DATA_DIR = \"../data/parallel-sorting\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read files into Thicket\n",
    "\n",
    "- `glob()` recursively grabs all Caliper files (`.cali`) in the data directory.\n",
    "- `from_caliperreader()` reads the Caliper files into Thicket and `fill_perfdata=False` will save memory, since we have so many files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = glob(f\"{DATA_DIR}/**/*.cali\", recursive=True)\n",
    "print(f\"Total files: {len(data)}\")\n",
    "\n",
    "# Read caliper files without filling the profile index as it expensive and unnecessary in our case\n",
    "tk = th.Thicket.from_caliperreader(\n",
    "    data,\n",
    "    fill_perfdata=False\n",
    ")\n",
    "print(f\"DataFrame shape {tk.dataframe.shape}\")\n",
    "print(f\"Metadata shape: {tk.metadata.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modify and Filter Metadata Values\n",
    "\n",
    "Since the dataset we are using is a compilation from many different implementations, there are various labeling inconsistencies in the metadata annotations which we can fix using Thicket. We have defined two dictionaries from manual analysis of the data to achieve this:\n",
    "\n",
    "- `META_FIX_DICT` is used to enforce consistency in the metadata by replacing inconsistent values.\n",
    "- `META_WHITELIST_DICT` is used to select the metadata parameters we are looking for from the experiments.\n",
    "\n",
    "The metadata we reference are the experiment parameters and important identifying metadata. We use these values for processing and removing anomalies, and `Algorithm` specifically is also used as the class label when modeling:\n",
    "\n",
    "- Experiment Parameters\n",
    "    - `InputType` - The type of sortedness of the input array.\n",
    "    - `Datatype` - The datatype of the values in the input array.\n",
    "    - `num_procs` - Number of parallel processes.\n",
    "    - `InputSize` - Size of the input array.\n",
    "- Parallel Algorithm Class Label\n",
    "    - `Algorithm` - The name of the parallel sorting algorithm.\n",
    "- Identifying metadata\n",
    "    - `group_num` - Unique identifier for different implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "META_FIX_DICT = {\n",
    "    \"Algorithm\": {\n",
    "        \"bitonic_sort\": \"BitonicSort\",\n",
    "        \"merge_sort\": \"MergeSort\",\n",
    "        \"Merge Sort\": \"MergeSort\",\n",
    "        \"odd_even_sort\": \"OddEvenSort\",\n",
    "        \"Merge sort\": \"MergeSort\",\n",
    "        \"Sample Sort\": \"SampleSort\",\n",
    "        \"Bitonic_Sort\": \"BitonicSort\",\n",
    "        \"Merge_Sort\": \"MergeSort\",\n",
    "        \"OddEvenTranspositionSort\": \"OddEvenSort\",\n",
    "        \"Bitonic Sort\": \"BitonicSort\",\n",
    "        \"Mergesort\": \"MergeSort\",\n",
    "        \"mergesort\": \"MergeSort\",\n",
    "        \"oddEven\": \"OddEvenSort\",\n",
    "        \"Odd Even Transposition Sort\": \"OddEvenSort\",\n",
    "        \"RadixSort Sort\": \"RadixSort\",\n",
    "        \"Odd Even Sort\": \"OddEvenSort\",\n",
    "        \"Odd-Even Sort\": \"OddEvenSort\",\n",
    "        \"OddevenSort\": \"OddEvenSort\",\n",
    "        \"oddeven_sort\": \"OddEvenSort\",\n",
    "        \"Radix Sort\": \"RadixSort\",\n",
    "        \"Odd-Even Bubble Sort\": \"OddEvenSort\",\n",
    "        \"Bubble_Sort\": \"OddEvenSort\",\n",
    "        \"Bubblesort\": \"OddEvenSort\",\n",
    "        \"Bubble Sort(Odd/Even)\": \"OddEvenSort\",\n",
    "        \"Bubble/Odd-Even Sort\": \"OddEvenSort\",\n",
    "        \"Parallel Bubble Sort\": \"OddEvenSort\",\n",
    "        \"BubbleSort\": \"OddEvenSort\",\n",
    "        \"Radix\": \"RadixSort\",\n",
    "        \"Bitonic\": \"BitonicSort\",\n",
    "    },\n",
    "    \"InputType\": {\n",
    "        \"perturbed_array\": \"1%perturbed\",\n",
    "        \"sorted_array\": \"Sorted\",\n",
    "        \"random_array\": \"Random\",\n",
    "        \"ascending_array\": \"Sorted\",\n",
    "        \"descending_array\": \"Reverse\",\n",
    "        \"reversed_array\": \"Reverse\",\n",
    "        \"reversedSort\": \"Reverse\",\n",
    "        \"1% Perturbed\": \"1%perturbed\",\n",
    "        \"reverse_sorted\": \"Reverse\",\n",
    "        \"1perturbed\": \"1%perturbed\",\n",
    "        r\"1%%perturbed\": \"1%perturbed\",\n",
    "        \"1 Perturbed\": \"1%perturbed\",\n",
    "        \"1 perturbed\": \"1%perturbed\",\n",
    "        \"Reverse Sorted\": \"Reverse\",\n",
    "        \"1%Perturbed\": \"1%perturbed\",\n",
    "        \"1% perturbation\": \"1%perturbed\",\n",
    "        \"1percentperturbed\": \"1%perturbed\",\n",
    "        \"1 percent noise\": \"1%perturbed\",\n",
    "        \"reverse sorted\": \"Reverse\",\n",
    "        \"sorted_1%_perturbed\": \"1%perturbed\",\n",
    "        \"Reversesorted\": \"Reverse\",\n",
    "        \"ReverseSorted\": \"Reverse\",\n",
    "        \"Reverse_Sorted\": \"Reverse\",\n",
    "        \"ReversedSort\": \"Reverse\",\n",
    "        \"Sorted_1%_perturbed\": \"1%perturbed\",\n",
    "        \"Randomized\": \"Random\",\n",
    "        \"Reversed\": \"Reverse\",\n",
    "        \"reversed\": \"Reverse\",\n",
    "        \"sorted\": \"Sorted\",\n",
    "        \"random\": \"Random\",\n",
    "        \"nearly\": \"Nearly\",\n",
    "        \"reverse\": \"Reverse\",\n",
    "        \" Reverse sorted\": \"Reverse\",\n",
    "        \"Perturbed\": \"1%perturbed\",\n",
    "        \"perturbed\": \"1%perturbed\",\n",
    "    },\n",
    "    \"Datatype\": {\n",
    "        \"integer\": \"int\",\n",
    "        \"Int\": \"int\",\n",
    "        \"Integer\": \"int\",\n",
    "        \"Double\": \"double\",\n",
    "    },\n",
    "}\n",
    "\n",
    "META_WHITELIST_DICT = {\n",
    "    \"InputType\": [\"Random\", \"Sorted\", \"Reverse\", \"1%perturbed\", \"Nearly\"],\n",
    "    \"Algorithm\": [\n",
    "        \"BitonicSort\",\n",
    "        \"MergeSort\",\n",
    "        \"OddEvenSort\",\n",
    "        \"RadixSort\",\n",
    "        \"SampleSort\",\n",
    "    ],\n",
    "    \"Datatype\": [\"int\", \"float\", \"double\"],\n",
    "    \"num_procs\": [2, 4, 8, 16, 32, 64, 128, 256, 512, 1024],\n",
    "    \"InputSize\": [65536, 262144, 1048576, 4194304, 16777216, 67108864, 268435456],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3A. Modify Metadata Values to Match Grammar\n",
    "\n",
    "The `pandas.DataFrame.replace()` function replaces values in the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for meta_col, values in META_FIX_DICT.items():\n",
    "    tk.metadata[meta_col] = tk.metadata[meta_col].replace(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3B. Filter Metadata Values from Whitelist\n",
    "\n",
    "We use the `Thicket.filter_metadata()` function to filter any values that are not contained in our metadata whitelist, which leaves performance profiles that contain the desired metadata for removing anomalies and modeling.\n",
    "\n",
    "*Note: This cell can take 10+ minutes to run*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total profiles before: {len(tk.profile)}\")\n",
    "tk = tk.filter_metadata(lambda meta: all([meta[key] in META_WHITELIST_DICT[key] for key in META_WHITELIST_DICT.keys()]))\n",
    "print(f\"Total profiles after: {len(tk.profile)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3C. Filter Duplicate Metadata Values\n",
    "\n",
    "Duplicate values across all of our experiment parameters indicates that one profile has incorrect metadata, since all of the profiles are single-trial. If we find duplicates of any profile we remove them all, as we cannot assume which profile contains the correct metadata. These occurrences are a result of incorrect manual annotation before generating the profiles.\n",
    "\n",
    "We can remove duplicate values by using `Thicket.groupby()` on our experiment parameters except \"num_procs\", and then checking if there are any duplicates of \"num_procs\" using `pandas.DataFrame.duplicated()`. We then remove the duplicate profiles using `Thicket.filter_profile()`.\n",
    "\n",
    "*Note: This cell can take 10+ minutes to run*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = tk.groupby([\"Algorithm\", \"InputType\", \"Datatype\", \"group_num\", \"InputSize\"])\n",
    "rm_profs = []\n",
    "for key, ttk in gb.items():\n",
    "    if ttk.metadata[\"num_procs\"].duplicated().any():\n",
    "        print(f\"Skipping {key} ({len(ttk.profile)} profiles) because it has duplicate num_procs\")\n",
    "        rm_profs += ttk.profile   \n",
    "tk = tk.filter_profile([p for p in tk.profile if p not in set(rm_profs)])\n",
    "print(f\"Total profiles after removing duplicates: {len(tk.profile)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Selecting Features\n",
    "\n",
    "In this section, we structure the performance data where each column is a feature, and each row is a feature vector for one performance profile, which is necessary for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4A. Query the Call Tree\n",
    "\n",
    "For this study, we used \"generalized\" nodes for annotations. So a given node in the calltree would be annotated by its functionality, communication or computation, and the amount of data it operated on, small or large.\n",
    "\n",
    "```\n",
    "main                 // Top-level function of the program\n",
    "|_ comm              // Parent for all communication nodes\n",
    "|    |_ comm_small   // All nodes communicating \"small\" data\n",
    "|    |_ comm_large   // All nodes communicating \"large\" data\n",
    "|_ comp              // Parent for all computation nodes\n",
    "|    |_ comp_small   // All nodes computing on \"small\" data\n",
    "|    |_ comp_large   // All nodes computing on \"large\" data\n",
    "```\n",
    "\n",
    "Not all implementations match this tree 100% correctly. Some implementations include additional nodes, or have generalized nodes at different depths in the calltree, which results in duplicates of the same nodes after composing the Thicket. We will use `Thicket.query()` to subselect the performance metrics for the generalized nodes that we want to use for modeling. Querying by node name will also combine nodes with the same name at various depths into one node at root depth.\n",
    "\n",
    "*Note: Printing the `Thicket.tree()` at this point will show the full calltree, which includes many nodes which are not relevant to our analysis.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform query\n",
    "nodes = [\n",
    "    \"comp\",\n",
    "    \"comp_large\",\n",
    "    \"comm\",\n",
    "    \"comm_large\",\n",
    "    \"comp_small\",\n",
    "    \"comm_small\"\n",
    "]\n",
    "ntk_dict = {n: tk.query(\n",
    "    th.query.Query().match(\n",
    "        \"*\",\n",
    "        lambda row: row[\"name\"].apply(\n",
    "            lambda tn: tn == n\n",
    "        ).all()\n",
    "    )\n",
    ") for n in nodes}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4B. Compose a New Thicket from the Queried Thickets\n",
    "\n",
    "We use `Thicket.concat_thickets()` to compose the Thickets we created from each query. Since many of these Thickets will contain the same profiles in their metadata, we drop duplicate values using `pandas.drop_duplicates()`\n",
    "\n",
    "*Note: Unlike when we read the files, fill_perfdata is True here. This is so we can later compute the feature \"Present\" using the None values in the \"name\" column.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-compose quieried Thickets\n",
    "tk = th.Thicket.concat_thickets(\n",
    "    thickets=list(ntk_dict.values()),\n",
    "    fill_perfdata=True,\n",
    ")\n",
    "# Drop duplicate profiles in the metadata from concat_thickets\n",
    "unhashable_cols = [\"libraries\", \"cmdline\"] # Can't pass these cols in the check or error will be thrown. Won't change the outcome of the check\n",
    "tk.metadata = tk.metadata.drop_duplicates(subset=[col for col in tk.metadata.columns if col not in unhashable_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4C. Remove Profiles with Missing Nodes\n",
    "\n",
    "Since we did not design our models to handle missing data points, we need to remove profiles with missing measurements for our selected nodes using `Thicket.filter_profile()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodes not considered in the check. They are only used for their presence T/F\n",
    "not_considered = [\"comp_small\", \"comm_small\"]\n",
    "profiles_per_node = [set(ntk_dict[n].dataframe.index.get_level_values(\"profile\")) for n in ntk_dict.keys() if n not in not_considered]\n",
    "# Intersection of the profiles\n",
    "profile_truth = list(profiles_per_node[0].intersection(*profiles_per_node[1:]))\n",
    "# Filter the Thicket to only contain these profiles\n",
    "tk = tk.filter_profile(profile_truth)\n",
    "print(f\"Total profiles that contain all data: {len(tk.profile)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4D. Computute Additional Features from Performance Data\n",
    "\n",
    "We compute the \"Present\" feature and the derived \"comp/comm\" features using a mixture of `pandas` functions. The `add_root_node()` function is used to add the \"comp/comm\" features to the performance data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_cols = [\n",
    "    \"Variance time/rank\",\n",
    "    \"Min time/rank\",\n",
    "    \"Max time/rank\",\n",
    "    \"Avg time/rank\",\n",
    "    \"Total time\",\n",
    "]\n",
    "\n",
    "# Compute Present feature\n",
    "tk.dataframe[\"Present\"] = tk.dataframe[\"name\"].apply(lambda name: False if name is None else True)\n",
    "\n",
    "# Compute comp/comm feature\n",
    "tk.add_root_node(attrs={\"name\": \"comp/comm\", \"type\": \"derived\"})\n",
    "tdf = tk.dataframe.loc[tk.get_node(\"comp\"), metric_cols].div(tk.dataframe.loc[tk.get_node(\"comm\"), metric_cols])\n",
    "# Replace inf with NaN where division by 0 occurred\n",
    "tdf = tdf.replace({np.inf: np.nan})\n",
    "for prof in tdf.index:\n",
    "    tk.dataframe.loc[(tk.get_node(\"comp/comm\"), prof), metric_cols] = tdf.loc[prof]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4E: Define Our Features as Pandas Slices\n",
    "\n",
    "Here we are essentially defining macros to refer to the features. There needs to be two macros because each macro indexes the data differently.\n",
    "\n",
    "To subselect the performance data we use a slice generated by either `perf_idx()` or `presence_idx()` (they are functions because the node objects can change `id`'s after certain Thicket operations). We use the `Thicket.get_node()` function to select node objects.\n",
    "\n",
    "We can index the performance data with these slices using `Thicket.dataframe.loc[perf_idx()]` or `Thicket.dataframe.loc[presence_idx()]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_idx():\n",
    "    return (\n",
    "        (\n",
    "            [\n",
    "                tk.get_node(\"comp/comm\"), \n",
    "                tk.get_node(\"comp_large\"),\n",
    "                tk.get_node(\"comm_large\")\n",
    "            ]\n",
    "        ), metric_cols\n",
    "    )\n",
    "\n",
    "def presence_idx():\n",
    "    return (\n",
    "        (\n",
    "            [\n",
    "                tk.get_node(\"comp_small\"),\n",
    "                tk.get_node(\"comm_small\"),\n",
    "            ]\n",
    "        ), [\n",
    "            \"Present\"\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4F. Remove Profiles with Missing Metrics\n",
    "\n",
    "Here we check for any missing data points in any of the profiles for each of the slices we just defined. This check is different from 4C, as we are checking that there are no missing metrics.\n",
    "\n",
    "`any_nan_rows_series` will be a series of boolean values for each profile that will be `True` if there are any missing data points. We use the `Thicket.filter_profile()` function once again to filter out the profiles with missing data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total profiles before dropping NaNs: {len(tk.profile)}\")\n",
    "nan_profs = []\n",
    "for idx in [perf_idx(), presence_idx()]:\n",
    "    any_nan_rows_series = tk.dataframe.loc[idx].isna().apply(lambda x: x.any(), axis=1)\n",
    "    nan_profs.extend(tk.dataframe.loc[idx][any_nan_rows_series].index.get_level_values(\"profile\").unique())\n",
    "tk = tk.filter_profile([p for p in tk.profile if p not in nan_profs])\n",
    "print(f\"Total profiles after dropping NaNs: {len(tk.profile)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Remove Anomalies \n",
    "In this section, we show how a custom function can be used on a Thicket object. We use the `find_outliers` function to identify profiles that fall outside certain quantile ranges for a given feature. We use the `filter_profile` function to filter the outliers returned by `find_outliers`. This idea can be used to apply custom criteria to the Thicket object, by identifying the profiles we want to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outliers(\n",
    "    tk,\n",
    "    cols_percs,\n",
    "):\n",
    "    \"\"\"Compute outliers for the combination of Algorithm, InputType, and Datatype.\n",
    "    Normalize values by num_procs and InputSize.\n",
    "\n",
    "    Arguments:\n",
    "        tk (Thicket): The Thicket object.\n",
    "        cols_percs (dict): Dictionary of columns and their percentiles.\n",
    "\n",
    "    Returns:\n",
    "        set: A set of outlier profiles.\n",
    "    \"\"\"\n",
    "\n",
    "    def find_single_outlier_profiles(df, node, col, percs):\n",
    "        df = df.loc[node]\n",
    "        upper = df[col].quantile(percs[1])\n",
    "        lower = df[col].quantile(percs[0])\n",
    "        return set(\n",
    "            df[(df[col] > upper) | (df[col] < lower)].index.get_level_values(\"profile\")\n",
    "        )\n",
    "\n",
    "    tkc = tk.deepcopy()\n",
    "    tkc.metadata_column_to_perfdata(\"num_procs\")\n",
    "    tkc.metadata_column_to_perfdata(\"InputSize\")\n",
    "\n",
    "    # Normalize the columns by num_procs and InputSize\n",
    "    tkc.dataframe[\"np*IS\"] = tkc.dataframe[\"num_procs\"] * tkc.dataframe[\"InputSize\"]\n",
    "    for node, col in cols_percs.keys():\n",
    "        tkc.dataframe[node, col] = tkc.dataframe.loc[node, col].div(tkc.dataframe.loc[node, \"np*IS\"])        \n",
    "\n",
    "    single_outlier_profiles = set()\n",
    "    grouped = tkc.groupby(\n",
    "        [\n",
    "            \"Algorithm\",\n",
    "            \"InputType\",\n",
    "            \"Datatype\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Find the outlier profiles\n",
    "    for alg_inp_dtype, ttk in grouped.items():\n",
    "        temp_set = set()\n",
    "        tdf = ttk.dataframe\n",
    "        if len(tdf) >= 3:\n",
    "            # Find outliers\n",
    "            for (node, col), percs in cols_percs.items():\n",
    "                prfs = find_single_outlier_profiles(tdf, node, col, percs)\n",
    "                temp_set |= prfs\n",
    "                single_outlier_profiles |= prfs\n",
    "            # Uncomment for extra information\n",
    "            # print(\n",
    "            #     f\"Checked {alg_inp_dtype}. Total outliers {len(temp_set)}/{len(tdf)} ({len(temp_set)/len(tdf)*100:.2f}%)\"\n",
    "            # )\n",
    "        else:\n",
    "            raise ValueError(f\"Insufficient profiles for {alg_inp_dtype}\")\n",
    "\n",
    "    # find single outlier profiles\n",
    "    print(\n",
    "        f\"Single outlier profiles: {len(single_outlier_profiles)}/{len(tkc.profile)} ({len(single_outlier_profiles)/len(tkc.profile)*100:.2f}%)\"\n",
    "    )\n",
    "\n",
    "    return single_outlier_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc=0.975\n",
    "outlier_profiles = find_outliers(\n",
    "    tk,\n",
    "    {\n",
    "        (tk.get_node(\"comp_large\"), \"Min time/rank\"): [0, perc],\n",
    "        (tk.get_node(\"comp_large\"), \"Max time/rank\"): [0, perc],\n",
    "        (tk.get_node(\"comp_large\"), \"Avg time/rank\"): [0, perc],\n",
    "        (tk.get_node(\"comp_large\"), \"Total time\"): [0, perc],\n",
    "        (tk.get_node(\"comp_large\"), \"Variance time/rank\"): [0, perc],\n",
    "        (tk.get_node(\"comm_large\"), \"Min time/rank\"): [0, perc],\n",
    "        (tk.get_node(\"comm_large\"), \"Max time/rank\"): [0, perc],\n",
    "        (tk.get_node(\"comm_large\"), \"Avg time/rank\"): [0, perc],\n",
    "        (tk.get_node(\"comm_large\"), \"Total time\"): [0, perc],\n",
    "        (tk.get_node(\"comm_large\"), \"Variance time/rank\"): [0, perc],\n",
    "    },\n",
    ")\n",
    "print(f\"Total profiles before dropping outliers: {len(tk.profile)}\")\n",
    "tk = tk.filter_profile([p for p in tk.profile if p not in outlier_profiles])\n",
    "print(f\"Total profiles after dropping outliers: {len(tk.profile)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Write Model Data\n",
    "\n",
    "Lastly we shuffle the data using `pandas.DataFrame.sample()` to reduce bias during model training, and pickle the Thicket object, which we will use to pick back up in the next notebook, part B, where we will create classification models using the performance data. Pickling is helpful in this scenario to avoid re-doing the steps in this notebook every time we want to re-run or make adjustments to our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print how many profiles for each sorting algorithm\n",
    "algs = tk.metadata.reset_index().groupby(\"Algorithm\")\n",
    "for name, data in algs:\n",
    "    print(f\"Algorithm: {name} has {len(data)} data points\")\n",
    "\n",
    "# Shuffle the data\n",
    "tk.dataframe = tk.dataframe.sample(frac=1.0)\n",
    "# Set useful attributes\n",
    "tk.perf_idx = perf_idx()\n",
    "tk.presence_idx = presence_idx()\n",
    "# Write thicket to file\n",
    "tk.to_pickle(\"thicket-modeldata.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-p3.11.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
