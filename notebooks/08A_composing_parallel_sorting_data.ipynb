{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Composing & Modeling Parallel Sorting Performance Data\n",
    "## Part A: Composing Parallel Sorting Data\n",
    "\n",
    "The parallel sorting dataset consists of over 10,000 MPI sorting algorithm performance profiles for 5 different algorithms, collected by about 100 users. We use this data to show how we can train models to determine the algorithm from the performance data.\n",
    "\n",
    "## 1. Import Necessary Packages\n",
    "\n",
    "Import packages and point to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Roundtrip module could not be loaded. Requires jupyter notebook version <= 7.x.\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "import thicket as th\n",
    "\n",
    "DATA_DIR = \"../data/parallel-sorting\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read files into Thicket\n",
    "\n",
    "- `glob()` recursively grabs all Caliper files (`.cali`) in the data directory.\n",
    "- `from_caliperreader()` reads the Caliper files into Thicket and `fill_perfdata=False` will save memory, since we have so many files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 12916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(1/2) Reading Files: 100%|██████████| 12916/12916 [01:34<00:00, 136.30it/s]\n",
      "(2/2) Creating Thicket: 100%|██████████| 12915/12915 [02:06<00:00, 102.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape (128716, 16)\n",
      "Metadata shape: (12916, 62)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = glob(f\"{DATA_DIR}/**/*.cali\", recursive=True)\n",
    "print(f\"Total files: {len(data)}\")\n",
    "\n",
    "# Read caliper files without filling the profile index as it expensive and unnecessary in our case\n",
    "tk = th.Thicket.from_caliperreader(\n",
    "    data,\n",
    "    fill_perfdata=False\n",
    ")\n",
    "print(f\"DataFrame shape {tk.dataframe.shape}\")\n",
    "print(f\"Metadata shape: {tk.metadata.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modify and Filter Metadata Values\n",
    "\n",
    "Since the dataset we are using is a compilation from many different users, there are various errors in the metadata annotations which we can fix using Thicket. We have defined two dictionaries from manual analysis of the data to achieve this:\n",
    "\n",
    "- `META_FIX_DICT` is used to enforce consistency in the metadata by replacing values.\n",
    "- `META_WHITELIST_DICT` is used to select the metadata parameters we are looking for from the experiments.\n",
    "\n",
    "The metadata we reference are the experiment parameters and important identifying metadata:\n",
    "\n",
    "- `InputType` - The type of sortedness of the input array.\n",
    "- `Datatype` - The datatype of the values in the input array.\n",
    "- `num_procs` - Number of parallel processes.\n",
    "- `InputSize` - Size of the input array.\n",
    "- `Algorithm` - The name of the parallel sorting algorithm.\n",
    "- `group_num` - Unique identifier for different implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "META_FIX_DICT = {\n",
    "    \"Algorithm\": {\n",
    "        \"bitonic_sort\": \"BitonicSort\",\n",
    "        \"merge_sort\": \"MergeSort\",\n",
    "        \"Merge Sort\": \"MergeSort\",\n",
    "        \"odd_even_sort\": \"OddEvenSort\",\n",
    "        \"Merge sort\": \"MergeSort\",\n",
    "        \"Sample Sort\": \"SampleSort\",\n",
    "        \"Bitonic_Sort\": \"BitonicSort\",\n",
    "        \"Merge_Sort\": \"MergeSort\",\n",
    "        \"OddEvenTranspositionSort\": \"OddEvenSort\",\n",
    "        \"Bitonic Sort\": \"BitonicSort\",\n",
    "        \"Mergesort\": \"MergeSort\",\n",
    "        \"mergesort\": \"MergeSort\",\n",
    "        \"oddEven\": \"OddEvenSort\",\n",
    "        \"Odd Even Transposition Sort\": \"OddEvenSort\",\n",
    "        \"RadixSort Sort\": \"RadixSort\",\n",
    "        \"Odd Even Sort\": \"OddEvenSort\",\n",
    "        \"Odd-Even Sort\": \"OddEvenSort\",\n",
    "        \"OddevenSort\": \"OddEvenSort\",\n",
    "        \"oddeven_sort\": \"OddEvenSort\",\n",
    "        \"Radix Sort\": \"RadixSort\",\n",
    "        \"Odd-Even Bubble Sort\": \"OddEvenSort\",\n",
    "        \"Bubble_Sort\": \"OddEvenSort\",\n",
    "        \"Bubblesort\": \"OddEvenSort\",\n",
    "        \"Bubble Sort(Odd/Even)\": \"OddEvenSort\",\n",
    "        \"Bubble/Odd-Even Sort\": \"OddEvenSort\",\n",
    "        \"Parallel Bubble Sort\": \"OddEvenSort\",\n",
    "        \"BubbleSort\": \"OddEvenSort\",\n",
    "        \"Radix\": \"RadixSort\",\n",
    "        \"Bitonic\": \"BitonicSort\",\n",
    "    },\n",
    "    \"InputType\": {\n",
    "        \"perturbed_array\": \"1%perturbed\",\n",
    "        \"sorted_array\": \"Sorted\",\n",
    "        \"random_array\": \"Random\",\n",
    "        \"ascending_array\": \"Sorted\",\n",
    "        \"descending_array\": \"Reverse\",\n",
    "        \"reversed_array\": \"Reverse\",\n",
    "        \"reversedSort\": \"Reverse\",\n",
    "        \"1% Perturbed\": \"1%perturbed\",\n",
    "        \"reverse_sorted\": \"Reverse\",\n",
    "        \"1perturbed\": \"1%perturbed\",\n",
    "        r\"1%%perturbed\": \"1%perturbed\",\n",
    "        \"1 Perturbed\": \"1%perturbed\",\n",
    "        \"1 perturbed\": \"1%perturbed\",\n",
    "        \"Reverse Sorted\": \"Reverse\",\n",
    "        \"1%Perturbed\": \"1%perturbed\",\n",
    "        \"1% perturbation\": \"1%perturbed\",\n",
    "        \"1percentperturbed\": \"1%perturbed\",\n",
    "        \"1 percent noise\": \"1%perturbed\",\n",
    "        \"reverse sorted\": \"Reverse\",\n",
    "        \"sorted_1%_perturbed\": \"1%perturbed\",\n",
    "        \"Reversesorted\": \"Reverse\",\n",
    "        \"ReverseSorted\": \"Reverse\",\n",
    "        \"Reverse_Sorted\": \"Reverse\",\n",
    "        \"ReversedSort\": \"Reverse\",\n",
    "        \"Sorted_1%_perturbed\": \"1%perturbed\",\n",
    "        \"Randomized\": \"Random\",\n",
    "        \"Reversed\": \"Reverse\",\n",
    "        \"reversed\": \"Reverse\",\n",
    "        \"sorted\": \"Sorted\",\n",
    "        \"random\": \"Random\",\n",
    "        \"nearly\": \"Nearly\",\n",
    "        \"reverse\": \"Reverse\",\n",
    "        \" Reverse sorted\": \"Reverse\",\n",
    "        \"Perturbed\": \"1%perturbed\",\n",
    "        \"perturbed\": \"1%perturbed\",\n",
    "    },\n",
    "    \"Datatype\": {\n",
    "        \"integer\": \"int\",\n",
    "        \"Int\": \"int\",\n",
    "        \"Integer\": \"int\",\n",
    "        \"Double\": \"double\",\n",
    "    },\n",
    "}\n",
    "\n",
    "META_WHITELIST_DICT = {\n",
    "    \"InputType\": [\"Random\", \"Sorted\", \"Reverse\", \"1%perturbed\", \"Nearly\"],\n",
    "    \"Algorithm\": [\n",
    "        \"BitonicSort\",\n",
    "        \"MergeSort\",\n",
    "        \"OddEvenSort\",\n",
    "        \"RadixSort\",\n",
    "        \"SampleSort\",\n",
    "    ],\n",
    "    \"Datatype\": [\"int\", \"float\", \"double\"],\n",
    "    \"num_procs\": [2, 4, 8, 16, 32, 64, 128, 256, 512, 1024],\n",
    "    \"InputSize\": [65536, 262144, 1048576, 4194304, 16777216, 67108864, 268435456],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3A. Modify Metadata Values to Match Grammar\n",
    "\n",
    "The `pandas.DataFrame.replace()` function replaces values in the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for meta_col, values in META_FIX_DICT.items():\n",
    "    tk.metadata[meta_col] = tk.metadata[meta_col].replace(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3B. Filter Metadata Values from Whitelist\n",
    "\n",
    "We use the `Thicket.filter_metadata()` function to filter any values that are not contained in our metadata whitelist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total profiles before: 12916\n",
      "Total profiles after: 10624\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total profiles before: {len(tk.profile)}\")\n",
    "tk = tk.filter_metadata(lambda meta: all([meta[key] in META_WHITELIST_DICT[key] for key in META_WHITELIST_DICT.keys()]))\n",
    "print(f\"Total profiles after: {len(tk.profile)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3C. Filter Duplicate Metadata Values\n",
    "\n",
    "Duplicate values across all of our experiment parameters indicates that one profile has incorrect metadata, since all of the profiles are single-trial. This is typically user error (metadata is manually annotated in Adiak).\n",
    "\n",
    "We can remove duplicate values by using `Thicket.groupby()` on our experiment parameters except \"num_procs\", and then checking if there are any duplicates of \"num_procs\" using `pandas.DataFrame.duplicated()`. We then remove the duplicate profiles using `Thicket.filter_profile()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping ('RadixSort', 'Random', 'double', 2, 65536) (27 profiles) because it has duplicate num_procs\n",
      "Skipping ('RadixSort', 'Random', 'double', 2, 262144) (26 profiles) because it has duplicate num_procs\n",
      "Total profiles after removing duplicates: 10571\n"
     ]
    }
   ],
   "source": [
    "gb = tk.groupby([\"Algorithm\", \"InputType\", \"Datatype\", \"group_num\", \"InputSize\"])\n",
    "rm_profs = []\n",
    "for key, ttk in gb.items():\n",
    "    if ttk.metadata[\"num_procs\"].duplicated().any():\n",
    "        print(f\"Skipping {key} ({len(ttk.profile)} profiles) because it has duplicate num_procs\")\n",
    "        rm_profs += ttk.profile   \n",
    "tk = tk.filter_profile([p for p in tk.profile if p not in set(rm_profs)])\n",
    "print(f\"Total profiles after removing duplicates: {len(tk.profile)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Features\n",
    "\n",
    "In this section, we structure the performance data where each column is a feature, and each row is a feature vector for one performance profile, which is necessary for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4A. Query the Call Tree and Re-compose the Thicket\n",
    "\n",
    "Here we select only the nodes we want to compare for our modeling using `Thicket.query()` and `Thicket.concat_thickets()`.\n",
    "\n",
    "*Note: Unlike when we read the files, fill_perfdata is True here. This is so we can compute node presence using the None values in the \"name\" column.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform query\n",
    "nodes = [\n",
    "    \"comp\",\n",
    "    \"comp_large\",\n",
    "    \"comm\",\n",
    "    \"comm_large\",\n",
    "    \"comp_small\",\n",
    "    \"comm_small\"\n",
    "]\n",
    "ntk_dict = {n: tk.query(\n",
    "    th.query.Query().match(\n",
    "        \"*\",\n",
    "        lambda row: row[\"name\"].apply(\n",
    "            lambda tn: tn == n\n",
    "        ).all()\n",
    "    )\n",
    ") for n in nodes}\n",
    "\n",
    "# Re-compose quieried Thickets\n",
    "tk = th.Thicket.concat_thickets(\n",
    "    thickets=list(ntk_dict.values()),\n",
    "    fill_perfdata=True,\n",
    ")\n",
    "# Drop duplicate profiles in the metadata from concat_thickets\n",
    "unhashable_cols = [\"libraries\", \"cmdline\"] # Can't pass these cols in the check or error will be thrown. Won't change the outcome of the check\n",
    "tk.metadata = tk.metadata.drop_duplicates(subset=[col for col in tk.metadata.columns if col not in unhashable_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4B. Remove Profiles not Containing All Nodes\n",
    "\n",
    "Because our features will use all of the nodes, we remove profiles that do not have data for all nodes using `Thicket.filter_profile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total profiles that contain all data: 9406\n"
     ]
    }
   ],
   "source": [
    "# Nodes not considered in the check. They are only used for their presence T/F\n",
    "not_considered = [\"comp_small\", \"comm_small\"]\n",
    "profiles_per_node = [set(ntk_dict[n].dataframe.index.get_level_values(\"profile\")) for n in ntk_dict.keys() if n not in not_considered]\n",
    "# Intersection of the profiles\n",
    "profile_truth = list(profiles_per_node[0].intersection(*profiles_per_node[1:]))\n",
    "# Filter the Thicket to only contain these profiles\n",
    "tk = tk.filter_profile(profile_truth)\n",
    "print(f\"Total profiles that contain all data: {len(tk.profile)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4C. Compute Features from Performance Data\n",
    "\n",
    "We compute the \"node presence\" feature and the derived \"comp/comm\" features using a mixture of `pandas` functions. The `add_root_node` function is used to add the \"comp/comm\" features to the performance data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_cols = [\n",
    "    \"Variance time/rank\",\n",
    "    \"Min time/rank\",\n",
    "    \"Max time/rank\",\n",
    "    \"Avg time/rank\",\n",
    "    \"Total time\",\n",
    "]\n",
    "\n",
    "# Compute node presence feature\n",
    "tk.dataframe[\"Node presence\"] = tk.dataframe[\"name\"].apply(lambda name: False if name is None else True)\n",
    "\n",
    "# Compute comp/comm feature\n",
    "tk.add_root_node(attrs={\"name\": \"comp/comm\", \"type\": \"derived\"})\n",
    "tdf = tk.dataframe.loc[tk.get_node(\"comp\"), metric_cols].div(tk.dataframe.loc[tk.get_node(\"comm\"), metric_cols])\n",
    "for prof in tdf.index:\n",
    "    tk.dataframe.loc[(tk.get_node(\"comp/comm\"), prof), metric_cols] = tdf.loc[prof]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4D: Define Our Features Using Pandas Slices\n",
    "\n",
    "To subselect the performance data we care about we use a slice generated by either `perf_idx()` or `presence_idx()` (they are functions because the node objects can change `id`'s after certain Thicket operations). We use the `Thicket.get_node()` function to select node objects.\n",
    "\n",
    "We can index the performance data with these slices using `Thicket.dataframe.loc[perf_idx()]` or `Thicket.dataframe.loc[presence_idx()]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_idx():\n",
    "    return (\n",
    "        (\n",
    "            [\n",
    "                tk.get_node(\"comp/comm\"), \n",
    "                tk.get_node(\"comp_large\"),\n",
    "                tk.get_node(\"comm_large\")\n",
    "            ]\n",
    "        ), metric_cols\n",
    "    )\n",
    "\n",
    "def presence_idx():\n",
    "    return (\n",
    "        (\n",
    "            [\n",
    "                tk.get_node(\"comp_small\"),\n",
    "                tk.get_node(\"comm_small\"),\n",
    "            ]\n",
    "        ), [\n",
    "            \"Node presence\"\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4E. Filter Features with NaN Values\n",
    "\n",
    "Here we check one last time for any missing data points in any of the profiles for each of the slices we just defined. `any_nan_rows_series` will be a series of boolean values for each profile that will be `True` if there are any missing data points. We use the `Thicket.filter_profile()` function once again to filter out the profiles with missing data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total profiles before dropping NaNs: 9406\n",
      "Total profiles after dropping NaNs: 9037\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total profiles before dropping NaNs: {len(tk.profile)}\")\n",
    "nan_profs = []\n",
    "for idx in [perf_idx(), presence_idx()]:\n",
    "    any_nan_rows_series = tk.dataframe.loc[idx].isna().apply(lambda x: x.any(), axis=1)\n",
    "    nan_profs.extend(tk.dataframe.loc[idx][any_nan_rows_series].index.get_level_values(\"profile\").unique())\n",
    "tk = tk.filter_profile([p for p in tk.profile if p not in nan_profs])\n",
    "print(f\"Total profiles after dropping NaNs: {len(tk.profile)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Remove Anomalies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Omitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Write Model Data\n",
    "\n",
    "Lastly we shuffle the data using `pandas.DataFrame.sample()` and pickle the Thicket object, which we will use to pick back up in the next notebook, part B, where we will create classification models using the performance data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm: BitonicSort has 1761 data points\n",
      "Algorithm: MergeSort has 2322 data points\n",
      "Algorithm: OddEvenSort has 2078 data points\n",
      "Algorithm: RadixSort has 591 data points\n",
      "Algorithm: SampleSort has 2285 data points\n"
     ]
    }
   ],
   "source": [
    "# Print how many profiles for each sorting algorithm\n",
    "algs = tk.metadata.reset_index().groupby(\"Algorithm\")\n",
    "for name, data in algs:\n",
    "    print(f\"Algorithm: {name} has {len(data)} data points\")\n",
    "\n",
    "# Shuffle the data\n",
    "tk.dataframe = tk.dataframe.sample(frac=1.0)\n",
    "# Set useful attributes\n",
    "tk.perf_idx = perf_idx()\n",
    "tk.presence_idx = presence_idx()\n",
    "# Write thicket to file\n",
    "tk.to_pickle(\"thicket-modeldata.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-p3.11.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
