{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Composing & Modeling Parallel Sorting Performance Data\n",
    "## Part B: Modeling Parallel Sorting Data\n",
    "\n",
    "In part B, we use machine learning to predict the parallel algorithm class from the performance data we processed and composed in part A. Running notebook `08A_composing_parallel_sorting_data.ipynb` is necessary to generate the data that we will use in this notebook.\n",
    "\n",
    "## 1. Import Necessary Packages\n",
    "\n",
    "Import packages:\n",
    "- `numpy` and `pandas` for help with data operations.\n",
    "- `sklearn` for modeling.\n",
    "- `matplotlib` to plot our model's performance statistics.\n",
    "- `thicket` to unpickle the Thicket from part A.\n",
    "- `tqdm` for modeling progress bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics, svm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "import thicket as th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Modeling Helper Functions\n",
    "- `prep_data` first performs standardization scaling on the numerical columns to boost model accuracy (for models that require normally distributed data). Then `prep_data` converts categorical columns to integer labels using the `sklearn.preprocessing.LabelEncoder()` function.\n",
    "- `split_X_y` splits a dataset into input features (X) and labels (y).\n",
    "- `compute_model_metrics` computes model statistics given the true labels, predicted labels, and probabilities. The model statistics we compute are accuracy, precision, recall, F1-score, the confusion matrix, and the ROC_AUC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_categorical(model_data, categories):\n",
    "    for col in categories:\n",
    "        # Encode any \"string\" categorical variables\n",
    "        if model_data[col].dtype == \"object\":\n",
    "            # Strings to ints\n",
    "            model_data.loc[:, [col]] = LabelEncoder().fit_transform(model_data[col])\n",
    "        else:\n",
    "            # anything else to int\n",
    "            model_data[col] = model_data[col].astype(int)\n",
    "            model_data.loc[:, [col]] = model_data[col]\n",
    "        # Set to categorical\n",
    "        model_data[col] = model_data[col].astype(\"category\")\n",
    "\n",
    "    return model_data\n",
    "\n",
    "def prep_data(\n",
    "    data,\n",
    "    numerical_columns,\n",
    "    categorical_columns,\n",
    "    scaling=False,\n",
    "):\n",
    "    # preprocessing\n",
    "    if scaling:\n",
    "        scaler = StandardScaler().set_output(transform=\"pandas\")\n",
    "        data[numerical_columns] = scaler.fit_transform(data[numerical_columns])\n",
    "\n",
    "    if len(categorical_columns) > 0:\n",
    "        data = configure_categorical(data, categorical_columns)\n",
    "\n",
    "    return data\n",
    "\n",
    "def split_X_y(data):\n",
    "    y = pd.get_dummies(data[\"Algorithm\"], dtype=np.float64)\n",
    "    y_index = y.index\n",
    "    y_values = y.values.argmax(axis=1)\n",
    "    y = pd.Series(y_values, index=y_index)\n",
    "    X = data.drop(columns=[\"Algorithm\"])\n",
    "    return X, y\n",
    "\n",
    "def compute_model_metrics(y_true, y_pred, y_proba):\n",
    "\n",
    "    def ravel(tlist, max_num):\n",
    "        tarr = []\n",
    "        for y in tlist:\n",
    "            t1 = np.zeros(max_num + 1)\n",
    "            t1[y] = 1\n",
    "            tarr += t1.tolist()\n",
    "        return tarr\n",
    "\n",
    "    def unravel_true_pred(y_true, y_pred):\n",
    "        max_num = max(max(y_pred), max(y_true))\n",
    "        unravel_true = ravel(y_true, max_num)\n",
    "        unravel_pred = ravel(y_pred, max_num)\n",
    "        return unravel_true, unravel_pred\n",
    "\n",
    "    acc = metrics.accuracy_score(y_true=y_true, y_pred=y_pred)  # Accuracy\n",
    "    pre = metrics.precision_score(\n",
    "        y_true=y_true, y_pred=y_pred, average=\"weighted\", zero_division=0\n",
    "    )  # Precision\n",
    "    rec = metrics.recall_score(\n",
    "        y_true=y_true, y_pred=y_pred, average=\"weighted\", zero_division=0\n",
    "    )  # Recall\n",
    "    f1 = metrics.f1_score(\n",
    "        y_true=y_true, y_pred=y_pred, average=\"weighted\", zero_division=0\n",
    "    )  # F1\n",
    "    conf_matrix = metrics.confusion_matrix(\n",
    "        y_true=y_true, y_pred=y_pred\n",
    "    )  # Confusion matrix\n",
    "    unravel_true, unravel_pred = unravel_true_pred(y_true, y_pred)\n",
    "    y_proba = y_proba.ravel()\n",
    "    try:  # case where class sample didnt get into data\n",
    "        roc_auc_score = metrics.roc_auc_score(\n",
    "            y_true=unravel_true,\n",
    "            y_score=y_proba,\n",
    "            multi_class=\"ovr\",\n",
    "            average=\"weighted\",\n",
    "        )\n",
    "    except ValueError:\n",
    "        roc_auc_score = 0\n",
    "    return acc, pre, rec, f1, conf_matrix, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3A. Unpickle the Thicket from part A\n",
    "\n",
    "Running `08A_composing_parallel_sorting_data.ipynb` is necessary before this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeldata_file = \"thicket-modeldata.pkl\"\n",
    "if not os.path.isfile(modeldata_file):\n",
    "    raise FileNotFoundError(f'You must run notebook \"08A_composing_parallel_sorting_data.ipynb\" before running this notebook to generate the model data \"{modeldata_file}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = th.Thicket.from_pickle(\"thicket-modeldata.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3B. Concatenate Features\n",
    "To match the expected format of the Scikit-learn models, we concatenate the features together such that each row in the `model_data` DataFrame is a data sample (profile). We can achieve this desired format by using `pd.DataFrame.unstack()` to pivot the node index into the column labels, and using `pd.concat()` to concatenate the results. \n",
    "\n",
    "As a simple example: applying the unstack operation to a MultiIndex DataFrame (node and profile) with two unique values for node and two columns will result in a DataFrame with one index level (profile) and 4 columns (2 nodes x 2 columns). We are essentially extending the DataFrame on the column axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = pd.concat(\n",
    "    [\n",
    "        tk.dataframe.loc[tk.perf_idx].unstack(level=\"node\"),\n",
    "        tk.dataframe.loc[tk.presence_idx].unstack(level=\"node\"),\n",
    "        tk.metadata[\"Algorithm\"]\n",
    "    ],\n",
    "    axis=\"columns\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3C. Define Categorical and Numerical Columns\n",
    "\n",
    "We manually define the two categorical \"Present\" columns we created in notebook 08A and then define the numerical columns, which are by definition the remaining columns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\n",
    "    (\"Present\", tk.get_node(\"comp_small\")),\n",
    "    (\"Present\", tk.get_node(\"comm_small\"))\n",
    "]\n",
    "numerical_columns = list(set(model_data.columns) - set(categorical_columns + [\"Algorithm\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D. Discretize the Dataset (Optional)\n",
    "\n",
    "By converting the numerical columns to integer labels (like the categorical columns) we improve accuracy for the SVM significantly. We do this for each numerical feature separately by computing at most `n` quantiles, where `n` is the number of samples. We then convert the quantile ranges to integer labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = len(model_data)\n",
    "for i in tqdm(range(len(numerical_columns))):\n",
    "    col = numerical_columns[i]\n",
    "    model_data[col] = pd.qcut(\n",
    "        model_data[col],\n",
    "        q=q,\n",
    "        duplicates=\"drop\"\n",
    "    )\n",
    "    model_data[col] = OrdinalEncoder(dtype=np.int64).fit_transform(model_data[[col]])\n",
    "model_data = configure_categorical(model_data, numerical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3E. Define Dictionary of Models\n",
    "\n",
    "Here we create a dictionary of each of the machine learning models we want to use to classify our algorithm dataset so that we can use them in the step 4 loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"DecisionTree\": DecisionTreeClassifier(\n",
    "            class_weight=\"balanced\",\n",
    "            min_samples_leaf=1,\n",
    "        ),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "            class_weight=\"balanced\",\n",
    "            n_estimators=100,\n",
    "            bootstrap=False,\n",
    "        ),\n",
    "    \"SVM\": svm.SVC(\n",
    "            kernel=\"rbf\",\n",
    "            probability=True,\n",
    "            class_weight=\"balanced\",\n",
    "            C=1000,\n",
    "        )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training Loop\n",
    "For each model we run for multiple trials where:    \n",
    "- We use `prep_data` which can be configured differently for different models.\n",
    "- We use `KFold` cross validation to ensure we are testing the entire dataset.\n",
    "    - Initialize a new model and fit it to the training data set.\n",
    "    - Compute the model statistics and store them in a dataframe with any other model metadata.\n",
    "\n",
    "We concatenate all of the model result information into `model_results`, which we will use to plot the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = pd.DataFrame()\n",
    "folds = 10\n",
    "trials = 3\n",
    "for model_name in classifiers.keys():\n",
    "    pbar = tqdm(range(trials))\n",
    "    for t in pbar:\n",
    "        metadata_list = []\n",
    "        mdc = model_data.copy()\n",
    "        mdc = prep_data(\n",
    "            data=mdc,\n",
    "            numerical_columns=numerical_columns,\n",
    "            categorical_columns=categorical_columns,\n",
    "            scaling=True,\n",
    "        )\n",
    "        kf = KFold(\n",
    "            n_splits=folds, \n",
    "            random_state=None,\n",
    "            )\n",
    "        for fold, (train_indices, test_indices) in enumerate(kf.split(mdc)):\n",
    "            pbar.set_description(f\"{model_name}: Trial {t+1}/{trials}, Fold {fold+1}/{folds}\")\n",
    "            \n",
    "            train_data = mdc.iloc[train_indices]\n",
    "            test_data = mdc.iloc[test_indices]\n",
    "            \n",
    "            X_train, y_train = split_X_y(train_data)\n",
    "            X_test, y_test = split_X_y(test_data)\n",
    "\n",
    "            # Init\n",
    "            model = classifiers[model_name]\n",
    "            \n",
    "            # Train\n",
    "            model.fold = fold\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # Compute scores\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_proba = model.predict_proba(X_test.astype(np.float32))\n",
    "            acc, pre, rec, f1, conf_matrix, roc_auc_score = compute_model_metrics(\n",
    "                y_true=y_test, y_pred=y_pred, y_proba=y_proba\n",
    "            )\n",
    "\n",
    "            y_proba = [tuple(i for i in j) for j in y_proba]\n",
    "            profile_labels = [mdc.index[i] for i in test_indices]\n",
    "            assert len(profile_labels) == len(test_indices)\n",
    "\n",
    "            values_dict = {\n",
    "                # Profile labels\n",
    "                \"profile\": profile_labels,\n",
    "                # Model preds\n",
    "                \"y_pred\": y_pred.tolist(),\n",
    "                \"y_proba\": y_proba,\n",
    "                \"y_true\": y_test.tolist(),\n",
    "                # Model Performance data\n",
    "                \"classifier\": model_name,\n",
    "                \"trial\": t+1,\n",
    "                \"test_acc\": acc,\n",
    "                \"test_pre\": pre,\n",
    "                \"test_rec\": rec,\n",
    "                \"test_f1\": f1,\n",
    "                \"test_roc_auc\": roc_auc_score,\n",
    "                \"trials\": trials,\n",
    "                \"fold\": fold,\n",
    "                \"n_fold\": folds,\n",
    "                \"num_files\": len(tk.profile),\n",
    "            }\n",
    "\n",
    "            tdf = pd.DataFrame.from_dict(values_dict)\n",
    "            metadata_list.append(tdf)\n",
    "        df = pd.concat(metadata_list)\n",
    "        model_results = pd.concat([model_results, df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Model Performance\n",
    "With the `_plot_bars()` function, we can visualize the per-fold accuracy of each classifier for each model performance statistic. We notice that the SVM and random forest perform comparibly, with the decision tree slightly less accurate than both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _plot_bars(\n",
    "        mean_df, \n",
    "        std_df,  \n",
    "        grouper, \n",
    "        x_group, \n",
    "        xlabel=None, \n",
    "        ylabel=None, \n",
    "        title=None, \n",
    "        font=None,\n",
    "        legend_dict=None,\n",
    "        legend=None,\n",
    "        color=None,\n",
    "        random=False,\n",
    "        ylim=(0, 1),\n",
    "        colorbar=None,\n",
    "        kind=\"bar\",\n",
    "    ):\n",
    "    unstacker = list(set(grouper) - set([x_group]))\n",
    "    mu_df = mean_df.unstack(level=unstacker)\n",
    "    su_df = std_df.unstack(level=unstacker)\n",
    "\n",
    "    if font:\n",
    "        plt.rcParams.update(font)\n",
    "\n",
    "    for col in mu_df.columns.get_level_values(0).unique():\n",
    "        tdf1 = mu_df[col]\n",
    "        if col == \"test_acc\" and random:\n",
    "            tdf1[\"Random Classifier\"] = [1/num_classes for num_classes in tdf1.index.get_level_values(0)]\n",
    "        ax = tdf1.plot(kind=kind, yerr=su_df[col], capsize=5, figsize=(10, 5), color=color, legend=legend)\n",
    "        plt.ylim(ylim)\n",
    "        if xlabel:\n",
    "            plt.xlabel(xlabel)\n",
    "        if ylabel:\n",
    "            plt.ylabel(ylabel)\n",
    "        if title:\n",
    "            plt.title(title)\n",
    "        plt.grid(False)\n",
    "        if legend_dict and legend:\n",
    "            plt.legend(\n",
    "                **legend_dict\n",
    "            )\n",
    "        if colorbar is not None:\n",
    "            plt.colorbar(colorbar, label='Parameter', ax=ax)\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can optionally join information from the Thicket metadata table to use in analysis of model performance.\n",
    "model_results = model_results.join(tk.metadata[[\"Algorithm\", \"InputSize\", \"InputType\", \"num_procs\", \"group_num\", \"Datatype\"]], on=\"profile\")\n",
    "\n",
    "for met in [\"test_acc\", \"test_pre\", \"test_rec\", \"test_f1\", \"test_roc_auc\"]:\n",
    "    config = [\n",
    "        \"fold\",\n",
    "        \"classifier\",\n",
    "    ]\n",
    "    mean_df = model_results[[met]+config].groupby(config).mean()\n",
    "    std_df = model_results[[met]+config].groupby(config).std()\n",
    "    _plot_bars(\n",
    "        mean_df=mean_df, \n",
    "        std_df=std_df, \n",
    "        grouper=config, \n",
    "        x_group=\"fold\",\n",
    "        ylabel=met,\n",
    "        xlabel=\"fold\",\n",
    "        legend=True,\n",
    "        title=f\"{met} vs fold\",\n",
    "        ylim=(0.5, 1),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tk-3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
